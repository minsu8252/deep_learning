{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a20c064",
   "metadata": {},
   "source": [
    "# <6조> 신용카드 부정사용 감지\n",
    "\n",
    "- <h3>조장 : 정연우</h3>\n",
    "\n",
    "- <h3>조원 : 김현수, 김민수, 김민승 </h3>\n",
    "\n",
    " \n",
    "\n",
    "##### 1. 데이터 전처리 (StanderdScaler, 필요없는 column 삭제)\n",
    "\n",
    "\n",
    "\n",
    "##### 2. 데이터 원본 크기\n",
    "\n",
    "    -  DNN 학습\n",
    "\n",
    "    -  MODEL 평가 및 예측\n",
    "\n",
    "\n",
    "\n",
    "##### 3. 데이터 비율 조정\n",
    "\n",
    "    -  DNN 학습\n",
    "\n",
    "    -  MODEL 평가 및 예측\n",
    "\n",
    "\n",
    "\n",
    "##### 4. ANN\n",
    "\n",
    "    -  ANN 학습\n",
    "\n",
    "    -  MODEL 평가 및 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a1b2107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 함수를 import했음\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14252bc7",
   "metadata": {},
   "source": [
    "# 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41e3f8dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         149.62\n",
      "1           2.69\n",
      "2         378.66\n",
      "3         123.50\n",
      "4          69.99\n",
      "           ...  \n",
      "284802      0.77\n",
      "284803     24.79\n",
      "284804     67.88\n",
      "284805     10.00\n",
      "284806    217.00\n",
      "Name: Amount, Length: 284807, dtype: float64 \n",
      "\n",
      " <class 'pandas.core.series.Series'> \n",
      "\n",
      " (284807,)\n"
     ]
    }
   ],
   "source": [
    "# 시드 값 지정\n",
    "np.random.seed(3)\n",
    "tf.random.set_seed(3)\n",
    "\n",
    "df = pd.read_csv('creditcard.csv')\n",
    "amount = df['Amount']\n",
    "\n",
    "print(f'{amount} \\n\\n {type(amount)} \\n\\n {amount.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1e59084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[149.62   2.69 378.66 ...  67.88  10.   217.  ]\n",
      "[[149.62]\n",
      " [  2.69]\n",
      " [378.66]\n",
      " ...\n",
      " [ 67.88]\n",
      " [ 10.  ]\n",
      " [217.  ]] \n",
      "\n",
      " <class 'numpy.ndarray'> \n",
      "\n",
      " (284807, 1)\n"
     ]
    }
   ],
   "source": [
    "amount_val = amount.values # 시리즈의 value값만 가져옴\n",
    "print(amount_val)\n",
    "\n",
    "amount = amount_val.reshape(-1,1) #amount.shape가 [1]이 없기때문에 reshape하여서 빈곳채움\n",
    "\n",
    "print(f'{amount} \\n\\n {type(amount)} \\n\\n {amount.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5cd3bdec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.24496426],\n",
       "       [-0.34247454],\n",
       "       [ 1.16068593],\n",
       "       ...,\n",
       "       [-0.0818393 ],\n",
       "       [-0.31324853],\n",
       "       [ 0.51435531]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# StandardScaler를 이용하여 Amount를 정규화\n",
    "scaler = StandardScaler() #가져온거임\n",
    "\n",
    "scaler.fit(amount) # 정규화 실행\n",
    "amount_scaled = scaler.transform(amount) # 실행된 값을 새로운 인스턴스에 부여함\n",
    "amount_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77d2a438",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "      <th>normalAmount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "      <td>0.244964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.342475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "      <td>1.160686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "      <td>0.140534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.073403</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...       V22       V23       V24       V25       V26  \\\n",
       "0  0.098698  0.363787  ...  0.277838 -0.110474  0.066928  0.128539 -0.189115   \n",
       "1  0.085102 -0.255425  ... -0.638672  0.101288 -0.339846  0.167170  0.125895   \n",
       "2  0.247676 -1.514654  ...  0.771679  0.909412 -0.689281 -0.327642 -0.139097   \n",
       "3  0.377436 -1.387024  ...  0.005274 -0.190321 -1.175575  0.647376 -0.221929   \n",
       "4 -0.270533  0.817739  ...  0.798278 -0.137458  0.141267 -0.206010  0.502292   \n",
       "\n",
       "        V27       V28  Amount  Class  normalAmount  \n",
       "0  0.133558 -0.021053  149.62      0      0.244964  \n",
       "1 -0.008983  0.014724    2.69      0     -0.342475  \n",
       "2 -0.055353 -0.059752  378.66      0      1.160686  \n",
       "3  0.062723  0.061458  123.50      0      0.140534  \n",
       "4  0.219422  0.215153   69.99      0     -0.073403  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터프레임에 정규화된 값을 normalAmount라는 새로운 column에 넣기\n",
    "\n",
    "df['normalAmount'] = amount_scaled\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8aab3e0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(284807, 32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "119bc5ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>normalAmount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0.244964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>-0.342475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>1.160686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0.140534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>-0.073403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284802</th>\n",
       "      <td>172786.0</td>\n",
       "      <td>-11.881118</td>\n",
       "      <td>10.071785</td>\n",
       "      <td>-9.834783</td>\n",
       "      <td>-2.066656</td>\n",
       "      <td>-5.364473</td>\n",
       "      <td>-2.606837</td>\n",
       "      <td>-4.918215</td>\n",
       "      <td>7.305334</td>\n",
       "      <td>1.914428</td>\n",
       "      <td>...</td>\n",
       "      <td>0.213454</td>\n",
       "      <td>0.111864</td>\n",
       "      <td>1.014480</td>\n",
       "      <td>-0.509348</td>\n",
       "      <td>1.436807</td>\n",
       "      <td>0.250034</td>\n",
       "      <td>0.943651</td>\n",
       "      <td>0.823731</td>\n",
       "      <td>0.77</td>\n",
       "      <td>-0.350151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284803</th>\n",
       "      <td>172787.0</td>\n",
       "      <td>-0.732789</td>\n",
       "      <td>-0.055080</td>\n",
       "      <td>2.035030</td>\n",
       "      <td>-0.738589</td>\n",
       "      <td>0.868229</td>\n",
       "      <td>1.058415</td>\n",
       "      <td>0.024330</td>\n",
       "      <td>0.294869</td>\n",
       "      <td>0.584800</td>\n",
       "      <td>...</td>\n",
       "      <td>0.214205</td>\n",
       "      <td>0.924384</td>\n",
       "      <td>0.012463</td>\n",
       "      <td>-1.016226</td>\n",
       "      <td>-0.606624</td>\n",
       "      <td>-0.395255</td>\n",
       "      <td>0.068472</td>\n",
       "      <td>-0.053527</td>\n",
       "      <td>24.79</td>\n",
       "      <td>-0.254117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284804</th>\n",
       "      <td>172788.0</td>\n",
       "      <td>1.919565</td>\n",
       "      <td>-0.301254</td>\n",
       "      <td>-3.249640</td>\n",
       "      <td>-0.557828</td>\n",
       "      <td>2.630515</td>\n",
       "      <td>3.031260</td>\n",
       "      <td>-0.296827</td>\n",
       "      <td>0.708417</td>\n",
       "      <td>0.432454</td>\n",
       "      <td>...</td>\n",
       "      <td>0.232045</td>\n",
       "      <td>0.578229</td>\n",
       "      <td>-0.037501</td>\n",
       "      <td>0.640134</td>\n",
       "      <td>0.265745</td>\n",
       "      <td>-0.087371</td>\n",
       "      <td>0.004455</td>\n",
       "      <td>-0.026561</td>\n",
       "      <td>67.88</td>\n",
       "      <td>-0.081839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284805</th>\n",
       "      <td>172788.0</td>\n",
       "      <td>-0.240440</td>\n",
       "      <td>0.530483</td>\n",
       "      <td>0.702510</td>\n",
       "      <td>0.689799</td>\n",
       "      <td>-0.377961</td>\n",
       "      <td>0.623708</td>\n",
       "      <td>-0.686180</td>\n",
       "      <td>0.679145</td>\n",
       "      <td>0.392087</td>\n",
       "      <td>...</td>\n",
       "      <td>0.265245</td>\n",
       "      <td>0.800049</td>\n",
       "      <td>-0.163298</td>\n",
       "      <td>0.123205</td>\n",
       "      <td>-0.569159</td>\n",
       "      <td>0.546668</td>\n",
       "      <td>0.108821</td>\n",
       "      <td>0.104533</td>\n",
       "      <td>10.00</td>\n",
       "      <td>-0.313249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284806</th>\n",
       "      <td>172792.0</td>\n",
       "      <td>-0.533413</td>\n",
       "      <td>-0.189733</td>\n",
       "      <td>0.703337</td>\n",
       "      <td>-0.506271</td>\n",
       "      <td>-0.012546</td>\n",
       "      <td>-0.649617</td>\n",
       "      <td>1.577006</td>\n",
       "      <td>-0.414650</td>\n",
       "      <td>0.486180</td>\n",
       "      <td>...</td>\n",
       "      <td>0.261057</td>\n",
       "      <td>0.643078</td>\n",
       "      <td>0.376777</td>\n",
       "      <td>0.008797</td>\n",
       "      <td>-0.473649</td>\n",
       "      <td>-0.818267</td>\n",
       "      <td>-0.002415</td>\n",
       "      <td>0.013649</td>\n",
       "      <td>217.00</td>\n",
       "      <td>0.514355</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>284807 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Time         V1         V2        V3        V4        V5  \\\n",
       "0            0.0  -1.359807  -0.072781  2.536347  1.378155 -0.338321   \n",
       "1            0.0   1.191857   0.266151  0.166480  0.448154  0.060018   \n",
       "2            1.0  -1.358354  -1.340163  1.773209  0.379780 -0.503198   \n",
       "3            1.0  -0.966272  -0.185226  1.792993 -0.863291 -0.010309   \n",
       "4            2.0  -1.158233   0.877737  1.548718  0.403034 -0.407193   \n",
       "...          ...        ...        ...       ...       ...       ...   \n",
       "284802  172786.0 -11.881118  10.071785 -9.834783 -2.066656 -5.364473   \n",
       "284803  172787.0  -0.732789  -0.055080  2.035030 -0.738589  0.868229   \n",
       "284804  172788.0   1.919565  -0.301254 -3.249640 -0.557828  2.630515   \n",
       "284805  172788.0  -0.240440   0.530483  0.702510  0.689799 -0.377961   \n",
       "284806  172792.0  -0.533413  -0.189733  0.703337 -0.506271 -0.012546   \n",
       "\n",
       "              V6        V7        V8        V9  ...       V21       V22  \\\n",
       "0       0.462388  0.239599  0.098698  0.363787  ... -0.018307  0.277838   \n",
       "1      -0.082361 -0.078803  0.085102 -0.255425  ... -0.225775 -0.638672   \n",
       "2       1.800499  0.791461  0.247676 -1.514654  ...  0.247998  0.771679   \n",
       "3       1.247203  0.237609  0.377436 -1.387024  ... -0.108300  0.005274   \n",
       "4       0.095921  0.592941 -0.270533  0.817739  ... -0.009431  0.798278   \n",
       "...          ...       ...       ...       ...  ...       ...       ...   \n",
       "284802 -2.606837 -4.918215  7.305334  1.914428  ...  0.213454  0.111864   \n",
       "284803  1.058415  0.024330  0.294869  0.584800  ...  0.214205  0.924384   \n",
       "284804  3.031260 -0.296827  0.708417  0.432454  ...  0.232045  0.578229   \n",
       "284805  0.623708 -0.686180  0.679145  0.392087  ...  0.265245  0.800049   \n",
       "284806 -0.649617  1.577006 -0.414650  0.486180  ...  0.261057  0.643078   \n",
       "\n",
       "             V23       V24       V25       V26       V27       V28  Amount  \\\n",
       "0      -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053  149.62   \n",
       "1       0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724    2.69   \n",
       "2       0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752  378.66   \n",
       "3      -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458  123.50   \n",
       "4      -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153   69.99   \n",
       "...          ...       ...       ...       ...       ...       ...     ...   \n",
       "284802  1.014480 -0.509348  1.436807  0.250034  0.943651  0.823731    0.77   \n",
       "284803  0.012463 -1.016226 -0.606624 -0.395255  0.068472 -0.053527   24.79   \n",
       "284804 -0.037501  0.640134  0.265745 -0.087371  0.004455 -0.026561   67.88   \n",
       "284805 -0.163298  0.123205 -0.569159  0.546668  0.108821  0.104533   10.00   \n",
       "284806  0.376777  0.008797 -0.473649 -0.818267 -0.002415  0.013649  217.00   \n",
       "\n",
       "        normalAmount  \n",
       "0           0.244964  \n",
       "1          -0.342475  \n",
       "2           1.160686  \n",
       "3           0.140534  \n",
       "4          -0.073403  \n",
       "...              ...  \n",
       "284802     -0.350151  \n",
       "284803     -0.254117  \n",
       "284804     -0.081839  \n",
       "284805     -0.313249  \n",
       "284806      0.514355  \n",
       "\n",
       "[284807 rows x 31 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Class가 아닌 column만 가져와서 새로운 데이터프레임을 생성\n",
    "# 여기서 != 는 is not을 의미\n",
    "df_x = df.iloc[:,df.columns != 'Class'].copy()\n",
    "df_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3171be1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         0\n",
       "1         0\n",
       "2         0\n",
       "3         0\n",
       "4         0\n",
       "         ..\n",
       "284802    0\n",
       "284803    0\n",
       "284804    0\n",
       "284805    0\n",
       "284806    0\n",
       "Name: Class, Length: 284807, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Class column만 가져옴\n",
    "df_y = df['Class']\n",
    "df_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a406faa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>normalAmount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>0.251412</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>0.244964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.069083</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>-0.342475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.524980</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>1.160686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.208038</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>0.140534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>0.408542</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>-0.073403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284802</th>\n",
       "      <td>172786.0</td>\n",
       "      <td>-11.881118</td>\n",
       "      <td>10.071785</td>\n",
       "      <td>-9.834783</td>\n",
       "      <td>-2.066656</td>\n",
       "      <td>-5.364473</td>\n",
       "      <td>-2.606837</td>\n",
       "      <td>-4.918215</td>\n",
       "      <td>7.305334</td>\n",
       "      <td>1.914428</td>\n",
       "      <td>...</td>\n",
       "      <td>1.475829</td>\n",
       "      <td>0.213454</td>\n",
       "      <td>0.111864</td>\n",
       "      <td>1.014480</td>\n",
       "      <td>-0.509348</td>\n",
       "      <td>1.436807</td>\n",
       "      <td>0.250034</td>\n",
       "      <td>0.943651</td>\n",
       "      <td>0.823731</td>\n",
       "      <td>-0.350151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284803</th>\n",
       "      <td>172787.0</td>\n",
       "      <td>-0.732789</td>\n",
       "      <td>-0.055080</td>\n",
       "      <td>2.035030</td>\n",
       "      <td>-0.738589</td>\n",
       "      <td>0.868229</td>\n",
       "      <td>1.058415</td>\n",
       "      <td>0.024330</td>\n",
       "      <td>0.294869</td>\n",
       "      <td>0.584800</td>\n",
       "      <td>...</td>\n",
       "      <td>0.059616</td>\n",
       "      <td>0.214205</td>\n",
       "      <td>0.924384</td>\n",
       "      <td>0.012463</td>\n",
       "      <td>-1.016226</td>\n",
       "      <td>-0.606624</td>\n",
       "      <td>-0.395255</td>\n",
       "      <td>0.068472</td>\n",
       "      <td>-0.053527</td>\n",
       "      <td>-0.254117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284804</th>\n",
       "      <td>172788.0</td>\n",
       "      <td>1.919565</td>\n",
       "      <td>-0.301254</td>\n",
       "      <td>-3.249640</td>\n",
       "      <td>-0.557828</td>\n",
       "      <td>2.630515</td>\n",
       "      <td>3.031260</td>\n",
       "      <td>-0.296827</td>\n",
       "      <td>0.708417</td>\n",
       "      <td>0.432454</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001396</td>\n",
       "      <td>0.232045</td>\n",
       "      <td>0.578229</td>\n",
       "      <td>-0.037501</td>\n",
       "      <td>0.640134</td>\n",
       "      <td>0.265745</td>\n",
       "      <td>-0.087371</td>\n",
       "      <td>0.004455</td>\n",
       "      <td>-0.026561</td>\n",
       "      <td>-0.081839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284805</th>\n",
       "      <td>172788.0</td>\n",
       "      <td>-0.240440</td>\n",
       "      <td>0.530483</td>\n",
       "      <td>0.702510</td>\n",
       "      <td>0.689799</td>\n",
       "      <td>-0.377961</td>\n",
       "      <td>0.623708</td>\n",
       "      <td>-0.686180</td>\n",
       "      <td>0.679145</td>\n",
       "      <td>0.392087</td>\n",
       "      <td>...</td>\n",
       "      <td>0.127434</td>\n",
       "      <td>0.265245</td>\n",
       "      <td>0.800049</td>\n",
       "      <td>-0.163298</td>\n",
       "      <td>0.123205</td>\n",
       "      <td>-0.569159</td>\n",
       "      <td>0.546668</td>\n",
       "      <td>0.108821</td>\n",
       "      <td>0.104533</td>\n",
       "      <td>-0.313249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284806</th>\n",
       "      <td>172792.0</td>\n",
       "      <td>-0.533413</td>\n",
       "      <td>-0.189733</td>\n",
       "      <td>0.703337</td>\n",
       "      <td>-0.506271</td>\n",
       "      <td>-0.012546</td>\n",
       "      <td>-0.649617</td>\n",
       "      <td>1.577006</td>\n",
       "      <td>-0.414650</td>\n",
       "      <td>0.486180</td>\n",
       "      <td>...</td>\n",
       "      <td>0.382948</td>\n",
       "      <td>0.261057</td>\n",
       "      <td>0.643078</td>\n",
       "      <td>0.376777</td>\n",
       "      <td>0.008797</td>\n",
       "      <td>-0.473649</td>\n",
       "      <td>-0.818267</td>\n",
       "      <td>-0.002415</td>\n",
       "      <td>0.013649</td>\n",
       "      <td>0.514355</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>284807 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Time         V1         V2        V3        V4        V5  \\\n",
       "0            0.0  -1.359807  -0.072781  2.536347  1.378155 -0.338321   \n",
       "1            0.0   1.191857   0.266151  0.166480  0.448154  0.060018   \n",
       "2            1.0  -1.358354  -1.340163  1.773209  0.379780 -0.503198   \n",
       "3            1.0  -0.966272  -0.185226  1.792993 -0.863291 -0.010309   \n",
       "4            2.0  -1.158233   0.877737  1.548718  0.403034 -0.407193   \n",
       "...          ...        ...        ...       ...       ...       ...   \n",
       "284802  172786.0 -11.881118  10.071785 -9.834783 -2.066656 -5.364473   \n",
       "284803  172787.0  -0.732789  -0.055080  2.035030 -0.738589  0.868229   \n",
       "284804  172788.0   1.919565  -0.301254 -3.249640 -0.557828  2.630515   \n",
       "284805  172788.0  -0.240440   0.530483  0.702510  0.689799 -0.377961   \n",
       "284806  172792.0  -0.533413  -0.189733  0.703337 -0.506271 -0.012546   \n",
       "\n",
       "              V6        V7        V8        V9  ...       V20       V21  \\\n",
       "0       0.462388  0.239599  0.098698  0.363787  ...  0.251412 -0.018307   \n",
       "1      -0.082361 -0.078803  0.085102 -0.255425  ... -0.069083 -0.225775   \n",
       "2       1.800499  0.791461  0.247676 -1.514654  ...  0.524980  0.247998   \n",
       "3       1.247203  0.237609  0.377436 -1.387024  ... -0.208038 -0.108300   \n",
       "4       0.095921  0.592941 -0.270533  0.817739  ...  0.408542 -0.009431   \n",
       "...          ...       ...       ...       ...  ...       ...       ...   \n",
       "284802 -2.606837 -4.918215  7.305334  1.914428  ...  1.475829  0.213454   \n",
       "284803  1.058415  0.024330  0.294869  0.584800  ...  0.059616  0.214205   \n",
       "284804  3.031260 -0.296827  0.708417  0.432454  ...  0.001396  0.232045   \n",
       "284805  0.623708 -0.686180  0.679145  0.392087  ...  0.127434  0.265245   \n",
       "284806 -0.649617  1.577006 -0.414650  0.486180  ...  0.382948  0.261057   \n",
       "\n",
       "             V22       V23       V24       V25       V26       V27       V28  \\\n",
       "0       0.277838 -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053   \n",
       "1      -0.638672  0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724   \n",
       "2       0.771679  0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752   \n",
       "3       0.005274 -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458   \n",
       "4       0.798278 -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "284802  0.111864  1.014480 -0.509348  1.436807  0.250034  0.943651  0.823731   \n",
       "284803  0.924384  0.012463 -1.016226 -0.606624 -0.395255  0.068472 -0.053527   \n",
       "284804  0.578229 -0.037501  0.640134  0.265745 -0.087371  0.004455 -0.026561   \n",
       "284805  0.800049 -0.163298  0.123205 -0.569159  0.546668  0.108821  0.104533   \n",
       "284806  0.643078  0.376777  0.008797 -0.473649 -0.818267 -0.002415  0.013649   \n",
       "\n",
       "        normalAmount  \n",
       "0           0.244964  \n",
       "1          -0.342475  \n",
       "2           1.160686  \n",
       "3           0.140534  \n",
       "4          -0.073403  \n",
       "...              ...  \n",
       "284802     -0.350151  \n",
       "284803     -0.254117  \n",
       "284804     -0.081839  \n",
       "284805     -0.313249  \n",
       "284806      0.514355  \n",
       "\n",
       "[284807 rows x 30 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 필요없는 Amount column을 제거\n",
    "df_x = df_x.drop('Amount',axis = 1).copy()\n",
    "df_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a30337a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "163152.0    36\n",
       "64947.0     26\n",
       "68780.0     25\n",
       "3767.0      21\n",
       "3770.0      20\n",
       "            ..\n",
       "81790.0      1\n",
       "54289.0      1\n",
       "37651.0      1\n",
       "112892.0     1\n",
       "119665.0     1\n",
       "Name: Time, Length: 124592, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 이상치 확인\n",
    "# TIME column을 봤을때 변수의 편차가 크고 학습에 관련이 없는 column이라고 판단하여 삭제\n",
    "\n",
    "df_x['Time'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85640dc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>normalAmount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>0.090794</td>\n",
       "      <td>...</td>\n",
       "      <td>0.251412</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>0.244964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>-0.166974</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.069083</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>-0.342475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>0.207643</td>\n",
       "      <td>...</td>\n",
       "      <td>0.524980</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>1.160686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>-0.054952</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.208038</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>0.140534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>0.753074</td>\n",
       "      <td>...</td>\n",
       "      <td>0.408542</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>-0.073403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284802</th>\n",
       "      <td>-11.881118</td>\n",
       "      <td>10.071785</td>\n",
       "      <td>-9.834783</td>\n",
       "      <td>-2.066656</td>\n",
       "      <td>-5.364473</td>\n",
       "      <td>-2.606837</td>\n",
       "      <td>-4.918215</td>\n",
       "      <td>7.305334</td>\n",
       "      <td>1.914428</td>\n",
       "      <td>4.356170</td>\n",
       "      <td>...</td>\n",
       "      <td>1.475829</td>\n",
       "      <td>0.213454</td>\n",
       "      <td>0.111864</td>\n",
       "      <td>1.014480</td>\n",
       "      <td>-0.509348</td>\n",
       "      <td>1.436807</td>\n",
       "      <td>0.250034</td>\n",
       "      <td>0.943651</td>\n",
       "      <td>0.823731</td>\n",
       "      <td>-0.350151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284803</th>\n",
       "      <td>-0.732789</td>\n",
       "      <td>-0.055080</td>\n",
       "      <td>2.035030</td>\n",
       "      <td>-0.738589</td>\n",
       "      <td>0.868229</td>\n",
       "      <td>1.058415</td>\n",
       "      <td>0.024330</td>\n",
       "      <td>0.294869</td>\n",
       "      <td>0.584800</td>\n",
       "      <td>-0.975926</td>\n",
       "      <td>...</td>\n",
       "      <td>0.059616</td>\n",
       "      <td>0.214205</td>\n",
       "      <td>0.924384</td>\n",
       "      <td>0.012463</td>\n",
       "      <td>-1.016226</td>\n",
       "      <td>-0.606624</td>\n",
       "      <td>-0.395255</td>\n",
       "      <td>0.068472</td>\n",
       "      <td>-0.053527</td>\n",
       "      <td>-0.254117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284804</th>\n",
       "      <td>1.919565</td>\n",
       "      <td>-0.301254</td>\n",
       "      <td>-3.249640</td>\n",
       "      <td>-0.557828</td>\n",
       "      <td>2.630515</td>\n",
       "      <td>3.031260</td>\n",
       "      <td>-0.296827</td>\n",
       "      <td>0.708417</td>\n",
       "      <td>0.432454</td>\n",
       "      <td>-0.484782</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001396</td>\n",
       "      <td>0.232045</td>\n",
       "      <td>0.578229</td>\n",
       "      <td>-0.037501</td>\n",
       "      <td>0.640134</td>\n",
       "      <td>0.265745</td>\n",
       "      <td>-0.087371</td>\n",
       "      <td>0.004455</td>\n",
       "      <td>-0.026561</td>\n",
       "      <td>-0.081839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284805</th>\n",
       "      <td>-0.240440</td>\n",
       "      <td>0.530483</td>\n",
       "      <td>0.702510</td>\n",
       "      <td>0.689799</td>\n",
       "      <td>-0.377961</td>\n",
       "      <td>0.623708</td>\n",
       "      <td>-0.686180</td>\n",
       "      <td>0.679145</td>\n",
       "      <td>0.392087</td>\n",
       "      <td>-0.399126</td>\n",
       "      <td>...</td>\n",
       "      <td>0.127434</td>\n",
       "      <td>0.265245</td>\n",
       "      <td>0.800049</td>\n",
       "      <td>-0.163298</td>\n",
       "      <td>0.123205</td>\n",
       "      <td>-0.569159</td>\n",
       "      <td>0.546668</td>\n",
       "      <td>0.108821</td>\n",
       "      <td>0.104533</td>\n",
       "      <td>-0.313249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284806</th>\n",
       "      <td>-0.533413</td>\n",
       "      <td>-0.189733</td>\n",
       "      <td>0.703337</td>\n",
       "      <td>-0.506271</td>\n",
       "      <td>-0.012546</td>\n",
       "      <td>-0.649617</td>\n",
       "      <td>1.577006</td>\n",
       "      <td>-0.414650</td>\n",
       "      <td>0.486180</td>\n",
       "      <td>-0.915427</td>\n",
       "      <td>...</td>\n",
       "      <td>0.382948</td>\n",
       "      <td>0.261057</td>\n",
       "      <td>0.643078</td>\n",
       "      <td>0.376777</td>\n",
       "      <td>0.008797</td>\n",
       "      <td>-0.473649</td>\n",
       "      <td>-0.818267</td>\n",
       "      <td>-0.002415</td>\n",
       "      <td>0.013649</td>\n",
       "      <td>0.514355</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>284807 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               V1         V2        V3        V4        V5        V6  \\\n",
       "0       -1.359807  -0.072781  2.536347  1.378155 -0.338321  0.462388   \n",
       "1        1.191857   0.266151  0.166480  0.448154  0.060018 -0.082361   \n",
       "2       -1.358354  -1.340163  1.773209  0.379780 -0.503198  1.800499   \n",
       "3       -0.966272  -0.185226  1.792993 -0.863291 -0.010309  1.247203   \n",
       "4       -1.158233   0.877737  1.548718  0.403034 -0.407193  0.095921   \n",
       "...           ...        ...       ...       ...       ...       ...   \n",
       "284802 -11.881118  10.071785 -9.834783 -2.066656 -5.364473 -2.606837   \n",
       "284803  -0.732789  -0.055080  2.035030 -0.738589  0.868229  1.058415   \n",
       "284804   1.919565  -0.301254 -3.249640 -0.557828  2.630515  3.031260   \n",
       "284805  -0.240440   0.530483  0.702510  0.689799 -0.377961  0.623708   \n",
       "284806  -0.533413  -0.189733  0.703337 -0.506271 -0.012546 -0.649617   \n",
       "\n",
       "              V7        V8        V9       V10  ...       V20       V21  \\\n",
       "0       0.239599  0.098698  0.363787  0.090794  ...  0.251412 -0.018307   \n",
       "1      -0.078803  0.085102 -0.255425 -0.166974  ... -0.069083 -0.225775   \n",
       "2       0.791461  0.247676 -1.514654  0.207643  ...  0.524980  0.247998   \n",
       "3       0.237609  0.377436 -1.387024 -0.054952  ... -0.208038 -0.108300   \n",
       "4       0.592941 -0.270533  0.817739  0.753074  ...  0.408542 -0.009431   \n",
       "...          ...       ...       ...       ...  ...       ...       ...   \n",
       "284802 -4.918215  7.305334  1.914428  4.356170  ...  1.475829  0.213454   \n",
       "284803  0.024330  0.294869  0.584800 -0.975926  ...  0.059616  0.214205   \n",
       "284804 -0.296827  0.708417  0.432454 -0.484782  ...  0.001396  0.232045   \n",
       "284805 -0.686180  0.679145  0.392087 -0.399126  ...  0.127434  0.265245   \n",
       "284806  1.577006 -0.414650  0.486180 -0.915427  ...  0.382948  0.261057   \n",
       "\n",
       "             V22       V23       V24       V25       V26       V27       V28  \\\n",
       "0       0.277838 -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053   \n",
       "1      -0.638672  0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724   \n",
       "2       0.771679  0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752   \n",
       "3       0.005274 -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458   \n",
       "4       0.798278 -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "284802  0.111864  1.014480 -0.509348  1.436807  0.250034  0.943651  0.823731   \n",
       "284803  0.924384  0.012463 -1.016226 -0.606624 -0.395255  0.068472 -0.053527   \n",
       "284804  0.578229 -0.037501  0.640134  0.265745 -0.087371  0.004455 -0.026561   \n",
       "284805  0.800049 -0.163298  0.123205 -0.569159  0.546668  0.108821  0.104533   \n",
       "284806  0.643078  0.376777  0.008797 -0.473649 -0.818267 -0.002415  0.013649   \n",
       "\n",
       "        normalAmount  \n",
       "0           0.244964  \n",
       "1          -0.342475  \n",
       "2           1.160686  \n",
       "3           0.140534  \n",
       "4          -0.073403  \n",
       "...              ...  \n",
       "284802     -0.350151  \n",
       "284803     -0.254117  \n",
       "284804     -0.081839  \n",
       "284805     -0.313249  \n",
       "284806      0.514355  \n",
       "\n",
       "[284807 rows x 29 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_x = df_x.drop('Time',axis = 1).copy()\n",
    "df_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1545b165",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>normalAmount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>0.090794</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>0.244964</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>-0.166974</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>-0.342475</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>0.207643</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>1.160686</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>-0.054952</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>0.140534</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>0.753074</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>-0.073403</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284802</th>\n",
       "      <td>-11.881118</td>\n",
       "      <td>10.071785</td>\n",
       "      <td>-9.834783</td>\n",
       "      <td>-2.066656</td>\n",
       "      <td>-5.364473</td>\n",
       "      <td>-2.606837</td>\n",
       "      <td>-4.918215</td>\n",
       "      <td>7.305334</td>\n",
       "      <td>1.914428</td>\n",
       "      <td>4.356170</td>\n",
       "      <td>...</td>\n",
       "      <td>0.213454</td>\n",
       "      <td>0.111864</td>\n",
       "      <td>1.014480</td>\n",
       "      <td>-0.509348</td>\n",
       "      <td>1.436807</td>\n",
       "      <td>0.250034</td>\n",
       "      <td>0.943651</td>\n",
       "      <td>0.823731</td>\n",
       "      <td>-0.350151</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284803</th>\n",
       "      <td>-0.732789</td>\n",
       "      <td>-0.055080</td>\n",
       "      <td>2.035030</td>\n",
       "      <td>-0.738589</td>\n",
       "      <td>0.868229</td>\n",
       "      <td>1.058415</td>\n",
       "      <td>0.024330</td>\n",
       "      <td>0.294869</td>\n",
       "      <td>0.584800</td>\n",
       "      <td>-0.975926</td>\n",
       "      <td>...</td>\n",
       "      <td>0.214205</td>\n",
       "      <td>0.924384</td>\n",
       "      <td>0.012463</td>\n",
       "      <td>-1.016226</td>\n",
       "      <td>-0.606624</td>\n",
       "      <td>-0.395255</td>\n",
       "      <td>0.068472</td>\n",
       "      <td>-0.053527</td>\n",
       "      <td>-0.254117</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284804</th>\n",
       "      <td>1.919565</td>\n",
       "      <td>-0.301254</td>\n",
       "      <td>-3.249640</td>\n",
       "      <td>-0.557828</td>\n",
       "      <td>2.630515</td>\n",
       "      <td>3.031260</td>\n",
       "      <td>-0.296827</td>\n",
       "      <td>0.708417</td>\n",
       "      <td>0.432454</td>\n",
       "      <td>-0.484782</td>\n",
       "      <td>...</td>\n",
       "      <td>0.232045</td>\n",
       "      <td>0.578229</td>\n",
       "      <td>-0.037501</td>\n",
       "      <td>0.640134</td>\n",
       "      <td>0.265745</td>\n",
       "      <td>-0.087371</td>\n",
       "      <td>0.004455</td>\n",
       "      <td>-0.026561</td>\n",
       "      <td>-0.081839</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284805</th>\n",
       "      <td>-0.240440</td>\n",
       "      <td>0.530483</td>\n",
       "      <td>0.702510</td>\n",
       "      <td>0.689799</td>\n",
       "      <td>-0.377961</td>\n",
       "      <td>0.623708</td>\n",
       "      <td>-0.686180</td>\n",
       "      <td>0.679145</td>\n",
       "      <td>0.392087</td>\n",
       "      <td>-0.399126</td>\n",
       "      <td>...</td>\n",
       "      <td>0.265245</td>\n",
       "      <td>0.800049</td>\n",
       "      <td>-0.163298</td>\n",
       "      <td>0.123205</td>\n",
       "      <td>-0.569159</td>\n",
       "      <td>0.546668</td>\n",
       "      <td>0.108821</td>\n",
       "      <td>0.104533</td>\n",
       "      <td>-0.313249</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284806</th>\n",
       "      <td>-0.533413</td>\n",
       "      <td>-0.189733</td>\n",
       "      <td>0.703337</td>\n",
       "      <td>-0.506271</td>\n",
       "      <td>-0.012546</td>\n",
       "      <td>-0.649617</td>\n",
       "      <td>1.577006</td>\n",
       "      <td>-0.414650</td>\n",
       "      <td>0.486180</td>\n",
       "      <td>-0.915427</td>\n",
       "      <td>...</td>\n",
       "      <td>0.261057</td>\n",
       "      <td>0.643078</td>\n",
       "      <td>0.376777</td>\n",
       "      <td>0.008797</td>\n",
       "      <td>-0.473649</td>\n",
       "      <td>-0.818267</td>\n",
       "      <td>-0.002415</td>\n",
       "      <td>0.013649</td>\n",
       "      <td>0.514355</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>284807 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               V1         V2        V3        V4        V5        V6  \\\n",
       "0       -1.359807  -0.072781  2.536347  1.378155 -0.338321  0.462388   \n",
       "1        1.191857   0.266151  0.166480  0.448154  0.060018 -0.082361   \n",
       "2       -1.358354  -1.340163  1.773209  0.379780 -0.503198  1.800499   \n",
       "3       -0.966272  -0.185226  1.792993 -0.863291 -0.010309  1.247203   \n",
       "4       -1.158233   0.877737  1.548718  0.403034 -0.407193  0.095921   \n",
       "...           ...        ...       ...       ...       ...       ...   \n",
       "284802 -11.881118  10.071785 -9.834783 -2.066656 -5.364473 -2.606837   \n",
       "284803  -0.732789  -0.055080  2.035030 -0.738589  0.868229  1.058415   \n",
       "284804   1.919565  -0.301254 -3.249640 -0.557828  2.630515  3.031260   \n",
       "284805  -0.240440   0.530483  0.702510  0.689799 -0.377961  0.623708   \n",
       "284806  -0.533413  -0.189733  0.703337 -0.506271 -0.012546 -0.649617   \n",
       "\n",
       "              V7        V8        V9       V10  ...       V21       V22  \\\n",
       "0       0.239599  0.098698  0.363787  0.090794  ... -0.018307  0.277838   \n",
       "1      -0.078803  0.085102 -0.255425 -0.166974  ... -0.225775 -0.638672   \n",
       "2       0.791461  0.247676 -1.514654  0.207643  ...  0.247998  0.771679   \n",
       "3       0.237609  0.377436 -1.387024 -0.054952  ... -0.108300  0.005274   \n",
       "4       0.592941 -0.270533  0.817739  0.753074  ... -0.009431  0.798278   \n",
       "...          ...       ...       ...       ...  ...       ...       ...   \n",
       "284802 -4.918215  7.305334  1.914428  4.356170  ...  0.213454  0.111864   \n",
       "284803  0.024330  0.294869  0.584800 -0.975926  ...  0.214205  0.924384   \n",
       "284804 -0.296827  0.708417  0.432454 -0.484782  ...  0.232045  0.578229   \n",
       "284805 -0.686180  0.679145  0.392087 -0.399126  ...  0.265245  0.800049   \n",
       "284806  1.577006 -0.414650  0.486180 -0.915427  ...  0.261057  0.643078   \n",
       "\n",
       "             V23       V24       V25       V26       V27       V28  \\\n",
       "0      -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053   \n",
       "1       0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724   \n",
       "2       0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752   \n",
       "3      -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458   \n",
       "4      -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153   \n",
       "...          ...       ...       ...       ...       ...       ...   \n",
       "284802  1.014480 -0.509348  1.436807  0.250034  0.943651  0.823731   \n",
       "284803  0.012463 -1.016226 -0.606624 -0.395255  0.068472 -0.053527   \n",
       "284804 -0.037501  0.640134  0.265745 -0.087371  0.004455 -0.026561   \n",
       "284805 -0.163298  0.123205 -0.569159  0.546668  0.108821  0.104533   \n",
       "284806  0.376777  0.008797 -0.473649 -0.818267 -0.002415  0.013649   \n",
       "\n",
       "        normalAmount  Class  \n",
       "0           0.244964      0  \n",
       "1          -0.342475      0  \n",
       "2           1.160686      0  \n",
       "3           0.140534      0  \n",
       "4          -0.073403      0  \n",
       "...              ...    ...  \n",
       "284802     -0.350151      0  \n",
       "284803     -0.254117      0  \n",
       "284804     -0.081839      0  \n",
       "284805     -0.313249      0  \n",
       "284806      0.514355      0  \n",
       "\n",
       "[284807 rows x 30 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 위의 데이터프레임과 class column만 있는 데이터프레임을 합침\n",
    "\n",
    "df = pd.concat([df_x, df_y],axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a783e1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 프레임을 csv 파일로 저장 후 다시 불러옴\n",
    "# 위의 전처리 과정을 반복하지 않기 위해 csv 파일로 저장 후 불러오는 것\n",
    "# index_col을 0으로 설정시 인덱스 값 생성X\n",
    "\n",
    "df.to_csv('credit_test.csv')\n",
    "df = pd.read_csv('credit_test.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "18b3330d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>normalAmount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>0.090794</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>0.244964</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>-0.166974</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>-0.342475</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>0.207643</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>1.160686</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>-0.054952</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>0.140534</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>0.753074</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>-0.073403</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9       V10  ...       V21       V22       V23       V24  \\\n",
       "0  0.098698  0.363787  0.090794  ... -0.018307  0.277838 -0.110474  0.066928   \n",
       "1  0.085102 -0.255425 -0.166974  ... -0.225775 -0.638672  0.101288 -0.339846   \n",
       "2  0.247676 -1.514654  0.207643  ...  0.247998  0.771679  0.909412 -0.689281   \n",
       "3  0.377436 -1.387024 -0.054952  ... -0.108300  0.005274 -0.190321 -1.175575   \n",
       "4 -0.270533  0.817739  0.753074  ... -0.009431  0.798278 -0.137458  0.141267   \n",
       "\n",
       "        V25       V26       V27       V28  normalAmount  Class  \n",
       "0  0.128539 -0.189115  0.133558 -0.021053      0.244964      0  \n",
       "1  0.167170  0.125895 -0.008983  0.014724     -0.342475      0  \n",
       "2 -0.327642 -0.139097 -0.055353 -0.059752      1.160686      0  \n",
       "3  0.647376 -0.221929  0.062723  0.061458      0.140534      0  \n",
       "4 -0.206010  0.502292  0.219422  0.215153     -0.073403      0  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf341a4",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1d7960",
   "metadata": {},
   "source": [
    "# Train set : Test set = 85 : 15 (4번)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8150d2b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(242085, 29)\n",
      "(42722, 29)\n"
     ]
    }
   ],
   "source": [
    "# 데이터를 train set 과 test set으로 나눔\n",
    "\n",
    "X =df.values[:,0:29] #독립변수\n",
    "Y =df.values[:,29] #종속변수\n",
    "X_train, X_test, Y_train, Y_test= train_test_split(X,Y, test_size = 0.15) #85 : 15니까\n",
    "print(X_train.shape) # 85프로\n",
    "print(X_test.shape) #15프로"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f938800",
   "metadata": {},
   "source": [
    "# DNN 학습 및 저장 / 정확도, Loss 그래프"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ce077f87",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "163/163 [==============================] - 1s 6ms/step - loss: 0.4658 - accuracy: 0.8723 - val_loss: 0.1492 - val_accuracy: 0.9981\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.14923, saving model to ./credit\\01-0.1492.hdf5\n",
      "Epoch 2/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.1186 - accuracy: 0.9984 - val_loss: 0.0660 - val_accuracy: 0.9981\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.14923 to 0.06598, saving model to ./credit\\02-0.0660.hdf5\n",
      "Epoch 3/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0581 - accuracy: 0.9983 - val_loss: 0.0414 - val_accuracy: 0.9981\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.06598 to 0.04144, saving model to ./credit\\03-0.0414.hdf5\n",
      "Epoch 4/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0374 - accuracy: 0.9986 - val_loss: 0.0296 - val_accuracy: 0.9990\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.04144 to 0.02964, saving model to ./credit\\04-0.0296.hdf5\n",
      "Epoch 5/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0273 - accuracy: 0.9992 - val_loss: 0.0227 - val_accuracy: 0.9992\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.02964 to 0.02273, saving model to ./credit\\05-0.0227.hdf5\n",
      "Epoch 6/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0214 - accuracy: 0.9993 - val_loss: 0.0183 - val_accuracy: 0.9992\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.02273 to 0.01828, saving model to ./credit\\06-0.0183.hdf5\n",
      "Epoch 7/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0172 - accuracy: 0.9994 - val_loss: 0.0152 - val_accuracy: 0.9993\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.01828 to 0.01518, saving model to ./credit\\07-0.0152.hdf5\n",
      "Epoch 8/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0147 - accuracy: 0.9992 - val_loss: 0.0130 - val_accuracy: 0.9993\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.01518 to 0.01297, saving model to ./credit\\08-0.0130.hdf5\n",
      "Epoch 9/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0121 - accuracy: 0.9994 - val_loss: 0.0113 - val_accuracy: 0.9993\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.01297 to 0.01130, saving model to ./credit\\09-0.0113.hdf5\n",
      "Epoch 10/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0107 - accuracy: 0.9994 - val_loss: 0.0100 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.01130 to 0.01002, saving model to ./credit\\10-0.0100.hdf5\n",
      "Epoch 11/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0091 - accuracy: 0.9995 - val_loss: 0.0090 - val_accuracy: 0.9993\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.01002 to 0.00904, saving model to ./credit\\11-0.0090.hdf5\n",
      "Epoch 12/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0082 - accuracy: 0.9994 - val_loss: 0.0082 - val_accuracy: 0.9993\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.00904 to 0.00822, saving model to ./credit\\12-0.0082.hdf5\n",
      "Epoch 13/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0070 - accuracy: 0.9995 - val_loss: 0.0076 - val_accuracy: 0.9993\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.00822 to 0.00763, saving model to ./credit\\13-0.0076.hdf5\n",
      "Epoch 14/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0072 - accuracy: 0.9994 - val_loss: 0.0071 - val_accuracy: 0.9993\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.00763 to 0.00709, saving model to ./credit\\14-0.0071.hdf5\n",
      "Epoch 15/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0065 - accuracy: 0.9994 - val_loss: 0.0066 - val_accuracy: 0.9993\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.00709 to 0.00660, saving model to ./credit\\15-0.0066.hdf5\n",
      "Epoch 16/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0059 - accuracy: 0.9995 - val_loss: 0.0062 - val_accuracy: 0.9993\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.00660 to 0.00621, saving model to ./credit\\16-0.0062.hdf5\n",
      "Epoch 17/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0055 - accuracy: 0.9994 - val_loss: 0.0059 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.00621 to 0.00585, saving model to ./credit\\17-0.0059.hdf5\n",
      "Epoch 18/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0050 - accuracy: 0.9995 - val_loss: 0.0056 - val_accuracy: 0.9993\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.00585 to 0.00555, saving model to ./credit\\18-0.0056.hdf5\n",
      "Epoch 19/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0049 - accuracy: 0.9995 - val_loss: 0.0053 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.00555 to 0.00529, saving model to ./credit\\19-0.0053.hdf5\n",
      "Epoch 20/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0042 - accuracy: 0.9996 - val_loss: 0.0050 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.00529 to 0.00498, saving model to ./credit\\20-0.0050.hdf5\n",
      "Epoch 21/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0042 - accuracy: 0.9995 - val_loss: 0.0050 - val_accuracy: 0.9993\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.00498\n",
      "Epoch 22/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0041 - accuracy: 0.9995 - val_loss: 0.0046 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.00498 to 0.00461, saving model to ./credit\\22-0.0046.hdf5\n",
      "Epoch 23/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0037 - accuracy: 0.9995 - val_loss: 0.0045 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.00461 to 0.00446, saving model to ./credit\\23-0.0045.hdf5\n",
      "Epoch 24/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0038 - accuracy: 0.9994 - val_loss: 0.0043 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.00446 to 0.00431, saving model to ./credit\\24-0.0043.hdf5\n",
      "Epoch 25/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0034 - accuracy: 0.9996 - val_loss: 0.0043 - val_accuracy: 0.9993\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.00431 to 0.00431, saving model to ./credit\\25-0.0043.hdf5\n",
      "Epoch 26/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0031 - accuracy: 0.9996 - val_loss: 0.0043 - val_accuracy: 0.9993\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.00431 to 0.00427, saving model to ./credit\\26-0.0043.hdf5\n",
      "Epoch 27/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0029 - accuracy: 0.9996 - val_loss: 0.0039 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.00427 to 0.00395, saving model to ./credit\\27-0.0039.hdf5\n",
      "Epoch 28/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0030 - accuracy: 0.9996 - val_loss: 0.0038 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.00395 to 0.00385, saving model to ./credit\\28-0.0038.hdf5\n",
      "Epoch 29/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0029 - accuracy: 0.9995 - val_loss: 0.0039 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.00385\n",
      "Epoch 30/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0027 - accuracy: 0.9996 - val_loss: 0.0037 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.00385 to 0.00368, saving model to ./credit\\30-0.0037.hdf5\n",
      "Epoch 31/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0028 - accuracy: 0.9995 - val_loss: 0.0036 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.00368 to 0.00364, saving model to ./credit\\31-0.0036.hdf5\n",
      "Epoch 32/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0032 - accuracy: 0.9995 - val_loss: 0.0036 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.00364 to 0.00361, saving model to ./credit\\32-0.0036.hdf5\n",
      "Epoch 33/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0026 - accuracy: 0.9997 - val_loss: 0.0036 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.00361 to 0.00357, saving model to ./credit\\33-0.0036.hdf5\n",
      "Epoch 34/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0027 - accuracy: 0.9996 - val_loss: 0.0036 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.00357\n",
      "Epoch 35/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0022 - accuracy: 0.9997 - val_loss: 0.0035 - val_accuracy: 0.9995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00035: val_loss improved from 0.00357 to 0.00347, saving model to ./credit\\35-0.0035.hdf5\n",
      "Epoch 36/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0027 - accuracy: 0.9996 - val_loss: 0.0035 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.00347\n",
      "Epoch 37/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0022 - accuracy: 0.9996 - val_loss: 0.0034 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.00347 to 0.00340, saving model to ./credit\\37-0.0034.hdf5\n",
      "Epoch 38/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0023 - accuracy: 0.9996 - val_loss: 0.0034 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.00340\n",
      "Epoch 39/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0019 - accuracy: 0.9997 - val_loss: 0.0033 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.00340 to 0.00334, saving model to ./credit\\39-0.0033.hdf5\n",
      "Epoch 40/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0020 - accuracy: 0.9997 - val_loss: 0.0033 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.00334 to 0.00334, saving model to ./credit\\40-0.0033.hdf5\n",
      "Epoch 41/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0021 - accuracy: 0.9997 - val_loss: 0.0034 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.00334\n",
      "Epoch 42/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0019 - accuracy: 0.9997 - val_loss: 0.0033 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.00334 to 0.00330, saving model to ./credit\\42-0.0033.hdf5\n",
      "Epoch 43/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0020 - accuracy: 0.9997 - val_loss: 0.0033 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.00330\n",
      "Epoch 44/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0021 - accuracy: 0.9997 - val_loss: 0.0034 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.00330\n",
      "Epoch 45/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0024 - accuracy: 0.9997 - val_loss: 0.0034 - val_accuracy: 0.9993\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.00330\n",
      "Epoch 46/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0019 - accuracy: 0.9997 - val_loss: 0.0033 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.00330\n",
      "Epoch 47/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0019 - accuracy: 0.9997 - val_loss: 0.0034 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.00330\n",
      "Epoch 48/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0016 - accuracy: 0.9997 - val_loss: 0.0033 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.00330 to 0.00330, saving model to ./credit\\48-0.0033.hdf5\n",
      "Epoch 49/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0017 - accuracy: 0.9998 - val_loss: 0.0035 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.00330\n",
      "Epoch 50/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0019 - accuracy: 0.9997 - val_loss: 0.0034 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.00330\n",
      "Epoch 51/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0022 - accuracy: 0.9997 - val_loss: 0.0033 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.00330\n",
      "Epoch 52/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0018 - accuracy: 0.9997 - val_loss: 0.0035 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.00330\n",
      "Epoch 53/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0017 - accuracy: 0.9997 - val_loss: 0.0034 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.00330\n",
      "Epoch 54/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0016 - accuracy: 0.9998 - val_loss: 0.0033 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.00330\n",
      "Epoch 55/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 0.0033 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.00330\n",
      "Epoch 56/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0012 - accuracy: 0.9998 - val_loss: 0.0034 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.00330\n",
      "Epoch 57/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0017 - accuracy: 0.9997 - val_loss: 0.0032 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.00330 to 0.00324, saving model to ./credit\\57-0.0032.hdf5\n",
      "Epoch 58/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0018 - accuracy: 0.9997 - val_loss: 0.0034 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.00324\n",
      "Epoch 59/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0020 - accuracy: 0.9997 - val_loss: 0.0034 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.00324\n",
      "Epoch 60/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0015 - accuracy: 0.9998 - val_loss: 0.0033 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.00324\n",
      "Epoch 61/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 0.0034 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.00324\n",
      "Epoch 62/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0017 - accuracy: 0.9997 - val_loss: 0.0033 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.00324\n",
      "Epoch 63/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0012 - accuracy: 0.9998 - val_loss: 0.0034 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.00324\n",
      "Epoch 64/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0015 - accuracy: 0.9998 - val_loss: 0.0034 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.00324\n",
      "Epoch 65/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0014 - accuracy: 0.9998 - val_loss: 0.0034 - val_accuracy: 0.9993\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.00324\n",
      "Epoch 66/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 0.0036 - val_accuracy: 0.9993\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.00324\n",
      "Epoch 67/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0010 - accuracy: 0.9999 - val_loss: 0.0033 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.00324\n",
      "Epoch 68/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.0035 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.00324\n",
      "Epoch 69/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.0035 - val_accuracy: 0.9993\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.00324\n",
      "Epoch 70/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0012 - accuracy: 0.9998 - val_loss: 0.0034 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.00324\n",
      "Epoch 71/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 0.0036 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.00324\n",
      "Epoch 72/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 0.0038 - val_accuracy: 0.9993\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.00324\n",
      "Epoch 73/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.0036 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.00324\n",
      "Epoch 74/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0015 - accuracy: 0.9997 - val_loss: 0.0035 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.00324\n",
      "Epoch 75/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.0036 - val_accuracy: 0.9994\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00075: val_loss did not improve from 0.00324\n",
      "Epoch 76/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0017 - accuracy: 0.9998 - val_loss: 0.0038 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.00324\n",
      "Epoch 77/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.0037 - val_accuracy: 0.9993\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.00324\n",
      "Epoch 78/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 0.0037 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.00324\n",
      "Epoch 79/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.0036 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.00324\n",
      "Epoch 80/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0012 - accuracy: 0.9998 - val_loss: 0.0038 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.00324\n",
      "Epoch 81/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 9.2583e-04 - accuracy: 0.9998 - val_loss: 0.0036 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.00324\n",
      "Epoch 82/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0012 - accuracy: 0.9998 - val_loss: 0.0037 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.00324\n",
      "Epoch 83/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 9.6597e-04 - accuracy: 0.9998 - val_loss: 0.0037 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.00324\n",
      "Epoch 84/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0010 - accuracy: 0.9998 - val_loss: 0.0037 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.00324\n",
      "Epoch 85/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.0038 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.00324\n",
      "Epoch 86/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 9.1298e-04 - accuracy: 0.9998 - val_loss: 0.0037 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.00324\n",
      "Epoch 87/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.0037 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.00324\n",
      "Epoch 88/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0014 - accuracy: 0.9998 - val_loss: 0.0038 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.00324\n",
      "Epoch 89/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0012 - accuracy: 0.9998 - val_loss: 0.0037 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.00324\n",
      "Epoch 90/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0012 - accuracy: 0.9998 - val_loss: 0.0037 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.00324\n",
      "Epoch 91/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 9.7099e-04 - accuracy: 0.9998 - val_loss: 0.0039 - val_accuracy: 0.9993\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.00324\n",
      "Epoch 92/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 0.0038 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.00324\n",
      "Epoch 93/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 8.7939e-04 - accuracy: 0.9998 - val_loss: 0.0039 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.00324\n",
      "Epoch 94/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0010 - accuracy: 0.9998 - val_loss: 0.0039 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.00324\n",
      "Epoch 95/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.0039 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.00324\n",
      "Epoch 96/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0012 - accuracy: 0.9998 - val_loss: 0.0039 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.00324\n",
      "Epoch 97/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 9.8079e-04 - accuracy: 0.9999 - val_loss: 0.0037 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.00324\n",
      "Epoch 98/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0012 - accuracy: 0.9998 - val_loss: 0.0039 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.00324\n",
      "Epoch 99/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0014 - accuracy: 0.9998 - val_loss: 0.0038 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.00324\n",
      "Epoch 100/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0010 - accuracy: 0.9998 - val_loss: 0.0040 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.00324\n",
      "Epoch 101/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 8.7665e-04 - accuracy: 0.9998 - val_loss: 0.0041 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.00324\n",
      "Epoch 102/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 9.6250e-04 - accuracy: 0.9999 - val_loss: 0.0040 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.00324\n",
      "Epoch 103/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0010 - accuracy: 0.9998 - val_loss: 0.0040 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.00324\n",
      "Epoch 104/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 9.8009e-04 - accuracy: 0.9998 - val_loss: 0.0041 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.00324\n",
      "Epoch 105/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0010 - accuracy: 0.9998 - val_loss: 0.0041 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.00324\n",
      "Epoch 106/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0010 - accuracy: 0.9998 - val_loss: 0.0039 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.00324\n",
      "Epoch 107/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 9.1905e-04 - accuracy: 0.9999 - val_loss: 0.0040 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.00324\n",
      "Epoch 108/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 0.0040 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.00324\n",
      "Epoch 109/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 7.8123e-04 - accuracy: 0.9999 - val_loss: 0.0040 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.00324\n",
      "Epoch 110/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 6.7525e-04 - accuracy: 0.9999 - val_loss: 0.0040 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.00324\n",
      "Epoch 111/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 5.5544e-04 - accuracy: 0.9999 - val_loss: 0.0039 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.00324\n",
      "Epoch 112/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 5.4597e-04 - accuracy: 0.9999 - val_loss: 0.0040 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.00324\n",
      "Epoch 113/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 8.3560e-04 - accuracy: 0.9999 - val_loss: 0.0040 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.00324\n",
      "Epoch 114/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 9.8458e-04 - accuracy: 0.9998 - val_loss: 0.0041 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.00324\n",
      "Epoch 115/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 7.8905e-04 - accuracy: 0.9999 - val_loss: 0.0041 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.00324\n",
      "Epoch 116/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 9.0065e-04 - accuracy: 0.9998 - val_loss: 0.0039 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.00324\n",
      "Epoch 117/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163/163 [==============================] - 0s 1ms/step - loss: 9.6739e-04 - accuracy: 0.9998 - val_loss: 0.0040 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.00324\n",
      "Epoch 118/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 9.7829e-04 - accuracy: 0.9998 - val_loss: 0.0039 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.00324\n",
      "Epoch 119/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 8.2471e-04 - accuracy: 0.9999 - val_loss: 0.0038 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.00324\n",
      "Epoch 120/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 6.2898e-04 - accuracy: 0.9999 - val_loss: 0.0038 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.00324\n",
      "Epoch 121/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 7.8326e-04 - accuracy: 0.9998 - val_loss: 0.0041 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.00324\n",
      "Epoch 122/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 8.9743e-04 - accuracy: 0.9998 - val_loss: 0.0038 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.00324\n",
      "Epoch 123/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0012 - accuracy: 0.9998 - val_loss: 0.0040 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.00324\n",
      "Epoch 124/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 7.8656e-04 - accuracy: 0.9998 - val_loss: 0.0039 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.00324\n",
      "Epoch 125/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0010 - accuracy: 0.9999 - val_loss: 0.0040 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.00324\n",
      "Epoch 126/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 8.0795e-04 - accuracy: 0.9999 - val_loss: 0.0039 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.00324\n",
      "Epoch 127/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 7.9906e-04 - accuracy: 0.9999 - val_loss: 0.0037 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.00324\n",
      "Epoch 128/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 7.1634e-04 - accuracy: 0.9999 - val_loss: 0.0040 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.00324\n",
      "Epoch 129/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 8.2129e-04 - accuracy: 0.9999 - val_loss: 0.0039 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.00324\n",
      "Epoch 130/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 9.5619e-04 - accuracy: 0.9999 - val_loss: 0.0039 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.00324\n",
      "Epoch 131/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.0038 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.00324\n",
      "Epoch 132/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 7.9946e-04 - accuracy: 0.9999 - val_loss: 0.0038 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.00324\n",
      "Epoch 133/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0015 - accuracy: 0.9997 - val_loss: 0.0038 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.00324\n",
      "Epoch 134/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 9.0277e-04 - accuracy: 0.9998 - val_loss: 0.0041 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.00324\n",
      "Epoch 135/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 9.3635e-04 - accuracy: 0.9998 - val_loss: 0.0039 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.00324\n",
      "Epoch 136/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 9.3944e-04 - accuracy: 0.9998 - val_loss: 0.0040 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.00324\n",
      "Epoch 137/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0010 - accuracy: 0.9998 - val_loss: 0.0042 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.00324\n",
      "Epoch 138/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 8.0317e-04 - accuracy: 0.9999 - val_loss: 0.0039 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.00324\n",
      "Epoch 139/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 8.2191e-04 - accuracy: 0.9998 - val_loss: 0.0038 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.00324\n",
      "Epoch 140/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 5.5092e-04 - accuracy: 0.9999 - val_loss: 0.0038 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.00324\n",
      "Epoch 141/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 8.2279e-04 - accuracy: 0.9999 - val_loss: 0.0041 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.00324\n",
      "Epoch 142/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 6.1531e-04 - accuracy: 0.9999 - val_loss: 0.0040 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.00324\n",
      "Epoch 143/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 6.0090e-04 - accuracy: 0.9999 - val_loss: 0.0041 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.00324\n",
      "Epoch 144/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 8.0852e-04 - accuracy: 0.9998 - val_loss: 0.0040 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.00324\n",
      "Epoch 145/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 9.4011e-04 - accuracy: 0.9998 - val_loss: 0.0041 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.00324\n",
      "Epoch 146/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 6.6238e-04 - accuracy: 0.9999 - val_loss: 0.0042 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.00324\n",
      "Epoch 147/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 5.6351e-04 - accuracy: 0.9999 - val_loss: 0.0042 - val_accuracy: 0.9993\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.00324\n",
      "Epoch 148/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 6.9874e-04 - accuracy: 0.9999 - val_loss: 0.0040 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.00324\n",
      "Epoch 149/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 8.6532e-04 - accuracy: 0.9999 - val_loss: 0.0039 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.00324\n",
      "Epoch 150/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 7.4798e-04 - accuracy: 0.9999 - val_loss: 0.0039 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.00324\n",
      "Epoch 151/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 8.4727e-04 - accuracy: 0.9998 - val_loss: 0.0042 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.00324\n",
      "Epoch 152/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 6.7673e-04 - accuracy: 0.9999 - val_loss: 0.0040 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.00324\n",
      "Epoch 153/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 8.7720e-04 - accuracy: 0.9998 - val_loss: 0.0044 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.00324\n",
      "Epoch 154/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 9.5839e-04 - accuracy: 0.9998 - val_loss: 0.0041 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.00324\n",
      "Epoch 155/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 8.8082e-04 - accuracy: 0.9998 - val_loss: 0.0040 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.00324\n",
      "Epoch 156/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0010 - accuracy: 0.9998 - val_loss: 0.0041 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.00324\n",
      "Epoch 157/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 6.5241e-04 - accuracy: 0.9999 - val_loss: 0.0041 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.00324\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 14)                420       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 7)                 105       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 32        \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 562\n",
      "Trainable params: 562\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 모델 설정\n",
    "# activation function으로 탄젠트 H, relum sigmoid를 사용\n",
    "# 탄젠트h는 sigmoid 함수를 뒤집은 모양이라 사용해봄\n",
    "model = Sequential()\n",
    "model.add(Dense(14, activation='tanh', input_dim=X.shape[1]))\n",
    "model.add(Dense(7, activation='relu'))\n",
    "model.add(Dense(4, activation='tanh'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# 모델 컴파일\n",
    "# 종속변수가 2개로 나뉘는 이진분류기 때문에 loss 함수로 binary_crossentropy를 사용\n",
    "model.compile(loss='binary_crossentropy',\n",
    "             optimizer='adam',\n",
    "             metrics= ['accuracy'])\n",
    "\n",
    "# 모델 저장 폴더 지정\n",
    "MODEL_DIR='./credit/'\n",
    "if not os.path.exists(MODEL_DIR):\n",
    "    os.mkdir(MODEL_DIR)\n",
    "\n",
    "# 모델 저장 방법\n",
    "modelpath = './credit/{epoch:02d}-{val_loss:.4f}.hdf5'\n",
    "checkpointer =  ModelCheckpoint(filepath=modelpath, monitor='val_loss', verbose=1, save_best_only=True)\n",
    "    \n",
    "# 학습 조기 종료\n",
    "early_stopping_callback= EarlyStopping(monitor='val_loss', patience=100)\n",
    "\n",
    "\n",
    "# 모델 학습\n",
    "# batch_size의 크기를 크게하니 학습 속도가 빨리짐\n",
    "history = model.fit(X_train, Y_train ,validation_split=0.33, epochs=1000, batch_size=1000,\n",
    "                    callbacks=[checkpointer, early_stopping_callback])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0b2cdde9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD4CAYAAAD4k815AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Z1A+gAAAACXBIWXMAAAsTAAALEwEAmpwYAAASSklEQVR4nO3df6zd913f8efLdh0KdE0bu5DFdu0id5q1sSW9lFgdYEgLTlQlQmPDWausS1ar3YIYVKCETmGEP0pbhAYiNA2QlJS2IXSlWJ2raAvxkIYbcoNpyA8MJk0bh2ZxMwgSVeu4fvPH93ub45v74/j63HvO+fT5kK7O+f7IOS99nPM63/v5fu85qSokSe1YN+4AkqTRstglqTEWuyQ1xmKXpMZY7JLUmA3jeuJNmzbV9u3bx/X0kjSVHnzwwS9V1eal9hlbsW/fvp3Z2dlxPb0kTaUkn19uH6diJKkxFrskNcZil6TGWOyS1BiLXZIas2yxJ7k9yTNJHl5ke5L8SpJjSR5KcsnoY0qShjXM5Y4fAn4VuHOR7ZcDO/uf7wY+0N+uqcOH4dAhuOACePbZ1b09cqR7zosvXv3nainbNGQ0W7sZJy3bnj2we/fq9OGyxV5Vf5hk+xK7XAXcWd3n/34myflJLqyqL44q5EIGi/zIEbjjDnj+eTh9GhKoWr3bQav9XC1lm4aMZms34yRlW7cOzjsP7r13dcp9FH+gdBHw5MDy8X7di4o9yX5gP8C2bdtW/ISHD8Nll8FXv3pmkc+Zu79at4NW+7layjYNGc3WbsZJynb6NJw82R2crkaxr+nJ06q6rapmqmpm8+Yl/yJ2SYcOdYNy+vTc4565PVnd27V8rpayTUNGs7WbcZKyrVsHGzd20zGrYRRH7E8BWweWt/TrVs2ePd2gzB2xr1sHGzbAtdeuzfzZpM3VTUu2achotnYzTlq2PXvGOMc+hAPA9Unuojtp+txqz6/v3t3NTR06tDaDJEnTZNliT/IxYA+wKclx4GeBlwBU1a3AQeAK4BjwZeA/rFbYQbt3W+SStJBhroq5epntBfznkSWSJJ0T//JUkhozdcV++DC85z3drSTpxcb2RRsrMXf9+smT3VUxq3VxvyRNs6k6Yj90qCv1r33thYv7JUlnmqpin7t+ff361b24X5Km2VRNxQxev+5165K0sKkqdvD6dUlazlRNxUiSlmexS1JjLHZJaozFLkmNsdglqTEWuyQ1xmKXpMZY7JLUGItdkhpjsUtSYyx2SWqMxS5JjbHYJakxFrskNcZil6TGWOyS1BiLXZIaY7FLUmMsdklqjMUuSY2x2CWpMRa7JDXGYpekxljsktSYoYo9yd4kR5McS3LDAtu3JbkvyZEkDyW5YvRRJUnDWLbYk6wHbgEuB3YBVyfZNW+3/wrcXVUXA/uAXxt1UEnScIY5Yn89cKyqHq+qk8BdwFXz9ingH/X3Xw789egiSpLOxjDFfhHw5MDy8X7doP8GvDXJceAg8GMLPVCS/Ulmk8yeOHFiBXElScsZ1cnTq4EPVdUW4Argw0le9NhVdVtVzVTVzObNm0f01JKkQcMU+1PA1oHlLf26QdcBdwNU1WHgm4BNowgoSTo7wxT7A8DOJDuSbKQ7OXpg3j5fAC4DSPJP6YrduRZJGoNli72qTgHXA/cAj9Fd/fJIkpuTXNnv9i7g7Uk+C3wMeFtV1WqFliQtbsMwO1XVQbqTooPrbhq4/yjwhtFGkySthH95KkmNsdglqTEWuyQ1xmKXpMZY7JLUGItdkhpjsUtSYyx2SWqMxS5JjbHYJakxFrskNcZil6TGWOyS1BiLXZIaY7FLUmMsdklqjMUuSY2x2CWpMRa7JDXGYpekxljsktQYi12SGmOxS1JjLHZJaozFLkmNsdglqTEWuyQ1xmKXpMZY7JLUGItdkhozVLEn2ZvkaJJjSW5YZJ9/m+TRJI8k+ehoY0qShrVhuR2SrAduAd4EHAceSHKgqh4d2GcncCPwhqr6mySvWq3AkqSlDXPE/nrgWFU9XlUngbuAq+bt83bglqr6G4Cqema0MSVJwxqm2C8CnhxYPt6vG/Ra4LVJ/m+SzyTZO6qAkqSzs+xUzFk8zk5gD7AF+MMk/7yq/nZwpyT7gf0A27ZtG9FTS5IGDXPE/hSwdWB5S79u0HHgQFU9X1WfA/6CrujPUFW3VdVMVc1s3rx5pZklSUsYptgfAHYm2ZFkI7APODBvn0/SHa2TZBPd1Mzjo4spSRrWssVeVaeA64F7gMeAu6vqkSQ3J7my3+0e4NkkjwL3AT9VVc+uVmhJ0uJSVWN54pmZmZqdnR3Lc0vStEryYFXNLLWPf3kqSY2x2CWpMRa7JDXGYpekxljsktQYi12SGmOxS1JjLHZJaozFLkmNsdglqTEWuyQ1xmKXpMZY7JLUGItdkhpjsUtSYyx2SWqMxS5JjbHYJakxFrskNcZil6TGWOyS1BiLXZIaY7FLUmMsdklqjMUuSY2x2CWpMRa7JDXGYpekxljsktQYi12SGmOxS1Jjhir2JHuTHE1yLMkNS+z3r5NUkpnRRZQknY1liz3JeuAW4HJgF3B1kl0L7Pcy4MeB+0cdUpI0vGGO2F8PHKuqx6vqJHAXcNUC+/088F7gKyPMJ0k6S8MU+0XAkwPLx/t1X5fkEmBrVf3PpR4oyf4ks0lmT5w4cdZhJUnLO+eTp0nWAb8EvGu5favqtqqaqaqZzZs3n+tTS5IWMEyxPwVsHVje0q+b8zLgnwGHkjwBXAoc8ASqJI3HMMX+ALAzyY4kG4F9wIG5jVX1XFVtqqrtVbUd+AxwZVXNrkpiSdKSli32qjoFXA/cAzwG3F1VjyS5OcmVqx1QknR2NgyzU1UdBA7OW3fTIvvuOfdYkqSV8i9PJakxFrskNcZil6TGWOyS1BiLXZIaY7FLUmMsdklqjMUuSY2x2CWpMRa7JDXGYpekxljsktQYi12SGmOxS1JjLHZJaozFLkmNsdglqTEWuyQ1xmKXpMZY7JLUGItdkhpjsUtSYyx2SWqMxS5JjbHYJakxFrskNcZil6TGWOyS1BiLXZIaY7FLUmMsdklqzFDFnmRvkqNJjiW5YYHtP5nk0SQPJbk3yatHH1WSNIxliz3JeuAW4HJgF3B1kl3zdjsCzFTVdwIfB9436qCSpOEMc8T+euBYVT1eVSeBu4CrBneoqvuq6sv94meALaONKUka1jDFfhHw5MDy8X7dYq4DPr3QhiT7k8wmmT1x4sTwKSVJQxvpydMkbwVmgPcvtL2qbquqmaqa2bx58yifWpLU2zDEPk8BWweWt/TrzpDkjcC7ge+rqq+OJp4k6WwNc8T+ALAzyY4kG4F9wIHBHZJcDHwQuLKqnhl9TEnSsJYt9qo6BVwP3AM8BtxdVY8kuTnJlf1u7we+FfjdJH+a5MAiDydJWmXDTMVQVQeBg/PW3TRw/40jziVJWiH/8lSSGmOxS1Jjpq/YDx+G97ynu5UkvchQc+wT4/BhuOwyOHkSNm6Ee++F3bvHnUqSJsp0HbEfOtSV+te+1t0eOjTuRJI0caar2Pfs6Y7U16/vbvfsGXciSZo40zUVs3t3N/1y6FBX6k7DSNKLTFexQ1fmFrokLWq6pmIkScuy2CWpMRa7JDXGYpekxljsktQYi12SGmOxS1JjLHZJaozFLkmNsdglqTHTW+x+LrskLWj6PisG/Fx2SVrCdB6x+7nskrSo6Sx2P5ddkhY1nVMxfi67JC1qOo/YoSvzG2/s7nsSVZK+bjqP2Od4ElWSXmR6j9jBk6iStIDpLvbBk6jr18MXvuCUjKRveNNd7HMnUd/+dkjg13+9K/t3vtOCl/QNa7qLHbpy37YNTp16YUrmgx+04CV9w5ruk6dz5qZkvvIVqOp+5gr+9tvh2mvh4ovh2Wfhggu6Wy+TlNSooYo9yV7gl4H1wG9U1S/M234ecCfwOuBZ4Eer6onRRl3C3JTMnXfCHXd0pT5Y8LfeOhe0W7duHWzYsHDhD3N75Ej3eNdc45uDpImTqlp6h2Q98BfAm4DjwAPA1VX16MA+/wn4zqp6R5J9wA9X1Y8u9bgzMzM1Ozt7rvlf7PDhFxf8cuYKf9jbOS95CVx33creHFbrdu5NZ5IyTVNGs7WbcdKyrXDWIMmDVTWz5E5VteQPsBu4Z2D5RuDGefvcA+zu728AvkT/prHYz+te97paVX/0R1XveEfVeedVrVvXHb8nZ96O8mf+Y4/rdhIzTVNGs7WbcZKyrVtX9dKXdj11loDZqqV7e5ipmIuAJweWjwPfvdg+VXUqyXPABX3Bj8fu3d3PNdd017fPf+e+4w54/nk4fXrlR+yD5taP+3YSM01TRrO1m3GSsp0+/cLf3qzCdO6anjxNsh/YD7Bt27a1edK5gp9vocI/m1/pnn4aPv3plb85rNbtoHFnmcaMZms34yRlW7duVT/AcJhifwrYOrC8pV+30D7Hk2wAXg48O/+Bquo24Dbo5thXEnhkFiv8s3H48MrfHL5R5hGnLaPZ2s04adn27Fm1iy+GKfYHgJ1JdtAV+D7g383b5wDw74HDwI8Af9DPBbVtFG8OkjRiyxZ7P2d+Pd0J0vXA7VX1SJKb6SbxDwC/CXw4yTHg/9OVvyRpDIaaY6+qg8DBeetuGrj/FeDfjDaaJGklpv8jBSRJZ7DYJakxFrskNcZil6TGZFxXJSY5AXx+hf/5Jsb5V61LM9vKmG1lzLYy05zt1VW1eakHGFuxn4sks7Xch+CMidlWxmwrY7aVaT2bUzGS1BiLXZIaM63Fftu4AyzBbCtjtpUx28o0nW0q59glSYub1iN2SdIiLHZJaszUFXuSvUmOJjmW5IYxZ9ma5L4kjyZ5JMmP9+tfmeR/JfnL/vYVY8y4PsmRJJ/ql3ckub8fv99JsnFMuc5P8vEkf57ksSS7J2XckvxE/+/5cJKPJfmmcY1bktuTPJPk4YF1C45TOr/SZ3woySVjyPb+/t/0oSS/l+T8gW039tmOJvmhtc42sO1dSSrJpn557OPWr/+xfuweSfK+gfVnP27LfXfeJP3QfWzwXwGvATYCnwV2jTHPhcAl/f2X0X3p9y7gfcAN/fobgPeOMeNPAh8FPtUv3w3s6+/fCrxzTLl+C/iP/f2NwPmTMG50X/P4OeClA+P1tnGNG/C9wCXAwwPrFhwn4Arg00CAS4H7x5DtB4EN/f33DmTb1b9ezwN29K/j9WuZrV+/le4jyD8PbJqgcft+4H8D5/XLrzqXcVvTF80IBmTZL9Yec77fB94EHAUu7NddCBwdU54twL3ADwCf6v/H/dLAC++M8VzDXC/vyzPz1o993Hjh+3tfSfex1p8Cfmic4wZsn1cCC44T8EHg6oX2W6ts87b9MPCR/v4Zr9W+XHevdTbg48C/AJ4YKPaxjxvdgcMbF9hvReM2bVMxC32x9kVjynKGJNuBi4H7gW+rqi/2m54Gvm1Msf478NPA6X75AuBvq+pUvzyu8dsBnADu6KeJfiPJtzAB41ZVTwG/CHwB+CLwHPAgkzFucxYbp0l7fVxLdyQME5AtyVXAU1X12Xmbxp4NeC3wPf103/9J8l3nkm3ain0iJflW4H8A/6Wq/m5wW3Vvs2t+TWmSNwPPVNWDa/3cQ9hA96voB6rqYuDv6aYUvm6M4/YK4Cq6N59/DHwLsHetcwxrXOO0nCTvBk4BHxl3FoAk3wz8DHDTcvuOyQa63xIvBX4KuDtJVvpg01bsw3yx9ppK8hK6Uv9IVX2iX/3/klzYb78QeGYM0d4AXJnkCeAuuumYXwbOT/eF4zC+8TsOHK+q+/vlj9MV/SSM2xuBz1XViap6HvgE3VhOwrjNWWycJuL1keRtwJuBt/RvPDD+bN9B92b92f41sQX4kyTfPgHZoHtNfKI6f0z3W/amlWabtmL/+hdr91cl7KP7Iu2x6N9RfxN4rKp+aWDT3Jd709/+/lpnq6obq2pLVW2nG6c/qKq3APfRfeH4OLM9DTyZ5J/0qy4DHmUCxo1uCubSJN/c//vOZRv7uA1YbJwOANf0V3lcCjw3MGWzJpLspZv+u7Kqvjyw6QCwL8l5SXYAO4E/XqtcVfVnVfWqqtrevyaO01348DQTMG7AJ+lOoJLktXQXFHyJlY7bap4gWKWTDlfQXX3yV8C7x5zlX9H9GvwQ8Kf9zxV0c9n3An9Jd6b7lWPOuYcXrop5Tf8/xjHgd+nPwo8h078EZvux+yTwikkZN+DngD8HHgY+THdFwljGDfgY3Vz/83RldN1i40R3cvyW/rXxZ8DMGLIdo5sTnns93Dqw/7v7bEeBy9c627ztT/DCydNJGLeNwG/3/8/9CfAD5zJufqSAJDVm2qZiJEnLsNglqTEWuyQ1xmKXpMZY7JLUGItdkhpjsUtSY/4BtR/8l6Rhg98AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 모델 반복횟수(epoch)에 따른 정확도 및 val_loss를 그래프로 확인\n",
    "y_vloss = history.history['val_loss']\n",
    "y_acc = history.history['accuracy']\n",
    "\n",
    "x_len = np.arange(len(y_acc))\n",
    "\n",
    "\n",
    "plt.plot(x_len, y_vloss, 'o', c='red',markersize=3)\n",
    "plt.plot(x_len, y_acc, 'o', c='blue',markersize=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d59f38b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1336/1336 [==============================] - 1s 393us/step - loss: 0.0034 - accuracy: 0.9994\n",
      "[0.0033500082790851593, 0.9993914365768433]\n",
      "가져올 데이터 번호 : 3\n",
      "신용카드를 부정사용 확률은 0.0142%입니다.\n"
     ]
    }
   ],
   "source": [
    "# 저장된 최적의 모델 불러오기\n",
    "from keras.models import load_model\n",
    "model= load_model('./credit/57-0.0032.hdf5')\n",
    "\n",
    "\n",
    "# 학습된 model을 평가(loss 및 정확도)\n",
    "# 학습된 model에 독립변수(x_test), 종속변수(y_test)를 가지고 loss 및 accuracy 출력\n",
    "print(model.evaluate(X_test,Y_test))\n",
    "\n",
    "# 예측\n",
    "# 학습된 model을 가지고 predict함수로 예측함.\n",
    "prediction = model.predict(X_test)\n",
    "부정사용 = prediction[int(input('가져올 데이터 번호 : '))]\n",
    "p=부정사용[0]*100\n",
    "print(f'신용카드를 부정사용 확률은 {p:.4f}%입니다.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2c611cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pred(prediction):\n",
    "    # round 함수로 0.5를 기준으로 하여 반올림 해서 0과 1로 나눔\n",
    "    a = pd.DataFrame(np.round(prediction))\n",
    "    print(a[0].value_counts(),'\\n')\n",
    "\n",
    "    # 확률 값에서 소수점 2자리에서 반올림 하여 prediction 값을 출력 및 확인\n",
    "    a1 = pd.DataFrame(np.round(prediction,1))\n",
    "    print(a1.value_counts(),'\\n')\n",
    "\n",
    "    # 위에서 values값만가져옴\n",
    "    a2 = a1[0].values\n",
    "    len(a2)\n",
    "\n",
    "    # 08 이상은 1로, 0.8 미만은 0으로 나눈 values 값을 확인하려 함\n",
    "    a3 = []\n",
    "    for i in range(len(a2)):\n",
    "        if a2[i] >= 0.8:\n",
    "            a3.append(1)\n",
    "        else:\n",
    "            a3.append(0)\n",
    "    a3 = np.array(a3)\n",
    "    a3\n",
    "\n",
    "    a4 = pd.DataFrame(a3)\n",
    "    print(a4[0].value_counts(),'\\n')\n",
    "\n",
    "    # 바뀐거 확인가능\n",
    "    y_pred_t = a4[0].values\n",
    "    y_pred_t = y_pred_t.reshape(-1,1)\n",
    "    y_pred_t.shape\n",
    "    return y_pred_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "61999337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0    42663\n",
      "1.0       59\n",
      "Name: 0, dtype: int64 \n",
      "\n",
      "0.0    42633\n",
      "1.0       48\n",
      "0.1       19\n",
      "0.9        6\n",
      "0.2        5\n",
      "0.4        3\n",
      "0.3        2\n",
      "0.5        2\n",
      "0.7        2\n",
      "0.6        1\n",
      "0.8        1\n",
      "dtype: int64 \n",
      "\n",
      "0    42667\n",
      "1       55\n",
      "Name: 0, dtype: int64 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       ...,\n",
       "       [0],\n",
       "       [0],\n",
       "       [0]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pred(prediction) 함수 불러와서 y_pred_t에 넣음\n",
    "y_pred_t = pred(prediction)\n",
    "y_pred_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3934961e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42722,)\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "(42722, 1)\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " ...\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "# reshape를 하기 전에는 모두 하나의 방에 들어가있었지만\n",
    "# reshape를 사용한 후에는 1방에 1개의 value 값이 들어간 걸 확인 가능\n",
    "print(Y_test.shape)\n",
    "print(Y_test)\n",
    "Y_test_t = Y_test.astype(int)\n",
    "Y_test_t = Y_test_t.reshape(-1,1)\n",
    "print(Y_test_t.shape)\n",
    "print(Y_test_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ae3224c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42722, 1)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0599a4fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2111,  5621,  9258,  9269, 11001, 11344, 15679, 16192, 16783,\n",
       "       19925, 20713, 23829, 25980, 28689, 32165, 33194, 34054, 37266,\n",
       "       38521, 39874, 40174, 40482, 41431, 42424], dtype=int64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 여기서 != 는 is not을 의미 \n",
    "# 예측값과 실제값 다른것들만 들고옴\n",
    "miss = np.where(Y_test_t != y_pred_t)[0]\n",
    "miss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8ea49c4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 몇개 잘못 분류 했나?\n",
    "len(miss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5d4ba0d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True값은 : [1]인데, Predic값은 : [0]이다\n"
     ]
    }
   ],
   "source": [
    "# random으로 miss값을 선택해서 하나만 들고옴 실행할때마다 바뀜\n",
    "i=np.random.choice(miss)\n",
    "print(\"True값은 : %s인데, Predic값은 : %s이다\"%(Y_test_t[i],y_pred_t[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce0a6fc",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38c3d4a",
   "metadata": {},
   "source": [
    "# 0 : 492개, 1: 492개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5ecccf4d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    284315\n",
       "1       492\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Class'].value_counts()\n",
    "# 출력층 몇개 있는지 확인하려고 클래스값 확인\n",
    "# 위에서 0이 1보다 압도적으로 많은걸 확인 가능해\n",
    "# 50 : 50으로 결과값을 확인해보려함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "03f66674",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>normalAmount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26225</th>\n",
       "      <td>-1.535821</td>\n",
       "      <td>0.191602</td>\n",
       "      <td>1.616817</td>\n",
       "      <td>0.107770</td>\n",
       "      <td>-0.458116</td>\n",
       "      <td>-0.340610</td>\n",
       "      <td>0.264827</td>\n",
       "      <td>0.473573</td>\n",
       "      <td>-0.545482</td>\n",
       "      <td>-0.984222</td>\n",
       "      <td>...</td>\n",
       "      <td>0.106112</td>\n",
       "      <td>-0.311542</td>\n",
       "      <td>0.112697</td>\n",
       "      <td>-0.122293</td>\n",
       "      <td>0.212961</td>\n",
       "      <td>0.269954</td>\n",
       "      <td>-0.132437</td>\n",
       "      <td>-0.064550</td>\n",
       "      <td>0.231931</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86319</th>\n",
       "      <td>-0.380287</td>\n",
       "      <td>1.177763</td>\n",
       "      <td>1.525612</td>\n",
       "      <td>0.183458</td>\n",
       "      <td>-0.191871</td>\n",
       "      <td>-1.274917</td>\n",
       "      <td>0.760897</td>\n",
       "      <td>-0.135313</td>\n",
       "      <td>-0.505331</td>\n",
       "      <td>-0.527699</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.222487</td>\n",
       "      <td>-0.551668</td>\n",
       "      <td>0.048063</td>\n",
       "      <td>0.879717</td>\n",
       "      <td>-0.214740</td>\n",
       "      <td>0.037228</td>\n",
       "      <td>0.249072</td>\n",
       "      <td>0.104815</td>\n",
       "      <td>-0.335278</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177124</th>\n",
       "      <td>-1.017361</td>\n",
       "      <td>1.508147</td>\n",
       "      <td>-0.782430</td>\n",
       "      <td>0.475826</td>\n",
       "      <td>1.075523</td>\n",
       "      <td>-1.540406</td>\n",
       "      <td>0.743348</td>\n",
       "      <td>0.075873</td>\n",
       "      <td>-0.777363</td>\n",
       "      <td>-1.875873</td>\n",
       "      <td>...</td>\n",
       "      <td>0.213252</td>\n",
       "      <td>0.724633</td>\n",
       "      <td>-0.126560</td>\n",
       "      <td>-0.198785</td>\n",
       "      <td>0.374713</td>\n",
       "      <td>0.092894</td>\n",
       "      <td>-0.043317</td>\n",
       "      <td>0.091424</td>\n",
       "      <td>-0.349231</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27608</th>\n",
       "      <td>-0.758601</td>\n",
       "      <td>-0.195946</td>\n",
       "      <td>2.022157</td>\n",
       "      <td>-2.272191</td>\n",
       "      <td>0.763050</td>\n",
       "      <td>0.551263</td>\n",
       "      <td>0.600992</td>\n",
       "      <td>-0.399396</td>\n",
       "      <td>2.155429</td>\n",
       "      <td>-1.042343</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.184667</td>\n",
       "      <td>0.188097</td>\n",
       "      <td>-0.432666</td>\n",
       "      <td>-1.324006</td>\n",
       "      <td>0.256272</td>\n",
       "      <td>-0.830837</td>\n",
       "      <td>-0.584589</td>\n",
       "      <td>-0.545280</td>\n",
       "      <td>-0.307691</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156546</th>\n",
       "      <td>-0.625133</td>\n",
       "      <td>0.869436</td>\n",
       "      <td>2.245992</td>\n",
       "      <td>-0.322305</td>\n",
       "      <td>0.297692</td>\n",
       "      <td>0.194029</td>\n",
       "      <td>0.485154</td>\n",
       "      <td>-0.172602</td>\n",
       "      <td>1.292575</td>\n",
       "      <td>-0.873580</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.261078</td>\n",
       "      <td>-0.353379</td>\n",
       "      <td>-0.359366</td>\n",
       "      <td>-0.430072</td>\n",
       "      <td>0.275881</td>\n",
       "      <td>-0.741504</td>\n",
       "      <td>-0.214020</td>\n",
       "      <td>-0.158192</td>\n",
       "      <td>-0.308171</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120177</th>\n",
       "      <td>1.226213</td>\n",
       "      <td>-0.489761</td>\n",
       "      <td>0.770496</td>\n",
       "      <td>-0.907153</td>\n",
       "      <td>-1.092935</td>\n",
       "      <td>-0.339341</td>\n",
       "      <td>-0.759104</td>\n",
       "      <td>0.198982</td>\n",
       "      <td>1.755053</td>\n",
       "      <td>-0.877353</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004814</td>\n",
       "      <td>0.173936</td>\n",
       "      <td>-0.075122</td>\n",
       "      <td>-0.003672</td>\n",
       "      <td>0.475684</td>\n",
       "      <td>-0.679911</td>\n",
       "      <td>0.082023</td>\n",
       "      <td>0.019367</td>\n",
       "      <td>-0.349231</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33327</th>\n",
       "      <td>-0.802557</td>\n",
       "      <td>0.804269</td>\n",
       "      <td>2.801807</td>\n",
       "      <td>0.845983</td>\n",
       "      <td>-0.744898</td>\n",
       "      <td>-0.000065</td>\n",
       "      <td>0.042982</td>\n",
       "      <td>0.195344</td>\n",
       "      <td>-0.108213</td>\n",
       "      <td>-0.296404</td>\n",
       "      <td>...</td>\n",
       "      <td>0.296091</td>\n",
       "      <td>0.956422</td>\n",
       "      <td>-0.244473</td>\n",
       "      <td>0.421427</td>\n",
       "      <td>0.299734</td>\n",
       "      <td>-0.106883</td>\n",
       "      <td>0.366922</td>\n",
       "      <td>0.164440</td>\n",
       "      <td>-0.169237</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139290</th>\n",
       "      <td>1.308233</td>\n",
       "      <td>-0.430137</td>\n",
       "      <td>-0.115292</td>\n",
       "      <td>-0.622978</td>\n",
       "      <td>-0.757243</td>\n",
       "      <td>-1.152110</td>\n",
       "      <td>-0.022722</td>\n",
       "      <td>-0.294667</td>\n",
       "      <td>-1.492340</td>\n",
       "      <td>0.876893</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.322530</td>\n",
       "      <td>-0.547070</td>\n",
       "      <td>-0.023115</td>\n",
       "      <td>0.578937</td>\n",
       "      <td>0.372956</td>\n",
       "      <td>1.038151</td>\n",
       "      <td>-0.097131</td>\n",
       "      <td>-0.006101</td>\n",
       "      <td>-0.182311</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62959</th>\n",
       "      <td>0.975164</td>\n",
       "      <td>-1.379533</td>\n",
       "      <td>0.958565</td>\n",
       "      <td>-0.594336</td>\n",
       "      <td>-1.395282</td>\n",
       "      <td>0.787762</td>\n",
       "      <td>-1.291359</td>\n",
       "      <td>0.444900</td>\n",
       "      <td>-0.254946</td>\n",
       "      <td>0.551664</td>\n",
       "      <td>...</td>\n",
       "      <td>0.484783</td>\n",
       "      <td>1.172820</td>\n",
       "      <td>-0.160345</td>\n",
       "      <td>-0.250580</td>\n",
       "      <td>0.203319</td>\n",
       "      <td>-0.015497</td>\n",
       "      <td>0.049435</td>\n",
       "      <td>0.029451</td>\n",
       "      <td>0.206503</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5462</th>\n",
       "      <td>-0.480687</td>\n",
       "      <td>1.434301</td>\n",
       "      <td>0.767363</td>\n",
       "      <td>-0.122144</td>\n",
       "      <td>0.853692</td>\n",
       "      <td>-0.139949</td>\n",
       "      <td>0.638639</td>\n",
       "      <td>-0.115268</td>\n",
       "      <td>1.015943</td>\n",
       "      <td>-0.600603</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.526256</td>\n",
       "      <td>-1.068106</td>\n",
       "      <td>-0.184388</td>\n",
       "      <td>-1.102490</td>\n",
       "      <td>0.089799</td>\n",
       "      <td>0.121418</td>\n",
       "      <td>0.335390</td>\n",
       "      <td>0.138319</td>\n",
       "      <td>-0.334998</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>492 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              V1        V2        V3        V4        V5        V6        V7  \\\n",
       "26225  -1.535821  0.191602  1.616817  0.107770 -0.458116 -0.340610  0.264827   \n",
       "86319  -0.380287  1.177763  1.525612  0.183458 -0.191871 -1.274917  0.760897   \n",
       "177124 -1.017361  1.508147 -0.782430  0.475826  1.075523 -1.540406  0.743348   \n",
       "27608  -0.758601 -0.195946  2.022157 -2.272191  0.763050  0.551263  0.600992   \n",
       "156546 -0.625133  0.869436  2.245992 -0.322305  0.297692  0.194029  0.485154   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "120177  1.226213 -0.489761  0.770496 -0.907153 -1.092935 -0.339341 -0.759104   \n",
       "33327  -0.802557  0.804269  2.801807  0.845983 -0.744898 -0.000065  0.042982   \n",
       "139290  1.308233 -0.430137 -0.115292 -0.622978 -0.757243 -1.152110 -0.022722   \n",
       "62959   0.975164 -1.379533  0.958565 -0.594336 -1.395282  0.787762 -1.291359   \n",
       "5462   -0.480687  1.434301  0.767363 -0.122144  0.853692 -0.139949  0.638639   \n",
       "\n",
       "              V8        V9       V10  ...       V21       V22       V23  \\\n",
       "26225   0.473573 -0.545482 -0.984222  ...  0.106112 -0.311542  0.112697   \n",
       "86319  -0.135313 -0.505331 -0.527699  ... -0.222487 -0.551668  0.048063   \n",
       "177124  0.075873 -0.777363 -1.875873  ...  0.213252  0.724633 -0.126560   \n",
       "27608  -0.399396  2.155429 -1.042343  ... -0.184667  0.188097 -0.432666   \n",
       "156546 -0.172602  1.292575 -0.873580  ... -0.261078 -0.353379 -0.359366   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "120177  0.198982  1.755053 -0.877353  ...  0.004814  0.173936 -0.075122   \n",
       "33327   0.195344 -0.108213 -0.296404  ...  0.296091  0.956422 -0.244473   \n",
       "139290 -0.294667 -1.492340  0.876893  ... -0.322530 -0.547070 -0.023115   \n",
       "62959   0.444900 -0.254946  0.551664  ...  0.484783  1.172820 -0.160345   \n",
       "5462   -0.115268  1.015943 -0.600603  ... -0.526256 -1.068106 -0.184388   \n",
       "\n",
       "             V24       V25       V26       V27       V28  normalAmount  Class  \n",
       "26225  -0.122293  0.212961  0.269954 -0.132437 -0.064550      0.231931      0  \n",
       "86319   0.879717 -0.214740  0.037228  0.249072  0.104815     -0.335278      0  \n",
       "177124 -0.198785  0.374713  0.092894 -0.043317  0.091424     -0.349231      0  \n",
       "27608  -1.324006  0.256272 -0.830837 -0.584589 -0.545280     -0.307691      0  \n",
       "156546 -0.430072  0.275881 -0.741504 -0.214020 -0.158192     -0.308171      0  \n",
       "...          ...       ...       ...       ...       ...           ...    ...  \n",
       "120177 -0.003672  0.475684 -0.679911  0.082023  0.019367     -0.349231      0  \n",
       "33327   0.421427  0.299734 -0.106883  0.366922  0.164440     -0.169237      0  \n",
       "139290  0.578937  0.372956  1.038151 -0.097131 -0.006101     -0.182311      0  \n",
       "62959  -0.250580  0.203319 -0.015497  0.049435  0.029451      0.206503      0  \n",
       "5462   -1.102490  0.089799  0.121418  0.335390  0.138319     -0.334998      0  \n",
       "\n",
       "[492 rows x 30 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 위에서 class 1값이 492개인거 확인하고\n",
    "# class 0값을 똑같이 492개 들고오기위해 샘플로 랜덤표본추출함\n",
    "df_class_0 = df[df['Class'] == 0].sample(492)\n",
    "df_class_0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6e828163",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    492\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0 만 492개 받아온거 확인가능\n",
    "df_class_0['Class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2114f96d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>normalAmount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>541</th>\n",
       "      <td>-2.312227</td>\n",
       "      <td>1.951992</td>\n",
       "      <td>-1.609851</td>\n",
       "      <td>3.997906</td>\n",
       "      <td>-0.522188</td>\n",
       "      <td>-1.426545</td>\n",
       "      <td>-2.537387</td>\n",
       "      <td>1.391657</td>\n",
       "      <td>-2.770089</td>\n",
       "      <td>-2.772272</td>\n",
       "      <td>...</td>\n",
       "      <td>0.517232</td>\n",
       "      <td>-0.035049</td>\n",
       "      <td>-0.465211</td>\n",
       "      <td>0.320198</td>\n",
       "      <td>0.044519</td>\n",
       "      <td>0.177840</td>\n",
       "      <td>0.261145</td>\n",
       "      <td>-0.143276</td>\n",
       "      <td>-0.353229</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>623</th>\n",
       "      <td>-3.043541</td>\n",
       "      <td>-3.157307</td>\n",
       "      <td>1.088463</td>\n",
       "      <td>2.288644</td>\n",
       "      <td>1.359805</td>\n",
       "      <td>-1.064823</td>\n",
       "      <td>0.325574</td>\n",
       "      <td>-0.067794</td>\n",
       "      <td>-0.270953</td>\n",
       "      <td>-0.838587</td>\n",
       "      <td>...</td>\n",
       "      <td>0.661696</td>\n",
       "      <td>0.435477</td>\n",
       "      <td>1.375966</td>\n",
       "      <td>-0.293803</td>\n",
       "      <td>0.279798</td>\n",
       "      <td>-0.145362</td>\n",
       "      <td>-0.252773</td>\n",
       "      <td>0.035764</td>\n",
       "      <td>1.761758</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4920</th>\n",
       "      <td>-2.303350</td>\n",
       "      <td>1.759247</td>\n",
       "      <td>-0.359745</td>\n",
       "      <td>2.330243</td>\n",
       "      <td>-0.821628</td>\n",
       "      <td>-0.075788</td>\n",
       "      <td>0.562320</td>\n",
       "      <td>-0.399147</td>\n",
       "      <td>-0.238253</td>\n",
       "      <td>-1.525412</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.294166</td>\n",
       "      <td>-0.932391</td>\n",
       "      <td>0.172726</td>\n",
       "      <td>-0.087330</td>\n",
       "      <td>-0.156114</td>\n",
       "      <td>-0.542628</td>\n",
       "      <td>0.039566</td>\n",
       "      <td>-0.153029</td>\n",
       "      <td>0.606031</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6108</th>\n",
       "      <td>-4.397974</td>\n",
       "      <td>1.358367</td>\n",
       "      <td>-2.592844</td>\n",
       "      <td>2.679787</td>\n",
       "      <td>-1.128131</td>\n",
       "      <td>-1.706536</td>\n",
       "      <td>-3.496197</td>\n",
       "      <td>-0.248778</td>\n",
       "      <td>-0.247768</td>\n",
       "      <td>-4.801637</td>\n",
       "      <td>...</td>\n",
       "      <td>0.573574</td>\n",
       "      <td>0.176968</td>\n",
       "      <td>-0.436207</td>\n",
       "      <td>-0.053502</td>\n",
       "      <td>0.252405</td>\n",
       "      <td>-0.657488</td>\n",
       "      <td>-0.827136</td>\n",
       "      <td>0.849573</td>\n",
       "      <td>-0.117342</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6329</th>\n",
       "      <td>1.234235</td>\n",
       "      <td>3.019740</td>\n",
       "      <td>-4.304597</td>\n",
       "      <td>4.732795</td>\n",
       "      <td>3.624201</td>\n",
       "      <td>-1.357746</td>\n",
       "      <td>1.713445</td>\n",
       "      <td>-0.496358</td>\n",
       "      <td>-1.282858</td>\n",
       "      <td>-2.447469</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.379068</td>\n",
       "      <td>-0.704181</td>\n",
       "      <td>-0.656805</td>\n",
       "      <td>-1.632653</td>\n",
       "      <td>1.488901</td>\n",
       "      <td>0.566797</td>\n",
       "      <td>-0.010016</td>\n",
       "      <td>0.146793</td>\n",
       "      <td>-0.349231</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279863</th>\n",
       "      <td>-1.927883</td>\n",
       "      <td>1.125653</td>\n",
       "      <td>-4.518331</td>\n",
       "      <td>1.749293</td>\n",
       "      <td>-1.566487</td>\n",
       "      <td>-2.010494</td>\n",
       "      <td>-0.882850</td>\n",
       "      <td>0.697211</td>\n",
       "      <td>-2.064945</td>\n",
       "      <td>-5.587794</td>\n",
       "      <td>...</td>\n",
       "      <td>0.778584</td>\n",
       "      <td>-0.319189</td>\n",
       "      <td>0.639419</td>\n",
       "      <td>-0.294885</td>\n",
       "      <td>0.537503</td>\n",
       "      <td>0.788395</td>\n",
       "      <td>0.292680</td>\n",
       "      <td>0.147968</td>\n",
       "      <td>1.206024</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280143</th>\n",
       "      <td>1.378559</td>\n",
       "      <td>1.289381</td>\n",
       "      <td>-5.004247</td>\n",
       "      <td>1.411850</td>\n",
       "      <td>0.442581</td>\n",
       "      <td>-1.326536</td>\n",
       "      <td>-1.413170</td>\n",
       "      <td>0.248525</td>\n",
       "      <td>-1.127396</td>\n",
       "      <td>-3.232153</td>\n",
       "      <td>...</td>\n",
       "      <td>0.370612</td>\n",
       "      <td>0.028234</td>\n",
       "      <td>-0.145640</td>\n",
       "      <td>-0.081049</td>\n",
       "      <td>0.521875</td>\n",
       "      <td>0.739467</td>\n",
       "      <td>0.389152</td>\n",
       "      <td>0.186637</td>\n",
       "      <td>-0.350191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280149</th>\n",
       "      <td>-0.676143</td>\n",
       "      <td>1.126366</td>\n",
       "      <td>-2.213700</td>\n",
       "      <td>0.468308</td>\n",
       "      <td>-1.120541</td>\n",
       "      <td>-0.003346</td>\n",
       "      <td>-2.234739</td>\n",
       "      <td>1.210158</td>\n",
       "      <td>-0.652250</td>\n",
       "      <td>-3.463891</td>\n",
       "      <td>...</td>\n",
       "      <td>0.751826</td>\n",
       "      <td>0.834108</td>\n",
       "      <td>0.190944</td>\n",
       "      <td>0.032070</td>\n",
       "      <td>-0.739695</td>\n",
       "      <td>0.471111</td>\n",
       "      <td>0.385107</td>\n",
       "      <td>0.194361</td>\n",
       "      <td>-0.041818</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281144</th>\n",
       "      <td>-3.113832</td>\n",
       "      <td>0.585864</td>\n",
       "      <td>-5.399730</td>\n",
       "      <td>1.817092</td>\n",
       "      <td>-0.840618</td>\n",
       "      <td>-2.943548</td>\n",
       "      <td>-2.208002</td>\n",
       "      <td>1.058733</td>\n",
       "      <td>-1.632333</td>\n",
       "      <td>-5.245984</td>\n",
       "      <td>...</td>\n",
       "      <td>0.583276</td>\n",
       "      <td>-0.269209</td>\n",
       "      <td>-0.456108</td>\n",
       "      <td>-0.183659</td>\n",
       "      <td>-0.328168</td>\n",
       "      <td>0.606116</td>\n",
       "      <td>0.884876</td>\n",
       "      <td>-0.253700</td>\n",
       "      <td>0.626302</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281674</th>\n",
       "      <td>1.991976</td>\n",
       "      <td>0.158476</td>\n",
       "      <td>-2.583441</td>\n",
       "      <td>0.408670</td>\n",
       "      <td>1.151147</td>\n",
       "      <td>-0.096695</td>\n",
       "      <td>0.223050</td>\n",
       "      <td>-0.068384</td>\n",
       "      <td>0.577829</td>\n",
       "      <td>-0.888722</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.164350</td>\n",
       "      <td>-0.295135</td>\n",
       "      <td>-0.072173</td>\n",
       "      <td>-0.450261</td>\n",
       "      <td>0.313267</td>\n",
       "      <td>-0.289617</td>\n",
       "      <td>0.002988</td>\n",
       "      <td>-0.015309</td>\n",
       "      <td>-0.183191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>492 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              V1        V2        V3        V4        V5        V6        V7  \\\n",
       "541    -2.312227  1.951992 -1.609851  3.997906 -0.522188 -1.426545 -2.537387   \n",
       "623    -3.043541 -3.157307  1.088463  2.288644  1.359805 -1.064823  0.325574   \n",
       "4920   -2.303350  1.759247 -0.359745  2.330243 -0.821628 -0.075788  0.562320   \n",
       "6108   -4.397974  1.358367 -2.592844  2.679787 -1.128131 -1.706536 -3.496197   \n",
       "6329    1.234235  3.019740 -4.304597  4.732795  3.624201 -1.357746  1.713445   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "279863 -1.927883  1.125653 -4.518331  1.749293 -1.566487 -2.010494 -0.882850   \n",
       "280143  1.378559  1.289381 -5.004247  1.411850  0.442581 -1.326536 -1.413170   \n",
       "280149 -0.676143  1.126366 -2.213700  0.468308 -1.120541 -0.003346 -2.234739   \n",
       "281144 -3.113832  0.585864 -5.399730  1.817092 -0.840618 -2.943548 -2.208002   \n",
       "281674  1.991976  0.158476 -2.583441  0.408670  1.151147 -0.096695  0.223050   \n",
       "\n",
       "              V8        V9       V10  ...       V21       V22       V23  \\\n",
       "541     1.391657 -2.770089 -2.772272  ...  0.517232 -0.035049 -0.465211   \n",
       "623    -0.067794 -0.270953 -0.838587  ...  0.661696  0.435477  1.375966   \n",
       "4920   -0.399147 -0.238253 -1.525412  ... -0.294166 -0.932391  0.172726   \n",
       "6108   -0.248778 -0.247768 -4.801637  ...  0.573574  0.176968 -0.436207   \n",
       "6329   -0.496358 -1.282858 -2.447469  ... -0.379068 -0.704181 -0.656805   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "279863  0.697211 -2.064945 -5.587794  ...  0.778584 -0.319189  0.639419   \n",
       "280143  0.248525 -1.127396 -3.232153  ...  0.370612  0.028234 -0.145640   \n",
       "280149  1.210158 -0.652250 -3.463891  ...  0.751826  0.834108  0.190944   \n",
       "281144  1.058733 -1.632333 -5.245984  ...  0.583276 -0.269209 -0.456108   \n",
       "281674 -0.068384  0.577829 -0.888722  ... -0.164350 -0.295135 -0.072173   \n",
       "\n",
       "             V24       V25       V26       V27       V28  normalAmount  Class  \n",
       "541     0.320198  0.044519  0.177840  0.261145 -0.143276     -0.353229      1  \n",
       "623    -0.293803  0.279798 -0.145362 -0.252773  0.035764      1.761758      1  \n",
       "4920   -0.087330 -0.156114 -0.542628  0.039566 -0.153029      0.606031      1  \n",
       "6108   -0.053502  0.252405 -0.657488 -0.827136  0.849573     -0.117342      1  \n",
       "6329   -1.632653  1.488901  0.566797 -0.010016  0.146793     -0.349231      1  \n",
       "...          ...       ...       ...       ...       ...           ...    ...  \n",
       "279863 -0.294885  0.537503  0.788395  0.292680  0.147968      1.206024      1  \n",
       "280143 -0.081049  0.521875  0.739467  0.389152  0.186637     -0.350191      1  \n",
       "280149  0.032070 -0.739695  0.471111  0.385107  0.194361     -0.041818      1  \n",
       "281144 -0.183659 -0.328168  0.606116  0.884876 -0.253700      0.626302      1  \n",
       "281674 -0.450261  0.313267 -0.289617  0.002988 -0.015309     -0.183191      1  \n",
       "\n",
       "[492 rows x 30 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_class_1 = df[df['Class'] == 1]\n",
    "df_class_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dcc00bf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    492\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1 만 492개 받아온거 확인가능\n",
    "df_class_1['Class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "51aa2f84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>normalAmount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26225</th>\n",
       "      <td>-1.535821</td>\n",
       "      <td>0.191602</td>\n",
       "      <td>1.616817</td>\n",
       "      <td>0.107770</td>\n",
       "      <td>-0.458116</td>\n",
       "      <td>-0.340610</td>\n",
       "      <td>0.264827</td>\n",
       "      <td>0.473573</td>\n",
       "      <td>-0.545482</td>\n",
       "      <td>-0.984222</td>\n",
       "      <td>...</td>\n",
       "      <td>0.106112</td>\n",
       "      <td>-0.311542</td>\n",
       "      <td>0.112697</td>\n",
       "      <td>-0.122293</td>\n",
       "      <td>0.212961</td>\n",
       "      <td>0.269954</td>\n",
       "      <td>-0.132437</td>\n",
       "      <td>-0.064550</td>\n",
       "      <td>0.231931</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86319</th>\n",
       "      <td>-0.380287</td>\n",
       "      <td>1.177763</td>\n",
       "      <td>1.525612</td>\n",
       "      <td>0.183458</td>\n",
       "      <td>-0.191871</td>\n",
       "      <td>-1.274917</td>\n",
       "      <td>0.760897</td>\n",
       "      <td>-0.135313</td>\n",
       "      <td>-0.505331</td>\n",
       "      <td>-0.527699</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.222487</td>\n",
       "      <td>-0.551668</td>\n",
       "      <td>0.048063</td>\n",
       "      <td>0.879717</td>\n",
       "      <td>-0.214740</td>\n",
       "      <td>0.037228</td>\n",
       "      <td>0.249072</td>\n",
       "      <td>0.104815</td>\n",
       "      <td>-0.335278</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177124</th>\n",
       "      <td>-1.017361</td>\n",
       "      <td>1.508147</td>\n",
       "      <td>-0.782430</td>\n",
       "      <td>0.475826</td>\n",
       "      <td>1.075523</td>\n",
       "      <td>-1.540406</td>\n",
       "      <td>0.743348</td>\n",
       "      <td>0.075873</td>\n",
       "      <td>-0.777363</td>\n",
       "      <td>-1.875873</td>\n",
       "      <td>...</td>\n",
       "      <td>0.213252</td>\n",
       "      <td>0.724633</td>\n",
       "      <td>-0.126560</td>\n",
       "      <td>-0.198785</td>\n",
       "      <td>0.374713</td>\n",
       "      <td>0.092894</td>\n",
       "      <td>-0.043317</td>\n",
       "      <td>0.091424</td>\n",
       "      <td>-0.349231</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27608</th>\n",
       "      <td>-0.758601</td>\n",
       "      <td>-0.195946</td>\n",
       "      <td>2.022157</td>\n",
       "      <td>-2.272191</td>\n",
       "      <td>0.763050</td>\n",
       "      <td>0.551263</td>\n",
       "      <td>0.600992</td>\n",
       "      <td>-0.399396</td>\n",
       "      <td>2.155429</td>\n",
       "      <td>-1.042343</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.184667</td>\n",
       "      <td>0.188097</td>\n",
       "      <td>-0.432666</td>\n",
       "      <td>-1.324006</td>\n",
       "      <td>0.256272</td>\n",
       "      <td>-0.830837</td>\n",
       "      <td>-0.584589</td>\n",
       "      <td>-0.545280</td>\n",
       "      <td>-0.307691</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156546</th>\n",
       "      <td>-0.625133</td>\n",
       "      <td>0.869436</td>\n",
       "      <td>2.245992</td>\n",
       "      <td>-0.322305</td>\n",
       "      <td>0.297692</td>\n",
       "      <td>0.194029</td>\n",
       "      <td>0.485154</td>\n",
       "      <td>-0.172602</td>\n",
       "      <td>1.292575</td>\n",
       "      <td>-0.873580</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.261078</td>\n",
       "      <td>-0.353379</td>\n",
       "      <td>-0.359366</td>\n",
       "      <td>-0.430072</td>\n",
       "      <td>0.275881</td>\n",
       "      <td>-0.741504</td>\n",
       "      <td>-0.214020</td>\n",
       "      <td>-0.158192</td>\n",
       "      <td>-0.308171</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279863</th>\n",
       "      <td>-1.927883</td>\n",
       "      <td>1.125653</td>\n",
       "      <td>-4.518331</td>\n",
       "      <td>1.749293</td>\n",
       "      <td>-1.566487</td>\n",
       "      <td>-2.010494</td>\n",
       "      <td>-0.882850</td>\n",
       "      <td>0.697211</td>\n",
       "      <td>-2.064945</td>\n",
       "      <td>-5.587794</td>\n",
       "      <td>...</td>\n",
       "      <td>0.778584</td>\n",
       "      <td>-0.319189</td>\n",
       "      <td>0.639419</td>\n",
       "      <td>-0.294885</td>\n",
       "      <td>0.537503</td>\n",
       "      <td>0.788395</td>\n",
       "      <td>0.292680</td>\n",
       "      <td>0.147968</td>\n",
       "      <td>1.206024</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280143</th>\n",
       "      <td>1.378559</td>\n",
       "      <td>1.289381</td>\n",
       "      <td>-5.004247</td>\n",
       "      <td>1.411850</td>\n",
       "      <td>0.442581</td>\n",
       "      <td>-1.326536</td>\n",
       "      <td>-1.413170</td>\n",
       "      <td>0.248525</td>\n",
       "      <td>-1.127396</td>\n",
       "      <td>-3.232153</td>\n",
       "      <td>...</td>\n",
       "      <td>0.370612</td>\n",
       "      <td>0.028234</td>\n",
       "      <td>-0.145640</td>\n",
       "      <td>-0.081049</td>\n",
       "      <td>0.521875</td>\n",
       "      <td>0.739467</td>\n",
       "      <td>0.389152</td>\n",
       "      <td>0.186637</td>\n",
       "      <td>-0.350191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280149</th>\n",
       "      <td>-0.676143</td>\n",
       "      <td>1.126366</td>\n",
       "      <td>-2.213700</td>\n",
       "      <td>0.468308</td>\n",
       "      <td>-1.120541</td>\n",
       "      <td>-0.003346</td>\n",
       "      <td>-2.234739</td>\n",
       "      <td>1.210158</td>\n",
       "      <td>-0.652250</td>\n",
       "      <td>-3.463891</td>\n",
       "      <td>...</td>\n",
       "      <td>0.751826</td>\n",
       "      <td>0.834108</td>\n",
       "      <td>0.190944</td>\n",
       "      <td>0.032070</td>\n",
       "      <td>-0.739695</td>\n",
       "      <td>0.471111</td>\n",
       "      <td>0.385107</td>\n",
       "      <td>0.194361</td>\n",
       "      <td>-0.041818</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281144</th>\n",
       "      <td>-3.113832</td>\n",
       "      <td>0.585864</td>\n",
       "      <td>-5.399730</td>\n",
       "      <td>1.817092</td>\n",
       "      <td>-0.840618</td>\n",
       "      <td>-2.943548</td>\n",
       "      <td>-2.208002</td>\n",
       "      <td>1.058733</td>\n",
       "      <td>-1.632333</td>\n",
       "      <td>-5.245984</td>\n",
       "      <td>...</td>\n",
       "      <td>0.583276</td>\n",
       "      <td>-0.269209</td>\n",
       "      <td>-0.456108</td>\n",
       "      <td>-0.183659</td>\n",
       "      <td>-0.328168</td>\n",
       "      <td>0.606116</td>\n",
       "      <td>0.884876</td>\n",
       "      <td>-0.253700</td>\n",
       "      <td>0.626302</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281674</th>\n",
       "      <td>1.991976</td>\n",
       "      <td>0.158476</td>\n",
       "      <td>-2.583441</td>\n",
       "      <td>0.408670</td>\n",
       "      <td>1.151147</td>\n",
       "      <td>-0.096695</td>\n",
       "      <td>0.223050</td>\n",
       "      <td>-0.068384</td>\n",
       "      <td>0.577829</td>\n",
       "      <td>-0.888722</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.164350</td>\n",
       "      <td>-0.295135</td>\n",
       "      <td>-0.072173</td>\n",
       "      <td>-0.450261</td>\n",
       "      <td>0.313267</td>\n",
       "      <td>-0.289617</td>\n",
       "      <td>0.002988</td>\n",
       "      <td>-0.015309</td>\n",
       "      <td>-0.183191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>984 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              V1        V2        V3        V4        V5        V6        V7  \\\n",
       "26225  -1.535821  0.191602  1.616817  0.107770 -0.458116 -0.340610  0.264827   \n",
       "86319  -0.380287  1.177763  1.525612  0.183458 -0.191871 -1.274917  0.760897   \n",
       "177124 -1.017361  1.508147 -0.782430  0.475826  1.075523 -1.540406  0.743348   \n",
       "27608  -0.758601 -0.195946  2.022157 -2.272191  0.763050  0.551263  0.600992   \n",
       "156546 -0.625133  0.869436  2.245992 -0.322305  0.297692  0.194029  0.485154   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "279863 -1.927883  1.125653 -4.518331  1.749293 -1.566487 -2.010494 -0.882850   \n",
       "280143  1.378559  1.289381 -5.004247  1.411850  0.442581 -1.326536 -1.413170   \n",
       "280149 -0.676143  1.126366 -2.213700  0.468308 -1.120541 -0.003346 -2.234739   \n",
       "281144 -3.113832  0.585864 -5.399730  1.817092 -0.840618 -2.943548 -2.208002   \n",
       "281674  1.991976  0.158476 -2.583441  0.408670  1.151147 -0.096695  0.223050   \n",
       "\n",
       "              V8        V9       V10  ...       V21       V22       V23  \\\n",
       "26225   0.473573 -0.545482 -0.984222  ...  0.106112 -0.311542  0.112697   \n",
       "86319  -0.135313 -0.505331 -0.527699  ... -0.222487 -0.551668  0.048063   \n",
       "177124  0.075873 -0.777363 -1.875873  ...  0.213252  0.724633 -0.126560   \n",
       "27608  -0.399396  2.155429 -1.042343  ... -0.184667  0.188097 -0.432666   \n",
       "156546 -0.172602  1.292575 -0.873580  ... -0.261078 -0.353379 -0.359366   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "279863  0.697211 -2.064945 -5.587794  ...  0.778584 -0.319189  0.639419   \n",
       "280143  0.248525 -1.127396 -3.232153  ...  0.370612  0.028234 -0.145640   \n",
       "280149  1.210158 -0.652250 -3.463891  ...  0.751826  0.834108  0.190944   \n",
       "281144  1.058733 -1.632333 -5.245984  ...  0.583276 -0.269209 -0.456108   \n",
       "281674 -0.068384  0.577829 -0.888722  ... -0.164350 -0.295135 -0.072173   \n",
       "\n",
       "             V24       V25       V26       V27       V28  normalAmount  Class  \n",
       "26225  -0.122293  0.212961  0.269954 -0.132437 -0.064550      0.231931      0  \n",
       "86319   0.879717 -0.214740  0.037228  0.249072  0.104815     -0.335278      0  \n",
       "177124 -0.198785  0.374713  0.092894 -0.043317  0.091424     -0.349231      0  \n",
       "27608  -1.324006  0.256272 -0.830837 -0.584589 -0.545280     -0.307691      0  \n",
       "156546 -0.430072  0.275881 -0.741504 -0.214020 -0.158192     -0.308171      0  \n",
       "...          ...       ...       ...       ...       ...           ...    ...  \n",
       "279863 -0.294885  0.537503  0.788395  0.292680  0.147968      1.206024      1  \n",
       "280143 -0.081049  0.521875  0.739467  0.389152  0.186637     -0.350191      1  \n",
       "280149  0.032070 -0.739695  0.471111  0.385107  0.194361     -0.041818      1  \n",
       "281144 -0.183659 -0.328168  0.606116  0.884876 -0.253700      0.626302      1  \n",
       "281674 -0.450261  0.313267 -0.289617  0.002988 -0.015309     -0.183191      1  \n",
       "\n",
       "[984 rows x 30 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 위에 두개를 병합 시킴\n",
    "df_sample = pd.concat([df_class_0,df_class_1])\n",
    "df_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f6d5085b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(836, 29)\n",
      "(148, 29)\n"
     ]
    }
   ],
   "source": [
    "X =df_sample.values[:,0:29] #독립변수\n",
    "Y =df_sample.values[:,29] #종속변수\n",
    "X_train, X_test, Y_train, Y_test= train_test_split(X,Y, test_size = 0.15) #85 : 15니까\n",
    "print(X_train.shape) # 85프로\n",
    "print(X_test.shape) #15프로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "eed05c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 0.5343 - accuracy: 0.7266 - val_loss: 0.4535 - val_accuracy: 0.8116\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.45355, saving model to ./credit_50\\01-0.4535.hdf5\n",
      "Epoch 2/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4592 - accuracy: 0.8179 - val_loss: 0.3976 - val_accuracy: 0.8804\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.45355 to 0.39761, saving model to ./credit_50\\02-0.3976.hdf5\n",
      "Epoch 3/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4382 - accuracy: 0.8269 - val_loss: 0.3528 - val_accuracy: 0.9167\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.39761 to 0.35279, saving model to ./credit_50\\03-0.3528.hdf5\n",
      "Epoch 4/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3905 - accuracy: 0.8655 - val_loss: 0.3139 - val_accuracy: 0.9348\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.35279 to 0.31387, saving model to ./credit_50\\04-0.3139.hdf5\n",
      "Epoch 5/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3264 - accuracy: 0.8869 - val_loss: 0.2813 - val_accuracy: 0.9457\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.31387 to 0.28131, saving model to ./credit_50\\05-0.2813.hdf5\n",
      "Epoch 6/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3239 - accuracy: 0.8734 - val_loss: 0.2530 - val_accuracy: 0.9493\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.28131 to 0.25303, saving model to ./credit_50\\06-0.2530.hdf5\n",
      "Epoch 7/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.2862 - accuracy: 0.9297 - val_loss: 0.2288 - val_accuracy: 0.9529\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.25303 to 0.22885, saving model to ./credit_50\\07-0.2288.hdf5\n",
      "Epoch 8/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.2613 - accuracy: 0.9286 - val_loss: 0.2084 - val_accuracy: 0.9529\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.22885 to 0.20839, saving model to ./credit_50\\08-0.2084.hdf5\n",
      "Epoch 9/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.2393 - accuracy: 0.9199 - val_loss: 0.1919 - val_accuracy: 0.9493\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.20839 to 0.19191, saving model to ./credit_50\\09-0.1919.hdf5\n",
      "Epoch 10/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.2290 - accuracy: 0.9285 - val_loss: 0.1781 - val_accuracy: 0.9529\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.19191 to 0.17808, saving model to ./credit_50\\10-0.1781.hdf5\n",
      "Epoch 11/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.2016 - accuracy: 0.9508 - val_loss: 0.1662 - val_accuracy: 0.9529\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.17808 to 0.16621, saving model to ./credit_50\\11-0.1662.hdf5\n",
      "Epoch 12/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.2013 - accuracy: 0.9321 - val_loss: 0.1588 - val_accuracy: 0.9529\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.16621 to 0.15879, saving model to ./credit_50\\12-0.1588.hdf5\n",
      "Epoch 13/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.1795 - accuracy: 0.9441 - val_loss: 0.1522 - val_accuracy: 0.9529\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.15879 to 0.15220, saving model to ./credit_50\\13-0.1522.hdf5\n",
      "Epoch 14/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.1562 - accuracy: 0.9533 - val_loss: 0.1480 - val_accuracy: 0.9529\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.15220 to 0.14797, saving model to ./credit_50\\14-0.1480.hdf5\n",
      "Epoch 15/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.1575 - accuracy: 0.9416 - val_loss: 0.1434 - val_accuracy: 0.9529\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.14797 to 0.14335, saving model to ./credit_50\\15-0.1434.hdf5\n",
      "Epoch 16/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.1635 - accuracy: 0.9442 - val_loss: 0.1412 - val_accuracy: 0.9529\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.14335 to 0.14122, saving model to ./credit_50\\16-0.1412.hdf5\n",
      "Epoch 17/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.1388 - accuracy: 0.9579 - val_loss: 0.1387 - val_accuracy: 0.9493\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.14122 to 0.13873, saving model to ./credit_50\\17-0.1387.hdf5\n",
      "Epoch 18/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.1223 - accuracy: 0.9591 - val_loss: 0.1382 - val_accuracy: 0.9493\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.13873 to 0.13820, saving model to ./credit_50\\18-0.1382.hdf5\n",
      "Epoch 19/1000\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.1744 - accuracy: 0.95 - 0s 2ms/step - loss: 0.1468 - accuracy: 0.9493 - val_loss: 0.1383 - val_accuracy: 0.9493\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.13820\n",
      "Epoch 20/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.1384 - accuracy: 0.9526 - val_loss: 0.1380 - val_accuracy: 0.9493\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.13820 to 0.13800, saving model to ./credit_50\\20-0.1380.hdf5\n",
      "Epoch 21/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.1616 - accuracy: 0.9370 - val_loss: 0.1371 - val_accuracy: 0.9457\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.13800 to 0.13711, saving model to ./credit_50\\21-0.1371.hdf5\n",
      "Epoch 22/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.1271 - accuracy: 0.9527 - val_loss: 0.1367 - val_accuracy: 0.9493\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.13711 to 0.13667, saving model to ./credit_50\\22-0.1367.hdf5\n",
      "Epoch 23/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.1384 - accuracy: 0.9493 - val_loss: 0.1362 - val_accuracy: 0.9493\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.13667 to 0.13623, saving model to ./credit_50\\23-0.1362.hdf5\n",
      "Epoch 24/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.1054 - accuracy: 0.9707 - val_loss: 0.1364 - val_accuracy: 0.9493\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.13623\n",
      "Epoch 25/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.1017 - accuracy: 0.9660 - val_loss: 0.1373 - val_accuracy: 0.9493\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.13623\n",
      "Epoch 26/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.1177 - accuracy: 0.9640 - val_loss: 0.1369 - val_accuracy: 0.9493\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.13623\n",
      "Epoch 27/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.1219 - accuracy: 0.9483 - val_loss: 0.1377 - val_accuracy: 0.9493\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.13623\n",
      "Epoch 28/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.1144 - accuracy: 0.9593 - val_loss: 0.1370 - val_accuracy: 0.9493\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.13623\n",
      "Epoch 29/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.0762 - accuracy: 0.9772 - val_loss: 0.1378 - val_accuracy: 0.9457\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.13623\n",
      "Epoch 30/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.1324 - accuracy: 0.9435 - val_loss: 0.1396 - val_accuracy: 0.9457\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.13623\n",
      "Epoch 31/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.1081 - accuracy: 0.9685 - val_loss: 0.1380 - val_accuracy: 0.9457\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.13623\n",
      "Epoch 32/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.1034 - accuracy: 0.9626 - val_loss: 0.1393 - val_accuracy: 0.9457\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.13623\n",
      "Epoch 33/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.1250 - accuracy: 0.9553 - val_loss: 0.1398 - val_accuracy: 0.9457\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.13623\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 14)                420       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 7)                 105       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 32        \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 562\n",
      "Trainable params: 562\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 모델 설정\n",
    "model_1 = Sequential()\n",
    "model_1.add(Dense(12, activation = 'tanh', input_dim=X.shape[1]))\n",
    "model_1.add(Dense(6, activation = 'relu'))\n",
    "model_1.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# 모델 컴파일\n",
    "model_1.compile(loss='binary_crossentropy',\n",
    "             optimizer='adam',\n",
    "             metrics= ['accuracy'])\n",
    "\n",
    "# 모델 저장 폴더 지정\n",
    "MODEL_DIR='./credit_50/'\n",
    "if not os.path.exists(MODEL_DIR):\n",
    "    os.mkdir(MODEL_DIR)\n",
    "\n",
    "# 모델 저장 방법\n",
    "modelpath = './credit_50/{epoch:02d}-{val_loss:.4f}.hdf5'\n",
    "checkpointer =  ModelCheckpoint(filepath=modelpath, monitor='val_loss', verbose=1, save_best_only=True)\n",
    "    \n",
    "# 학습 조기 종료\n",
    "early_stopping_callback= EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "\n",
    "# 모델 학습\n",
    "history = model_1.fit(X_train, Y_train ,validation_split=0.33, epochs=1000, batch_size=20,\n",
    "                    callbacks=[checkpointer, early_stopping_callback])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6b06c118",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 모델 반복횟수에 따른 정확도 및 loss를 그래프로 확인\n",
    "y_vloss = history.history['val_loss']\n",
    "y_acc = history.history['accuracy']\n",
    "\n",
    "x_len = np.arange(len(y_acc))\n",
    "\n",
    "plt.plot(x_len, y_vloss, 'o', c='red',markersize=3)\n",
    "plt.plot(x_len, y_acc, 'o', c='blue',markersize=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "54489731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 749us/step - loss: 0.1546 - accuracy: 0.9527\n",
      "[0.1545546054840088, 0.9527027010917664]\n",
      "가져올 데이터 번호3\n",
      "신용카드를 부정사용 확률은 72.39424586296082%입니다.\n"
     ]
    }
   ],
   "source": [
    "#학습기 불러오기\n",
    "from keras.models import load_model\n",
    "model_1 = load_model('./credit_50/23-0.1362.hdf5')\n",
    "\n",
    "\n",
    "#평가\n",
    "print(model_1 .evaluate(X_test,Y_test))\n",
    "\n",
    "#예측\n",
    "prediction = model_1.predict(X_test)\n",
    "부정사용 = prediction[int(input('가져올 데이터 번호'))]\n",
    "p=부정사용[0]*100\n",
    "print(f'신용카드를 부정사용 확률은 {p}%입니다.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "aefb4253",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0    76\n",
      "1.0    72\n",
      "Name: 0, dtype: int64 \n",
      "\n",
      "1.0    64\n",
      "0.0    42\n",
      "0.1    21\n",
      "0.2     7\n",
      "0.3     6\n",
      "0.6     2\n",
      "0.7     2\n",
      "0.8     2\n",
      "0.5     1\n",
      "0.9     1\n",
      "dtype: int64 \n",
      "\n",
      "0    81\n",
      "1    67\n",
      "Name: 0, dtype: int64 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pred(prediction) 함수 불러와서 y_pred_t에 넣음\n",
    "y_pred_t = pred(prediction)\n",
    "y_pred_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "509e2756",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_t = Y_test.astype(int)\n",
    "Y_test_t = Y_test_t.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "df188d1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(148, 1)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8cf92b24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  3,  23,  33,  64, 114, 126, 131, 145], dtype=int64)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 예측값과 실제값 다른것들만 들고와서 보여주기위함\n",
    "miss = np.where(Y_test_t != y_pred_t)[0]\n",
    "miss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "48f15c82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 몇개 잘못 분류 했나?\n",
    "len(miss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d25c07a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131번째학습데이터의 True값은 : [0]인데, Predic값은 : [1]이다\n"
     ]
    }
   ],
   "source": [
    "i=np.random.choice(miss)\n",
    "print(\"%s번째학습데이터의 True값은 : %s인데, Predic값은 : %s이다\"%(i,Y_test_t[i],y_pred_t[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "19c0c788",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9459459459459459"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 - len(miss)/Y_test_t.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7b7e27",
   "metadata": {},
   "source": [
    "# ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "df8aff29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(242085, 29)\n",
      "(42722, 29)\n"
     ]
    }
   ],
   "source": [
    "X =df.values[:,0:29] #독립변수\n",
    "Y =df.values[:,29] #종속변수 \n",
    "X_train, X_test, Y_train, Y_test= train_test_split(X,Y, test_size = 0.15) #85 : 15니까\n",
    "print(X_train.shape) # 85프로\n",
    "print(X_test.shape) #15프로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "37218c2a",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "163/163 [==============================] - 1s 2ms/step - loss: 0.6418 - accuracy: 0.6653 - val_loss: 0.3032 - val_accuracy: 0.9732\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.30316, saving model to ./credit_ann\\01-0.3032.hdf5\n",
      "Epoch 2/1000\n",
      "163/163 [==============================] - 0s 947us/step - loss: 0.2314 - accuracy: 0.9863 - val_loss: 0.0884 - val_accuracy: 0.9986\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.30316 to 0.08840, saving model to ./credit_ann\\02-0.0884.hdf5\n",
      "Epoch 3/1000\n",
      "163/163 [==============================] - 0s 960us/step - loss: 0.0701 - accuracy: 0.9984 - val_loss: 0.0360 - val_accuracy: 0.9992\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.08840 to 0.03598, saving model to ./credit_ann\\03-0.0360.hdf5\n",
      "Epoch 4/1000\n",
      "163/163 [==============================] - 0s 998us/step - loss: 0.0308 - accuracy: 0.9993 - val_loss: 0.0203 - val_accuracy: 0.9993\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.03598 to 0.02026, saving model to ./credit_ann\\04-0.0203.hdf5\n",
      "Epoch 5/1000\n",
      "163/163 [==============================] - 0s 961us/step - loss: 0.0186 - accuracy: 0.9992 - val_loss: 0.0136 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.02026 to 0.01359, saving model to ./credit_ann\\05-0.0136.hdf5\n",
      "Epoch 6/1000\n",
      "163/163 [==============================] - 0s 968us/step - loss: 0.0126 - accuracy: 0.9994 - val_loss: 0.0102 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.01359 to 0.01016, saving model to ./credit_ann\\06-0.0102.hdf5\n",
      "Epoch 7/1000\n",
      "163/163 [==============================] - 0s 974us/step - loss: 0.0098 - accuracy: 0.9993 - val_loss: 0.0081 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.01016 to 0.00814, saving model to ./credit_ann\\07-0.0081.hdf5\n",
      "Epoch 8/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0075 - accuracy: 0.9994 - val_loss: 0.0068 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.00814 to 0.00684, saving model to ./credit_ann\\08-0.0068.hdf5\n",
      "Epoch 9/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0067 - accuracy: 0.9993 - val_loss: 0.0059 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.00684 to 0.00594, saving model to ./credit_ann\\09-0.0059.hdf5\n",
      "Epoch 10/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0059 - accuracy: 0.9994 - val_loss: 0.0053 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.00594 to 0.00528, saving model to ./credit_ann\\10-0.0053.hdf5\n",
      "Epoch 11/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0056 - accuracy: 0.9994 - val_loss: 0.0048 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.00528 to 0.00481, saving model to ./credit_ann\\11-0.0048.hdf5\n",
      "Epoch 12/1000\n",
      "163/163 [==============================] - 0s 959us/step - loss: 0.0043 - accuracy: 0.9995 - val_loss: 0.0044 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.00481 to 0.00443, saving model to ./credit_ann\\12-0.0044.hdf5\n",
      "Epoch 13/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0042 - accuracy: 0.9995 - val_loss: 0.0042 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.00443 to 0.00416, saving model to ./credit_ann\\13-0.0042.hdf5\n",
      "Epoch 14/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0039 - accuracy: 0.9995 - val_loss: 0.0040 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.00416 to 0.00395, saving model to ./credit_ann\\14-0.0040.hdf5\n",
      "Epoch 15/1000\n",
      "163/163 [==============================] - 0s 959us/step - loss: 0.0044 - accuracy: 0.9994 - val_loss: 0.0038 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.00395 to 0.00383, saving model to ./credit_ann\\15-0.0038.hdf5\n",
      "Epoch 16/1000\n",
      "163/163 [==============================] - 0s 954us/step - loss: 0.0040 - accuracy: 0.9994 - val_loss: 0.0036 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.00383 to 0.00363, saving model to ./credit_ann\\16-0.0036.hdf5\n",
      "Epoch 17/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0037 - accuracy: 0.9995 - val_loss: 0.0035 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.00363 to 0.00351, saving model to ./credit_ann\\17-0.0035.hdf5\n",
      "Epoch 18/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0032 - accuracy: 0.9995 - val_loss: 0.0034 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.00351 to 0.00345, saving model to ./credit_ann\\18-0.0034.hdf5\n",
      "Epoch 19/1000\n",
      "163/163 [==============================] - 0s 990us/step - loss: 0.0036 - accuracy: 0.9994 - val_loss: 0.0034 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.00345 to 0.00337, saving model to ./credit_ann\\19-0.0034.hdf5\n",
      "Epoch 20/1000\n",
      "163/163 [==============================] - 0s 954us/step - loss: 0.0036 - accuracy: 0.9994 - val_loss: 0.0032 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.00337 to 0.00322, saving model to ./credit_ann\\20-0.0032.hdf5\n",
      "Epoch 21/1000\n",
      "163/163 [==============================] - 0s 986us/step - loss: 0.0034 - accuracy: 0.9994 - val_loss: 0.0030 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.00322 to 0.00303, saving model to ./credit_ann\\21-0.0030.hdf5\n",
      "Epoch 22/1000\n",
      "163/163 [==============================] - 0s 967us/step - loss: 0.0029 - accuracy: 0.9995 - val_loss: 0.0030 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.00303 to 0.00298, saving model to ./credit_ann\\22-0.0030.hdf5\n",
      "Epoch 23/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0030 - accuracy: 0.9994 - val_loss: 0.0030 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.00298 to 0.00296, saving model to ./credit_ann\\23-0.0030.hdf5\n",
      "Epoch 24/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0031 - accuracy: 0.9995 - val_loss: 0.0029 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.00296 to 0.00290, saving model to ./credit_ann\\24-0.0029.hdf5\n",
      "Epoch 25/1000\n",
      "163/163 [==============================] - 0s 978us/step - loss: 0.0031 - accuracy: 0.9995 - val_loss: 0.0029 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.00290 to 0.00288, saving model to ./credit_ann\\25-0.0029.hdf5\n",
      "Epoch 26/1000\n",
      "163/163 [==============================] - 0s 965us/step - loss: 0.0026 - accuracy: 0.9996 - val_loss: 0.0029 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.00288 to 0.00286, saving model to ./credit_ann\\26-0.0029.hdf5\n",
      "Epoch 27/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0025 - accuracy: 0.9995 - val_loss: 0.0028 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.00286 to 0.00283, saving model to ./credit_ann\\27-0.0028.hdf5\n",
      "Epoch 28/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0031 - accuracy: 0.9994 - val_loss: 0.0028 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.00283 to 0.00279, saving model to ./credit_ann\\28-0.0028.hdf5\n",
      "Epoch 29/1000\n",
      "163/163 [==============================] - 0s 951us/step - loss: 0.0026 - accuracy: 0.9995 - val_loss: 0.0028 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.00279 to 0.00278, saving model to ./credit_ann\\29-0.0028.hdf5\n",
      "Epoch 30/1000\n",
      "163/163 [==============================] - 0s 939us/step - loss: 0.0034 - accuracy: 0.9993 - val_loss: 0.0028 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.00278\n",
      "Epoch 31/1000\n",
      "163/163 [==============================] - 0s 986us/step - loss: 0.0031 - accuracy: 0.9994 - val_loss: 0.0027 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.00278 to 0.00272, saving model to ./credit_ann\\31-0.0027.hdf5\n",
      "Epoch 32/1000\n",
      "163/163 [==============================] - 0s 964us/step - loss: 0.0028 - accuracy: 0.9994 - val_loss: 0.0027 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.00272 to 0.00271, saving model to ./credit_ann\\32-0.0027.hdf5\n",
      "Epoch 33/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0029 - accuracy: 0.9995 - val_loss: 0.0027 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.00271 to 0.00270, saving model to ./credit_ann\\33-0.0027.hdf5\n",
      "Epoch 34/1000\n",
      "163/163 [==============================] - 0s 978us/step - loss: 0.0025 - accuracy: 0.9995 - val_loss: 0.0027 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.00270 to 0.00268, saving model to ./credit_ann\\34-0.0027.hdf5\n",
      "Epoch 35/1000\n",
      "163/163 [==============================] - 0s 939us/step - loss: 0.0024 - accuracy: 0.9995 - val_loss: 0.0027 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.00268 to 0.00267, saving model to ./credit_ann\\35-0.0027.hdf5\n",
      "Epoch 36/1000\n",
      "163/163 [==============================] - 0s 959us/step - loss: 0.0032 - accuracy: 0.9993 - val_loss: 0.0027 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.00267 to 0.00266, saving model to ./credit_ann\\36-0.0027.hdf5\n",
      "Epoch 37/1000\n",
      "163/163 [==============================] - 0s 943us/step - loss: 0.0025 - accuracy: 0.9995 - val_loss: 0.0026 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.00266 to 0.00264, saving model to ./credit_ann\\37-0.0026.hdf5\n",
      "Epoch 38/1000\n",
      "163/163 [==============================] - 0s 946us/step - loss: 0.0024 - accuracy: 0.9995 - val_loss: 0.0027 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.00264\n",
      "Epoch 39/1000\n",
      "163/163 [==============================] - 0s 945us/step - loss: 0.0022 - accuracy: 0.9995 - val_loss: 0.0026 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.00264 to 0.00262, saving model to ./credit_ann\\39-0.0026.hdf5\n",
      "Epoch 40/1000\n",
      "163/163 [==============================] - 0s 947us/step - loss: 0.0025 - accuracy: 0.9995 - val_loss: 0.0027 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.00262\n",
      "Epoch 41/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0025 - accuracy: 0.9995 - val_loss: 0.0026 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.00262\n",
      "Epoch 42/1000\n",
      "163/163 [==============================] - 0s 969us/step - loss: 0.0026 - accuracy: 0.9995 - val_loss: 0.0026 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.00262\n",
      "Epoch 43/1000\n",
      "163/163 [==============================] - 0s 979us/step - loss: 0.0024 - accuracy: 0.9996 - val_loss: 0.0026 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.00262 to 0.00260, saving model to ./credit_ann\\43-0.0026.hdf5\n",
      "Epoch 44/1000\n",
      "163/163 [==============================] - 0s 949us/step - loss: 0.0024 - accuracy: 0.9994 - val_loss: 0.0026 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.00260\n",
      "Epoch 45/1000\n",
      "163/163 [==============================] - 0s 953us/step - loss: 0.0025 - accuracy: 0.9994 - val_loss: 0.0026 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.00260\n",
      "Epoch 46/1000\n",
      "163/163 [==============================] - 0s 947us/step - loss: 0.0026 - accuracy: 0.9994 - val_loss: 0.0026 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.00260 to 0.00257, saving model to ./credit_ann\\46-0.0026.hdf5\n",
      "Epoch 47/1000\n",
      "163/163 [==============================] - 0s 933us/step - loss: 0.0024 - accuracy: 0.9995 - val_loss: 0.0026 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.00257 to 0.00256, saving model to ./credit_ann\\47-0.0026.hdf5\n",
      "Epoch 48/1000\n",
      "163/163 [==============================] - 0s 948us/step - loss: 0.0024 - accuracy: 0.9995 - val_loss: 0.0026 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.00256\n",
      "Epoch 49/1000\n",
      "163/163 [==============================] - 0s 946us/step - loss: 0.0024 - accuracy: 0.9995 - val_loss: 0.0026 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.00256\n",
      "Epoch 50/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0025 - accuracy: 0.9995 - val_loss: 0.0026 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.00256\n",
      "Epoch 51/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0022 - accuracy: 0.9996 - val_loss: 0.0026 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.00256\n",
      "Epoch 52/1000\n",
      "163/163 [==============================] - 0s 945us/step - loss: 0.0025 - accuracy: 0.9995 - val_loss: 0.0026 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.00256\n",
      "Epoch 53/1000\n",
      "163/163 [==============================] - 0s 972us/step - loss: 0.0022 - accuracy: 0.9996 - val_loss: 0.0026 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.00256\n",
      "Epoch 54/1000\n",
      "163/163 [==============================] - 0s 959us/step - loss: 0.0022 - accuracy: 0.9996 - val_loss: 0.0026 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.00256\n",
      "Epoch 55/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0026 - accuracy: 0.9995 - val_loss: 0.0026 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.00256\n",
      "Epoch 56/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0022 - accuracy: 0.9996 - val_loss: 0.0026 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.00256 to 0.00255, saving model to ./credit_ann\\56-0.0026.hdf5\n",
      "Epoch 57/1000\n",
      "163/163 [==============================] - 0s 950us/step - loss: 0.0022 - accuracy: 0.9995 - val_loss: 0.0026 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.00255 to 0.00255, saving model to ./credit_ann\\57-0.0026.hdf5\n",
      "Epoch 58/1000\n",
      "163/163 [==============================] - 0s 956us/step - loss: 0.0022 - accuracy: 0.9996 - val_loss: 0.0026 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.00255\n",
      "Epoch 59/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0028 - accuracy: 0.9995 - val_loss: 0.0026 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.00255\n",
      "Epoch 60/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0024 - accuracy: 0.9996 - val_loss: 0.0026 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.00255\n",
      "Epoch 61/1000\n",
      "163/163 [==============================] - 0s 971us/step - loss: 0.0028 - accuracy: 0.9995 - val_loss: 0.0026 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.00255\n",
      "Epoch 62/1000\n",
      "163/163 [==============================] - 0s 957us/step - loss: 0.0026 - accuracy: 0.9995 - val_loss: 0.0026 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.00255\n",
      "Epoch 63/1000\n",
      "163/163 [==============================] - 0s 959us/step - loss: 0.0019 - accuracy: 0.9996 - val_loss: 0.0026 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.00255\n",
      "Epoch 64/1000\n",
      "163/163 [==============================] - 0s 955us/step - loss: 0.0018 - accuracy: 0.9997 - val_loss: 0.0026 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.00255\n",
      "Epoch 65/1000\n",
      "163/163 [==============================] - 0s 950us/step - loss: 0.0027 - accuracy: 0.9995 - val_loss: 0.0026 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.00255\n",
      "Epoch 66/1000\n",
      "163/163 [==============================] - 0s 968us/step - loss: 0.0021 - accuracy: 0.9995 - val_loss: 0.0026 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.00255\n",
      "Epoch 67/1000\n",
      "163/163 [==============================] - 0s 965us/step - loss: 0.0022 - accuracy: 0.9996 - val_loss: 0.0026 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.00255\n",
      "Epoch 68/1000\n",
      "163/163 [==============================] - 0s 989us/step - loss: 0.0022 - accuracy: 0.9995 - val_loss: 0.0026 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.00255\n",
      "Epoch 69/1000\n",
      "163/163 [==============================] - 0s 972us/step - loss: 0.0020 - accuracy: 0.9996 - val_loss: 0.0026 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.00255\n",
      "Epoch 70/1000\n",
      "163/163 [==============================] - 0s 948us/step - loss: 0.0020 - accuracy: 0.9996 - val_loss: 0.0026 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.00255\n",
      "Epoch 71/1000\n",
      "163/163 [==============================] - 0s 961us/step - loss: 0.0020 - accuracy: 0.9996 - val_loss: 0.0026 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.00255\n",
      "Epoch 72/1000\n",
      "163/163 [==============================] - 0s 955us/step - loss: 0.0021 - accuracy: 0.9996 - val_loss: 0.0026 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.00255\n",
      "Epoch 73/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163/163 [==============================] - 0s 940us/step - loss: 0.0021 - accuracy: 0.9996 - val_loss: 0.0026 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.00255\n",
      "Epoch 74/1000\n",
      "163/163 [==============================] - 0s 984us/step - loss: 0.0021 - accuracy: 0.9995 - val_loss: 0.0026 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.00255\n",
      "Epoch 75/1000\n",
      "163/163 [==============================] - 0s 954us/step - loss: 0.0023 - accuracy: 0.9995 - val_loss: 0.0026 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.00255\n",
      "Epoch 76/1000\n",
      "163/163 [==============================] - 0s 948us/step - loss: 0.0024 - accuracy: 0.9995 - val_loss: 0.0026 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.00255\n",
      "Epoch 77/1000\n",
      "163/163 [==============================] - 0s 943us/step - loss: 0.0018 - accuracy: 0.9996 - val_loss: 0.0026 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.00255\n",
      "Epoch 78/1000\n",
      "163/163 [==============================] - 0s 945us/step - loss: 0.0019 - accuracy: 0.9997 - val_loss: 0.0025 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00078: val_loss improved from 0.00255 to 0.00255, saving model to ./credit_ann\\78-0.0025.hdf5\n",
      "Epoch 79/1000\n",
      "163/163 [==============================] - 0s 945us/step - loss: 0.0023 - accuracy: 0.9996 - val_loss: 0.0026 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.00255\n",
      "Epoch 80/1000\n",
      "163/163 [==============================] - 0s 942us/step - loss: 0.0017 - accuracy: 0.9997 - val_loss: 0.0026 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.00255\n",
      "Epoch 81/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0022 - accuracy: 0.9996 - val_loss: 0.0026 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.00255\n",
      "Epoch 82/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0018 - accuracy: 0.9997 - val_loss: 0.0026 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.00255\n",
      "Epoch 83/1000\n",
      "163/163 [==============================] - 0s 998us/step - loss: 0.0020 - accuracy: 0.9996 - val_loss: 0.0027 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.00255\n",
      "Epoch 84/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0018 - accuracy: 0.9996 - val_loss: 0.0026 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.00255\n",
      "Epoch 85/1000\n",
      "163/163 [==============================] - 0s 984us/step - loss: 0.0022 - accuracy: 0.9996 - val_loss: 0.0026 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.00255\n",
      "Epoch 86/1000\n",
      "163/163 [==============================] - 0s 934us/step - loss: 0.0023 - accuracy: 0.9995 - val_loss: 0.0026 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.00255\n",
      "Epoch 87/1000\n",
      "163/163 [==============================] - 0s 964us/step - loss: 0.0022 - accuracy: 0.9995 - val_loss: 0.0026 - val_accuracy: 0.9996\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.00255\n",
      "Epoch 88/1000\n",
      "163/163 [==============================] - 0s 971us/step - loss: 0.0021 - accuracy: 0.9996 - val_loss: 0.0028 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.00255\n",
      "Epoch 89/1000\n",
      "163/163 [==============================] - 0s 954us/step - loss: 0.0020 - accuracy: 0.9995 - val_loss: 0.0026 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.00255\n",
      "Epoch 90/1000\n",
      "163/163 [==============================] - 0s 949us/step - loss: 0.0016 - accuracy: 0.9997 - val_loss: 0.0026 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.00255\n",
      "Epoch 91/1000\n",
      "163/163 [==============================] - 0s 976us/step - loss: 0.0018 - accuracy: 0.9996 - val_loss: 0.0026 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.00255\n",
      "Epoch 92/1000\n",
      "163/163 [==============================] - 0s 951us/step - loss: 0.0019 - accuracy: 0.9995 - val_loss: 0.0026 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.00255\n",
      "Epoch 93/1000\n",
      "163/163 [==============================] - 0s 961us/step - loss: 0.0014 - accuracy: 0.9997 - val_loss: 0.0026 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.00255\n",
      "Epoch 94/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0022 - accuracy: 0.9996 - val_loss: 0.0026 - val_accuracy: 0.9996\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.00255\n",
      "Epoch 95/1000\n",
      "163/163 [==============================] - 0s 962us/step - loss: 0.0019 - accuracy: 0.9996 - val_loss: 0.0026 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.00255\n",
      "Epoch 96/1000\n",
      "163/163 [==============================] - 0s 955us/step - loss: 0.0023 - accuracy: 0.9996 - val_loss: 0.0027 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.00255\n",
      "Epoch 97/1000\n",
      "163/163 [==============================] - 0s 957us/step - loss: 0.0017 - accuracy: 0.9996 - val_loss: 0.0026 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.00255\n",
      "Epoch 98/1000\n",
      "163/163 [==============================] - 0s 984us/step - loss: 0.0018 - accuracy: 0.9996 - val_loss: 0.0026 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.00255\n",
      "Epoch 99/1000\n",
      "163/163 [==============================] - 0s 958us/step - loss: 0.0022 - accuracy: 0.9996 - val_loss: 0.0026 - val_accuracy: 0.9996\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.00255\n",
      "Epoch 100/1000\n",
      "163/163 [==============================] - 0s 956us/step - loss: 0.0018 - accuracy: 0.9996 - val_loss: 0.0026 - val_accuracy: 0.9996\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.00255\n",
      "Epoch 101/1000\n",
      "163/163 [==============================] - 0s 943us/step - loss: 0.0020 - accuracy: 0.9996 - val_loss: 0.0026 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.00255\n",
      "Epoch 102/1000\n",
      "163/163 [==============================] - 0s 939us/step - loss: 0.0020 - accuracy: 0.9996 - val_loss: 0.0026 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.00255\n",
      "Epoch 103/1000\n",
      "163/163 [==============================] - 0s 947us/step - loss: 0.0021 - accuracy: 0.9996 - val_loss: 0.0027 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.00255\n",
      "Epoch 104/1000\n",
      "163/163 [==============================] - 0s 943us/step - loss: 0.0017 - accuracy: 0.9996 - val_loss: 0.0027 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.00255\n",
      "Epoch 105/1000\n",
      "163/163 [==============================] - 0s 950us/step - loss: 0.0017 - accuracy: 0.9997 - val_loss: 0.0026 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.00255\n",
      "Epoch 106/1000\n",
      "163/163 [==============================] - 0s 953us/step - loss: 0.0016 - accuracy: 0.9997 - val_loss: 0.0027 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.00255\n",
      "Epoch 107/1000\n",
      "163/163 [==============================] - 0s 946us/step - loss: 0.0018 - accuracy: 0.9996 - val_loss: 0.0027 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.00255\n",
      "Epoch 108/1000\n",
      "163/163 [==============================] - 0s 967us/step - loss: 0.0017 - accuracy: 0.9996 - val_loss: 0.0027 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.00255\n",
      "Epoch 109/1000\n",
      "163/163 [==============================] - 0s 941us/step - loss: 0.0019 - accuracy: 0.9996 - val_loss: 0.0026 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.00255\n",
      "Epoch 110/1000\n",
      "163/163 [==============================] - 0s 945us/step - loss: 0.0020 - accuracy: 0.9996 - val_loss: 0.0027 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.00255\n",
      "Epoch 111/1000\n",
      "163/163 [==============================] - 0s 936us/step - loss: 0.0018 - accuracy: 0.9996 - val_loss: 0.0026 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.00255\n",
      "Epoch 112/1000\n",
      "163/163 [==============================] - 0s 967us/step - loss: 0.0018 - accuracy: 0.9996 - val_loss: 0.0026 - val_accuracy: 0.9996\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.00255\n",
      "Epoch 113/1000\n",
      "163/163 [==============================] - 0s 979us/step - loss: 0.0020 - accuracy: 0.9996 - val_loss: 0.0027 - val_accuracy: 0.9996\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.00255\n",
      "Epoch 114/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0018 - accuracy: 0.9996 - val_loss: 0.0026 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.00255\n",
      "Epoch 115/1000\n",
      "163/163 [==============================] - 0s 951us/step - loss: 0.0018 - accuracy: 0.9996 - val_loss: 0.0026 - val_accuracy: 0.9996\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.00255\n",
      "Epoch 116/1000\n",
      "163/163 [==============================] - 0s 949us/step - loss: 0.0013 - accuracy: 0.9997 - val_loss: 0.0027 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.00255\n",
      "Epoch 117/1000\n",
      "163/163 [==============================] - 0s 999us/step - loss: 0.0015 - accuracy: 0.9997 - val_loss: 0.0027 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.00255\n",
      "Epoch 118/1000\n",
      "163/163 [==============================] - 0s 952us/step - loss: 0.0016 - accuracy: 0.9997 - val_loss: 0.0027 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.00255\n",
      "Epoch 119/1000\n",
      "163/163 [==============================] - 0s 959us/step - loss: 0.0018 - accuracy: 0.9996 - val_loss: 0.0026 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.00255\n",
      "Epoch 120/1000\n",
      "163/163 [==============================] - 0s 956us/step - loss: 0.0017 - accuracy: 0.9996 - val_loss: 0.0027 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.00255\n",
      "Epoch 121/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0018 - accuracy: 0.9996 - val_loss: 0.0027 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.00255\n",
      "Epoch 122/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0018 - accuracy: 0.9996 - val_loss: 0.0027 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.00255\n",
      "Epoch 123/1000\n",
      "163/163 [==============================] - 0s 952us/step - loss: 0.0017 - accuracy: 0.9997 - val_loss: 0.0027 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.00255\n",
      "Epoch 124/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0018 - accuracy: 0.9996 - val_loss: 0.0027 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.00255\n",
      "Epoch 125/1000\n",
      "163/163 [==============================] - 0s 996us/step - loss: 0.0019 - accuracy: 0.9996 - val_loss: 0.0027 - val_accuracy: 0.9996\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.00255\n",
      "Epoch 126/1000\n",
      "163/163 [==============================] - 0s 960us/step - loss: 0.0014 - accuracy: 0.9997 - val_loss: 0.0026 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.00255\n",
      "Epoch 127/1000\n",
      "163/163 [==============================] - 0s 957us/step - loss: 0.0014 - accuracy: 0.9997 - val_loss: 0.0027 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.00255\n",
      "Epoch 128/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0015 - accuracy: 0.9997 - val_loss: 0.0026 - val_accuracy: 0.9996\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.00255\n",
      "Epoch 129/1000\n",
      "163/163 [==============================] - 0s 999us/step - loss: 0.0019 - accuracy: 0.9996 - val_loss: 0.0027 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.00255\n",
      "Epoch 130/1000\n",
      "163/163 [==============================] - 0s 980us/step - loss: 0.0017 - accuracy: 0.9996 - val_loss: 0.0027 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.00255\n",
      "Epoch 131/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0015 - accuracy: 0.9997 - val_loss: 0.0027 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.00255\n",
      "Epoch 132/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0018 - accuracy: 0.9997 - val_loss: 0.0027 - val_accuracy: 0.9994\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.00255\n",
      "Epoch 133/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0015 - accuracy: 0.9997 - val_loss: 0.0027 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.00255\n",
      "Epoch 134/1000\n",
      "163/163 [==============================] - 0s 983us/step - loss: 0.0019 - accuracy: 0.9996 - val_loss: 0.0027 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.00255\n",
      "Epoch 135/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0015 - accuracy: 0.9997 - val_loss: 0.0027 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.00255\n",
      "Epoch 136/1000\n",
      "163/163 [==============================] - 0s 957us/step - loss: 0.0014 - accuracy: 0.9997 - val_loss: 0.0027 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.00255\n",
      "Epoch 137/1000\n",
      "163/163 [==============================] - 0s 981us/step - loss: 0.0013 - accuracy: 0.9997 - val_loss: 0.0028 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.00255\n",
      "Epoch 138/1000\n",
      "163/163 [==============================] - 0s 970us/step - loss: 0.0016 - accuracy: 0.9997 - val_loss: 0.0027 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.00255\n",
      "Epoch 139/1000\n",
      "163/163 [==============================] - 0s 966us/step - loss: 0.0015 - accuracy: 0.9997 - val_loss: 0.0027 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.00255\n",
      "Epoch 140/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0015 - accuracy: 0.9997 - val_loss: 0.0028 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.00255\n",
      "Epoch 141/1000\n",
      "163/163 [==============================] - 0s 998us/step - loss: 0.0019 - accuracy: 0.9996 - val_loss: 0.0026 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.00255\n",
      "Epoch 142/1000\n",
      "163/163 [==============================] - 0s 982us/step - loss: 0.0019 - accuracy: 0.9996 - val_loss: 0.0027 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.00255\n",
      "Epoch 143/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0016 - accuracy: 0.9996 - val_loss: 0.0027 - val_accuracy: 0.9996\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.00255\n",
      "Epoch 144/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0019 - accuracy: 0.9996 - val_loss: 0.0027 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.00255\n",
      "Epoch 145/1000\n",
      "163/163 [==============================] - 0s 992us/step - loss: 0.0016 - accuracy: 0.9996 - val_loss: 0.0027 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.00255\n",
      "Epoch 146/1000\n",
      "163/163 [==============================] - 0s 988us/step - loss: 0.0016 - accuracy: 0.9997 - val_loss: 0.0027 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.00255\n",
      "Epoch 147/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0016 - accuracy: 0.9996 - val_loss: 0.0027 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.00255\n",
      "Epoch 148/1000\n",
      "163/163 [==============================] - 0s 996us/step - loss: 0.0016 - accuracy: 0.9996 - val_loss: 0.0026 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.00255\n",
      "Epoch 149/1000\n",
      "163/163 [==============================] - 0s 997us/step - loss: 0.0016 - accuracy: 0.9997 - val_loss: 0.0027 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.00255\n",
      "Epoch 150/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0017 - accuracy: 0.9997 - val_loss: 0.0028 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.00255\n",
      "Epoch 151/1000\n",
      "163/163 [==============================] - 0s 985us/step - loss: 0.0017 - accuracy: 0.9996 - val_loss: 0.0027 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.00255\n",
      "Epoch 152/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0017 - accuracy: 0.9996 - val_loss: 0.0027 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.00255\n",
      "Epoch 153/1000\n",
      "163/163 [==============================] - 0s 966us/step - loss: 0.0014 - accuracy: 0.9997 - val_loss: 0.0027 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.00255\n",
      "Epoch 154/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0013 - accuracy: 0.9997 - val_loss: 0.0027 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.00255\n",
      "Epoch 155/1000\n",
      "163/163 [==============================] - 0s 958us/step - loss: 0.0017 - accuracy: 0.9996 - val_loss: 0.0027 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.00255\n",
      "Epoch 156/1000\n",
      "163/163 [==============================] - 0s 1ms/step - loss: 0.0016 - accuracy: 0.9996 - val_loss: 0.0028 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.00255\n",
      "Epoch 157/1000\n",
      "163/163 [==============================] - 0s 966us/step - loss: 0.0014 - accuracy: 0.9997 - val_loss: 0.0027 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.00255\n",
      "Epoch 158/1000\n",
      "163/163 [==============================] - 0s 953us/step - loss: 0.0015 - accuracy: 0.9997 - val_loss: 0.0027 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.00255\n",
      "Epoch 159/1000\n",
      "163/163 [==============================] - 0s 950us/step - loss: 0.0015 - accuracy: 0.9997 - val_loss: 0.0028 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.00255\n",
      "Epoch 160/1000\n",
      "163/163 [==============================] - 0s 963us/step - loss: 0.0015 - accuracy: 0.9997 - val_loss: 0.0027 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 0.00255\n",
      "Epoch 161/1000\n",
      "163/163 [==============================] - 0s 988us/step - loss: 0.0015 - accuracy: 0.9997 - val_loss: 0.0027 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.00255\n",
      "Epoch 162/1000\n",
      "163/163 [==============================] - 0s 987us/step - loss: 0.0017 - accuracy: 0.9996 - val_loss: 0.0027 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.00255\n",
      "Epoch 163/1000\n",
      "163/163 [==============================] - 0s 954us/step - loss: 0.0017 - accuracy: 0.9996 - val_loss: 0.0027 - val_accuracy: 0.9996\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.00255\n",
      "Epoch 164/1000\n",
      "163/163 [==============================] - 0s 955us/step - loss: 0.0017 - accuracy: 0.9997 - val_loss: 0.0027 - val_accuracy: 0.9996\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.00255\n",
      "Epoch 165/1000\n",
      "163/163 [==============================] - 0s 936us/step - loss: 0.0014 - accuracy: 0.9997 - val_loss: 0.0028 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.00255\n",
      "Epoch 166/1000\n",
      "163/163 [==============================] - 0s 968us/step - loss: 0.0016 - accuracy: 0.9996 - val_loss: 0.0027 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.00255\n",
      "Epoch 167/1000\n",
      "163/163 [==============================] - 0s 940us/step - loss: 0.0014 - accuracy: 0.9997 - val_loss: 0.0027 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 0.00255\n",
      "Epoch 168/1000\n",
      "163/163 [==============================] - 0s 953us/step - loss: 0.0013 - accuracy: 0.9997 - val_loss: 0.0027 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.00255\n",
      "Epoch 169/1000\n",
      "163/163 [==============================] - 0s 950us/step - loss: 0.0014 - accuracy: 0.9997 - val_loss: 0.0027 - val_accuracy: 0.9996\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 0.00255\n",
      "Epoch 170/1000\n",
      "163/163 [==============================] - 0s 956us/step - loss: 0.0018 - accuracy: 0.9996 - val_loss: 0.0027 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.00255\n",
      "Epoch 171/1000\n",
      "163/163 [==============================] - 0s 942us/step - loss: 0.0012 - accuracy: 0.9997 - val_loss: 0.0028 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.00255\n",
      "Epoch 172/1000\n",
      "163/163 [==============================] - 0s 965us/step - loss: 0.0012 - accuracy: 0.9997 - val_loss: 0.0027 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.00255\n",
      "Epoch 173/1000\n",
      "163/163 [==============================] - 0s 956us/step - loss: 0.0013 - accuracy: 0.9997 - val_loss: 0.0027 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.00255\n",
      "Epoch 174/1000\n",
      "163/163 [==============================] - 0s 925us/step - loss: 0.0016 - accuracy: 0.9997 - val_loss: 0.0027 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.00255\n",
      "Epoch 175/1000\n",
      "163/163 [==============================] - 0s 993us/step - loss: 0.0018 - accuracy: 0.9996 - val_loss: 0.0028 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 0.00255\n",
      "Epoch 176/1000\n",
      "163/163 [==============================] - 0s 977us/step - loss: 0.0014 - accuracy: 0.9997 - val_loss: 0.0027 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.00255\n",
      "Epoch 177/1000\n",
      "163/163 [==============================] - 0s 947us/step - loss: 0.0012 - accuracy: 0.9997 - val_loss: 0.0027 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 0.00255\n",
      "Epoch 178/1000\n",
      "163/163 [==============================] - 0s 954us/step - loss: 0.0014 - accuracy: 0.9997 - val_loss: 0.0027 - val_accuracy: 0.9995\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.00255\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              (None, 14)                420       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 15        \n",
      "=================================================================\n",
      "Total params: 435\n",
      "Trainable params: 435\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 모델 설정\n",
    "model_2 = Sequential()\n",
    "model_2.add(Dense(14, activation='tanh', input_dim=X.shape[1]))\n",
    "model_2.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# 모델 컴파일\n",
    "model_2.compile(loss='binary_crossentropy',\n",
    "             optimizer='adam',\n",
    "             metrics= ['accuracy'])\n",
    "\n",
    "# 모델 저장 폴더 지정\n",
    "MODEL_DIR='./credit_ann/'\n",
    "if not os.path.exists(MODEL_DIR):\n",
    "    os.mkdir(MODEL_DIR)\n",
    "\n",
    "# 모델 저장 방법\n",
    "modelpath = './credit_ann/{epoch:02d}-{val_loss:.4f}.hdf5'\n",
    "checkpointer =  ModelCheckpoint(filepath=modelpath, monitor='val_loss', verbose=1, save_best_only=True)\n",
    "    \n",
    "# 학습 조기 종료 \n",
    "early_stopping_callback= EarlyStopping(monitor='val_loss', patience=100)\n",
    "\n",
    "\n",
    "#모델 학습\n",
    "history = model_2.fit(X_train, Y_train ,validation_split=0.33, epochs=1000, batch_size=1000,\n",
    "                    callbacks=[checkpointer, early_stopping_callback])\n",
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cf07f826",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Z1A+gAAAACXBIWXMAAAsTAAALEwEAmpwYAAASDElEQVR4nO3dfYxd913n8ffHdh0QlHaJB6jyUKcrd7URQiQdhY54Gsllcawlhi0LDovCQ0SEIIiKJ6Uqykbhj1AqUBdtlhK2VUkFDWV5WEvrKm28nUWik5JJk6Z5INQNgTikjTGoIBXiJP7uH+eMczO+M3Md3/G995f3Sxqdc37nN/d8/Zvx5575nXvvSVUhSZp92yZdgCRpPAx0SWqEgS5JjTDQJakRBrokNWLHpA68a9eu2r1796QOL0kz6f777//7qpobtm9igb57925WVlYmdXhJmklJ/ma9fU65SFIjDHRJaoSBLkmNMNAlqREGuiQ1YtNAT/KBJM8meXid/Unym0mOJnkoyZXjL1OStJlRXrb4QeC/A3eus/9qYE//9S3Ab/XL82J5Ge7sK7viCjhxAi688KXlAw+cuW9Y2yvdN239rXU6+lurtW7U/8QJWFyEhQXGatNAr6o/S7J7gy4HgDur+xzee5O8PskbquqZcRW5nuXlblBOnnypLYGql5aDNmp7pfumrb+1Tkd/a7XWjfpv2wYXXABHjow31Mcxh34R8NTA9rG+7QxJbkiykmTl+PHj53TQ5WW45RZ4/vmXt68O5LCPed+o7ZXum7b+1jod/a3VWjfqf+pUdyK6tHRm33NxXi+KVtUdVTVfVfNzc0PfuTqS5WXYuxfuuWf4M+Hgcti+s+0/zsey1uk49lb3t1Zr3aj/tm2wc2c3wzBO43jr/9PAJQPbF/dtW2ZpqXt2O3WqG5j5ebjySuftJn3slmtt+d9mra+iOfQRHAJuTHIX3cXQL231/PniYvfsdvJkt3zve8c/MJI0azYN9CQfBhaBXUmOAf8VeA1AVb0POAzsB44CXwZ+bKuKXbWw0F1MWFrammc5SZpFmdRNoufn58tPW5Sks5Pk/qqaH7bPd4pKUiMMdElqhIEuSY0w0CWpEQa6JDXCQJekRhjoktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RGGOiS1AgDXZIaYaBLUiMMdElqhIEuSY0w0CWpEQa6JDXCQJekRhjoktSIkQI9yb4kjyc5muSmIfsvTfKJJA8keSjJ/vGXKknayKaBnmQ7cDtwNXA5cG2Sy9d0+2XgI1V1BXAQ+B/jLnTV8jLcdlu3lCS9ZMcIfa4CjlbVEwBJ7gIOAI8O9Cnga/r11wF/N84iVy0vw969cPIk7NwJR47AwsJWHEmSZs8oUy4XAU8NbB/r2wbdAvxwkmPAYeBnhj1QkhuSrCRZOX78+FkXu7TUhfmLL3bLpaWzfghJata4LopeC3ywqi4G9gMfSnLGY1fVHVU1X1Xzc3NzZ32QxcXuzHz79m65uHiuZUtSO0aZcnkauGRg++K+bdD1wD6AqlpO8hXALuDZcRS5amGhm2ZZWurC3OkWSXrJKIF+H7AnyWV0QX4Q+KE1ff4W2At8MMm/B74COPs5lREsLBjkkjTMplMuVfUCcCNwN/AY3atZHklya5Jr+m4/D/xEks8AHwZ+tKpqq4qWJJ1plDN0quow3cXOwbabB9YfBb51vKVJks6G7xSVpEYY6JLUCANdkhphoEtSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RGGOiS1AgDXZIaYaBLUiMMdElqhIEuSY0w0CWpEQa6JDXCQJekRhjoktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5JjRgp0JPsS/J4kqNJblqnzw8keTTJI0l+f7xlSpI2s2OzDkm2A7cD3wUcA+5LcqiqHh3oswd4J/CtVfWPSb5uqwqWJA03yhn6VcDRqnqiqk4CdwEH1vT5CeD2qvpHgKp6drxlSpI2M0qgXwQ8NbB9rG8b9GbgzUn+PMm9SfYNe6AkNyRZSbJy/PjxV1axJGmocV0U3QHsARaBa4HfSfL6tZ2q6o6qmq+q+bm5uTEdWpIEowX608AlA9sX922DjgGHqur5qvpr4K/oAl6SdJ6MEuj3AXuSXJZkJ3AQOLSmz5/SnZ2TZBfdFMwT4ytTkrSZTQO9ql4AbgTuBh4DPlJVjyS5Nck1fbe7gRNJHgU+AfxiVZ3YqqIlSWdKVU3kwPPz87WysjKRY0vSrEpyf1XND9vnO0UlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RGGOiS1AgDXZIaYaBLUiMMdElqhIEuSY0w0CWpEQa6JDXCQJekRhjoktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWrESIGeZF+Sx5McTXLTBv3enqSSzI+vREnSKDYN9CTbgduBq4HLgWuTXD6k32uBnwU+Ne4iJUmbG+UM/SrgaFU9UVUngbuAA0P6/QrwbuBfx1ifJGlEowT6RcBTA9vH+rbTklwJXFJV/2ejB0pyQ5KVJCvHjx8/62IlSes754uiSbYBvwH8/GZ9q+qOqpqvqvm5ublzPbQkacAogf40cMnA9sV926rXAt8ILCV5EngrcMgLo5J0fo0S6PcBe5JclmQncBA4tLqzqr5UVbuqandV7QbuBa6pqpUtqViSNNSmgV5VLwA3AncDjwEfqapHktya5JqtLlCSNJodo3SqqsPA4TVtN6/Td/Hcy5IknS3fKSpJjTDQJakRBrokNcJAl6RGGOiS1AgDXZIaYaBLUiMMdElqhIEuSY0w0CWpEQa6JDVi9gJ9eRluu61bSpJOG+nDuabG8jLs3QsnT8LOnXDkCCwsTLoqSZoKs3WGvrTUhfmLL3bLpaVJVyRJU2O2An1xsTsz3769Wy4uTroiSZoaszXlsrDQTbMsLXVh7nSLJJ02W4EOXYgb5JJ0htmacpEkrctAl6RGGOiS1AgDXZIaYaBLUiMMdElqhIEuSY0w0CWpEQa6JDXCQJekRhjoktQIA12SGmGgS1IjDHRJasRIgZ5kX5LHkxxNctOQ/T+X5NEkDyU5kuSN4y9VkrSRTQM9yXbgduBq4HLg2iSXr+n2ADBfVd8E/C/g18ZdqCRpY6OcoV8FHK2qJ6rqJHAXcGCwQ1V9oqq+3G/eC1w83jIlSZsZJdAvAp4a2D7Wt63neuCjw3YkuSHJSpKV48ePj16lJGlTY70omuSHgXngPcP2V9UdVTVfVfNzc3PjPLQkveqNck/Rp4FLBrYv7tteJsnbgHcB31lVz42nPEnSqEY5Q78P2JPksiQ7gYPAocEOSa4Afhu4pqqeHX+ZkqTNbBroVfUCcCNwN/AY8JGqeiTJrUmu6bu9B/hq4A+TPJjk0DoPJ0naIqNMuVBVh4HDa9puHlh/25jrkiSdJd8pKkmNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RGGOiS1AgDXZIaMbuBvrwMt93WLSVJo30e+tRZXoa9e+HkSdi5E44cgYWFSVclSRM1m2foS0tdmL/4YrdcWpp0RZI0cbMZ6IuL3Zn59u3dcnFx0hVJ0sTN5pTLwkI3zbK01IW50y2SNKOBDl2IG+SSdNpsTrlIks5goEtSIwx0SWqEgS5JjZj9QPcdo5IEzPKrXMB3jErSgNk+Q/cdo5J02mwH+uo7RrdtgwQuvHDSFUnSxMx2oC8swHvf230EwKlT8I53OJcu6VVrtufQAU6c6ML81Cl47jm45RZ4+9u7dj8WQNKryEiBnmQf8N+A7cD/rKpfXbP/AuBO4C3ACeAHq+rJ8Za6jtVpl+ee60L94x+Hj32sm4bZsQP274dv+Aa44oou5C+8EB54oPve664z8CU1I1W1cYdkO/BXwHcBx4D7gGur6tGBPj8FfFNV/WSSg8D3VdUPbvS48/PztbKycq71d5aXuzPze+7pQn09CQz+e1/zGrj++uFhP9i20b5p62+t09HfWq11o/7nMIOQ5P6qmh+6s6o2/AIWgLsHtt8JvHNNn7uBhX59B/D39E8W63295S1vqbH65CervvIrq7Ztq4KqpFuO+jWs/2rbRvumrb+1Tkd/a7XWjfpv29bl1Sc/edZRB6wMy9SqGmnK5SLgqYHtY8C3rNenql5I8iXgwj7YB59ZbgBuALj00ktHOPRZGPxI3dVnx/e/H55/frCAbniHGda+2rbRvmnrb63T0d9arXWj/qdOvfRS6zFO+57Xi6JVdQdwB3RTLmM/wNqP1L3uOrjzzm597Z9CX/gCfPSjXeCfOjU87FfbNto3bf2tdTr6W6u1btR/27YtuTnPKIH+NHDJwPbFfduwPseS7ABeR3dxdLI2+8z05eWXzuinYV7t1TbHOEu1tvxvs9aZmkPfyCiBfh+wJ8lldMF9EPihNX0OAT8CLAPfD/zffq5nunmTDEkN2TTQ+znxG+kufG4HPlBVjyS5lW5y/hDwfuBDSY4C/0AX+pKk82ikOfSqOgwcXtN288D6vwL/ebylSZLOxmy/9V+SdJqBLkmNMNAlqREGuiQ1YtPPctmyAyfHgb95hd++izXvQp1i1ro1rHVrWOvWGGetb6yquWE7Jhbo5yLJSq334TRTxlq3hrVuDWvdGuerVqdcJKkRBrokNWJWA/2OSRdwFqx1a1jr1rDWrXFeap3JOXRJ0plm9QxdkrSGgS5JjZi5QE+yL8njSY4muWnS9QxKckmSTyR5NMkjSX62b78lydNJHuy/9k+6VoAkTyb5bF/TSt/2tUk+nuRz/fLfTEGd/25g7B5M8k9J3jEt45rkA0meTfLwQNvQcUznN/vf34eSXDkFtb4nyV/29fxJktf37buT/MvA+L5vCmpd92ee5J39uD6e5LunoNY/GKjzySQP9u1bN67r3ZtuGr/oPr7388CbgJ3AZ4DLJ13XQH1vAK7s119Ld3Pty4FbgF+YdH1D6n0S2LWm7deAm/r1m4B3T7rOIb8DXwDeOC3jCnwHcCXw8GbjCOwHPgoEeCvwqSmo9T8AO/r1dw/Uunuw35SM69Cfef//7DPABcBlfU5sn2Sta/b/OnDzVo/rrJ2hXwUcraonquokcBdwYMI1nVZVz1TVp/v1fwYeo7vf6iw5APxuv/67wPdOrpSh9gKfr6pX+i7jsauqP6O7D8Cg9cbxAHBnde4FXp/kDeelUIbXWlUfq6oX+s176e5KNnHrjOt6DgB3VdVzVfXXwFG6vDgvNqo1SYAfAD681XXMWqAPu2H1VAZmkt3AFcCn+qYb+z9pPzAN0xi9Aj6W5P7+Bt4AX19Vz/TrXwC+fjKlresgL/+PMY3jCuuP47T/Dv843V8Qqy5L8kCS/5fk2ydV1BrDfubTPK7fDnyxqj430LYl4zprgT4Tknw18EfAO6rqn4DfAv4t8M3AM3R/fk2Db6uqK4GrgZ9O8h2DO6v7+3BqXteaZCdwDfCHfdO0juvLTNs4rifJu4AXgN/rm54BLq2qK4CfA34/yddMqr7eTPzM17iWl5+EbNm4zlqgj3LD6olK8hq6MP+9qvpjgKr6YlW9WFWngN/hPP4puJGqerpfPgv8CV1dX1ydAuiXz06uwjNcDXy6qr4I0zuuvfXGcSp/h5P8KPAfgf/SPwHRT1+c6Nfvp5uXfvPEimTDn/m0jusO4D8Bf7DatpXjOmuBfvqG1f3Z2kG6G1RPhX6u7P3AY1X1GwPtg3Ok3wc8vPZ7z7ckX5XktavrdBfGHualG37TL//3ZCoc6mVnOtM4rgPWG8dDwHX9q13eCnxpYGpmIpLsA34JuKaqvjzQPpdke7/+JmAP8MRkqjxd03o/80PAwSQXpLuh/R7gL853fUO8DfjLqjq22rCl43q+rgKP8WryfrpXj3weeNek61lT27fR/Wn9EPBg/7Uf+BDw2b79EPCGKaj1TXSvCvgM8MjqWAIXAkeAzwH3AF876Vr7ur4KOAG8bqBtKsaV7knmGeB5urnb69cbR7pXt9ze//5+FpifglqP0s0/r/7Ovq/v+/b+d+NB4NPA90xBrev+zIF39eP6OHD1pGvt2z8I/OSavls2rr71X5IaMWtTLpKkdRjoktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqRH/HyE9OLFxD/HfAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 모델 반복횟수에 따른 정확도 및 loss를 그래프로 확인\n",
    "y_vloss = history.history['val_loss']\n",
    "y_acc = history.history['accuracy']\n",
    "\n",
    "x_len = np.arange(len(y_acc))\n",
    "\n",
    "\n",
    "plt.plot(x_len, y_vloss, 'o', c='red',markersize=3)\n",
    "plt.plot(x_len, y_acc, 'o', c='blue',markersize=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b4662d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1336/1336 [==============================] - 1s 368us/step - loss: 0.0026 - accuracy: 0.9995\n",
      "[0.0026411355938762426, 0.9995084404945374]\n",
      "가져올 데이터 번호3\n",
      "신용카드를 부정사용 확률은 0.017529726028442383%입니다.\n"
     ]
    }
   ],
   "source": [
    "#학습기 불러오기\n",
    "from keras.models import load_model\n",
    "model_2 = load_model('./credit_ann/78-0.0025.hdf5')\n",
    "\n",
    "\n",
    "#평가\n",
    "print(model_2 .evaluate(X_test,Y_test))\n",
    "\n",
    "#예측\n",
    "prediction = model_2.predict(X_test)\n",
    "부정사용 = prediction[int(input('가져올 데이터 번호'))]\n",
    "p=부정사용[0]*100\n",
    "print(f'신용카드를 부정사용 확률은 {p}%입니다.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3a907ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0    42658\n",
      "1.0       64\n",
      "Name: 0, dtype: int64 \n",
      "\n",
      "0.0    42627\n",
      "1.0       49\n",
      "0.1       16\n",
      "0.9       10\n",
      "0.2        7\n",
      "0.5        4\n",
      "0.3        3\n",
      "0.8        3\n",
      "0.7        2\n",
      "0.4        1\n",
      "dtype: int64 \n",
      "\n",
      "0    42660\n",
      "1       62\n",
      "Name: 0, dtype: int64 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       ...,\n",
       "       [0],\n",
       "       [0],\n",
       "       [0]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_t = pred(prediction)\n",
    "y_pred_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3a3e70f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_t = Y_test.astype(int)\n",
    "Y_test_t = Y_test_t.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a1d30fb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42722, 1)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "49a7e874",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  263,  1646,  7163,  8038, 10622, 11302, 14409, 15062, 15653,\n",
       "       15691, 19037, 19389, 20300, 20370, 22413, 23680, 25593, 27104,\n",
       "       27410, 32573, 36547], dtype=int64)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 예측값과 실제값 다른것들만 들고와서 보여주기위함\n",
    "miss = np.where(Y_test_t != y_pred_t)[0]\n",
    "miss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5db6f141",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 몇개 잘못 분류 했나?\n",
    "len(miss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8b914a95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36547번째학습데이터의 True값은 : [1]인데, Predic값은 : [0]이다\n"
     ]
    }
   ],
   "source": [
    "i=np.random.choice(miss)\n",
    "print(\"%s번째학습데이터의 True값은 : %s인데, Predic값은 : %s이다\"%(i,Y_test_t[i],y_pred_t[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2a78a572",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9995084499789336"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 - len(miss)/Y_test_t.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052ccf6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
