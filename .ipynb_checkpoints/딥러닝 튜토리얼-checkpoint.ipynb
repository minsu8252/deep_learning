{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b4bce5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25820c6b",
   "metadata": {},
   "source": [
    "# Overview the Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9864dc55",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.46666667 0.4745098  0.47843137 ... 0.5176471  0.5137255  0.5019608 ]\n",
      "  [0.49803922 0.4862745  0.4862745  ... 0.5254902  0.5176471  0.50980395]\n",
      "  [0.54509807 0.49411765 0.49019608 ... 0.5294118  0.5254902  0.5137255 ]\n",
      "  ...\n",
      "  [0.5019608  0.5137255  0.5176471  ... 0.5529412  0.54509807 0.53333336]\n",
      "  [0.49803922 0.5058824  0.5137255  ... 0.54509807 0.53333336 0.52156866]\n",
      "  [0.49019608 0.49803922 0.5019608  ... 0.5294118  0.52156866 0.50980395]]\n",
      "\n",
      " [[0.59607846 0.60784316 0.61960787 ... 0.4627451  0.4509804  0.44313726]\n",
      "  [0.60784316 0.61960787 0.6313726  ... 0.4745098  0.4627451  0.4509804 ]\n",
      "  [0.6156863  0.627451   0.6431373  ... 0.4862745  0.4745098  0.4627451 ]\n",
      "  ...\n",
      "  [0.50980395 0.5176471  0.5254902  ... 0.40392157 0.39607844 0.3882353 ]\n",
      "  [0.5019608  0.50980395 0.5176471  ... 0.4        0.39215687 0.38431373]\n",
      "  [0.49411765 0.5058824  0.50980395 ... 0.3882353  0.38039216 0.37254903]]\n",
      "\n",
      " [[0.5882353  0.6039216  0.61960787 ... 0.53333336 0.5137255  0.5019608 ]\n",
      "  [0.5921569  0.6117647  0.6156863  ... 0.5411765  0.52156866 0.50980395]\n",
      "  [0.5882353  0.61960787 0.627451   ... 0.54901963 0.5294118  0.5137255 ]\n",
      "  ...\n",
      "  [0.4745098  0.4862745  0.49411765 ... 0.42745098 0.41960785 0.40784314]\n",
      "  [0.47058824 0.47843137 0.4862745  ... 0.41568628 0.4117647  0.4       ]\n",
      "  [0.45882353 0.47058824 0.47843137 ... 0.40392157 0.4        0.3882353 ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.5254902  0.54509807 0.5568628  ... 0.49803922 0.49019608 0.4862745 ]\n",
      "  [0.5372549  0.5529412  0.5686275  ... 0.5019608  0.49803922 0.49019608]\n",
      "  [0.54509807 0.5568628  0.57254905 ... 0.5058824  0.49803922 0.49019608]\n",
      "  ...\n",
      "  [0.5058824  0.50980395 0.5137255  ... 0.4862745  0.47843137 0.47058824]\n",
      "  [0.49411765 0.5058824  0.50980395 ... 0.47843137 0.47058824 0.4627451 ]\n",
      "  [0.48235294 0.49411765 0.5058824  ... 0.47058824 0.4627451  0.45490196]]\n",
      "\n",
      " [[0.5137255  0.5254902  0.5372549  ... 0.45490196 0.44705883 0.43529412]\n",
      "  [0.5176471  0.53333336 0.54901963 ... 0.46666667 0.45882353 0.44705883]\n",
      "  [0.5254902  0.5411765  0.56078434 ... 0.4745098  0.4627451  0.4509804 ]\n",
      "  ...\n",
      "  [0.53333336 0.5411765  0.5529412  ... 0.54509807 0.5372549  0.5294118 ]\n",
      "  [0.52156866 0.53333336 0.54509807 ... 0.5372549  0.5294118  0.52156866]\n",
      "  [0.5137255  0.5254902  0.5372549  ... 0.5294118  0.5254902  0.5137255 ]]\n",
      "\n",
      " [[0.92941177 0.9372549  0.9372549  ... 0.9647059  0.972549   0.972549  ]\n",
      "  [0.92941177 0.93333334 0.93333334 ... 0.95686275 0.9607843  0.9647059 ]\n",
      "  [0.93333334 0.93333334 0.93333334 ... 0.9490196  0.9490196  0.9529412 ]\n",
      "  ...\n",
      "  [0.76862746 0.76862746 0.77254903 ... 0.73333335 0.74509805 0.68235296]\n",
      "  [0.7647059  0.7647059  0.76862746 ... 0.7254902  0.7372549  0.6784314 ]\n",
      "  [0.75686276 0.75686276 0.7607843  ... 0.72156864 0.73333335 0.67058825]]]\n"
     ]
    }
   ],
   "source": [
    "# data load\n",
    "x_1 = np.load('X.npy')\n",
    "y_1 = np.load('Y.npy')\n",
    "img_size = 64\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(x_1[260].reshape(img_size, img_size))\n",
    "plt.axis('off')\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(x_1[900].reshape(img_size, img_size))\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6175b850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (410, 64, 64)\n",
      "Y shape: (410, 1)\n"
     ]
    }
   ],
   "source": [
    "X = np.concatenate((x_1[204:409], x_1[822:1027]), axis=0)\n",
    "Z = np.zeros(205)\n",
    "O = np.ones(205)\n",
    "\n",
    "Y = np.concatenate((Z,O), axis=0).reshape(X.shape[0],1)\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"Y shape:\", Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a2e6ee83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "348 62\n"
     ]
    }
   ],
   "source": [
    "# train set, test set create\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.15, random_state=42)\n",
    "number_of_train = X_train.shape[0]\n",
    "number_of_test = X_test.shape[0]\n",
    "print(number_of_train, number_of_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dd295197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(348, 4096) (62, 4096)\n"
     ]
    }
   ],
   "source": [
    "# 차원을 변경\n",
    "X_train_flattern=X_train.reshape(number_of_train, X_train.shape[1]*X_train.shape[2])\n",
    "X_test_flattern=X_test.reshape(number_of_test, X_test.shape[1]*X_test.shape[2])\n",
    "\n",
    "#reshape 함수 형식 알아보기\n",
    "\n",
    "print(X_train_flattern.shape, X_test_flattern.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1c52e27f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4096, 348)\n"
     ]
    }
   ],
   "source": [
    "# 행렬을 전치\n",
    "\n",
    "x_train = X_train_flattern.T\n",
    "x_test = X_test_flattern.T\n",
    "y_train = Y_train.T\n",
    "y_test = Y_test.T\n",
    "\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c05623",
   "metadata": {},
   "source": [
    "# logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c051a98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy(param):\n",
    "    dummy_param = param +5\n",
    "    return dummy_param\n",
    "result = dummy(3)\n",
    "\n",
    "def initialize_weight_and_bias(dimension):\n",
    "    w = np.full((dimension , 1), 0.01)\n",
    "    b = 0.0\n",
    "    return w,b\n",
    "\n",
    "def sigmoid(x):\n",
    "    y_head= 1/(1+np.e**(-x))\n",
    "    return y_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2839fe5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7b52c447",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(w, b, x_train, y_train):\n",
    "    x = np.dot(w.T, x_train) +b # dot함수는 내적  다시 알아보기\n",
    "    y_head = sigmoid(x)\n",
    "    loss = -y_train*np.log(y_head)-(1-y_train)*np.log(1-y_head)\n",
    "    cost = np.sum(loss)/x_train.shape[1]\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "ec9f16e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.01]\n",
      " [0.01]\n",
      " [0.01]\n",
      " ...\n",
      " [0.01]\n",
      " [0.01]\n",
      " [0.01]]\n",
      "0.0\n",
      "(4096, 1)\n",
      "<class 'float'>\n",
      "14.014222401929988\n"
     ]
    }
   ],
   "source": [
    "w,b=initialize_weight_and_bias(4096)\n",
    "print(w)\n",
    "print(b)\n",
    "print(w.shape)\n",
    "print(type(b))\n",
    "cost = forward_propagation(w, b, x_train, y_train)\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483c978b",
   "metadata": {},
   "source": [
    "# gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "443a6f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weight_and_bias(dimension):\n",
    "    w = np.full((dimension , 1), 0.01)\n",
    "    b = 0.0\n",
    "    return w,b\n",
    "\n",
    "def forward_backward_propagation(w,b, x_train, y_train):\n",
    "    x = np.dot(w.T, x_train) +b # dot함수는 내적  다시 알아보기\n",
    "    y_head = sigmoid(x)\n",
    "    loss = -y_train*np.log(y_head)-(1-y_train)*np.log(1-y_head)\n",
    "    cost = np.sum(loss)/x_train.shape[1]\n",
    "    \n",
    "    #backward propagation\n",
    "    derivate_weight = np.dot(x_train,((y_head-y_train).T))/x_train.shape[1]\n",
    "    derivate_bias = np.sum(y_head-y_train)/x_train.shape[1]\n",
    "    gradients = {\"derivate_weight\":derivate_weight, \"derivate_bias\": derivate_bias}\n",
    "    return cost, gradients\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "0acbdc6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.01]\n",
      " [0.01]\n",
      " [0.01]\n",
      " ...\n",
      " [0.01]\n",
      " [0.01]\n",
      " [0.01]]\n",
      "0.0\n",
      "(4096, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'float'>\n",
      "14.014222401929988 {'derivate_weight': array([[0.26381565],\n",
      "       [0.2691909 ],\n",
      "       [0.27409286],\n",
      "       ...,\n",
      "       [0.24135678],\n",
      "       [0.23780708],\n",
      "       [0.23305161]]), 'derivate_bias': 0.4942528735619787}\n"
     ]
    }
   ],
   "source": [
    "w,b=initialize_weight_and_bias(4096)\n",
    "print(w)\n",
    "print(b)\n",
    "print(w.shape)\n",
    "print(type(w))\n",
    "print(type(b))\n",
    "\n",
    "\n",
    "cost , derivate_dict = forward_backward_propagation(w,b, x_train, y_train)\n",
    "\n",
    "print(cost, derivate_dict)\n",
    "# print(derivate_dict['derivate_weight'])\n",
    "# print(derivate_dict['derivate_weight'].shape)\n",
    "# print(derivate_dict['derivate_bias'])\n",
    "# print(derivate_dict['derivate_bias'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "ea61a4dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan {'derivate_weight': array([[0.26381565],\n",
      "       [0.2691909 ],\n",
      "       [0.27409286],\n",
      "       ...,\n",
      "       [0.24135678],\n",
      "       [0.23780708],\n",
      "       [0.23305161]]), 'derivate_bias': 0.4942528735632184}\n"
     ]
    }
   ],
   "source": [
    "w = derivate_dict['derivate_weight']\n",
    "b = derivate_dict['derivate_bias']\n",
    "b=float(b)\n",
    "\n",
    "cost , derivate_dict = forward_backward_propagation(w,b, x_train, y_train)\n",
    "\n",
    "print(cost, derivate_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "59a314f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_backward_propagation(w ,b ,x_train, y_train):\n",
    "    x = np.dot(w.T , x_train)+b\n",
    "    y_head = sigmoid(x)\n",
    "    loss = -y_train*np.log(y_head)-(1-y_train)*np.log(1-y_head)\n",
    "    cost = np.sum(loss)/x_train.shape[1]\n",
    "    print(cost)\n",
    "    #backward propagation\n",
    "    derivative_weight = np.dot(x_train,((y_head-y_train).T))/x_train.shape[1]\n",
    "    print(derivate_weight)\n",
    "    derivative_bias = np.sum(y_head-y_train)/x_train.shape[1]\n",
    "    gradients = {\"derivative_weight\":derivative_weight, \"derivative_bias\":derivative_bias }\n",
    "    return cost , gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "ee2bbbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updating(learning) parameters\n",
    "def update(w, b, x_train, y_train, learning_rate,number_of_iterarion):\n",
    "    cost_list = []\n",
    "    cost_list2 = []\n",
    "    index = []\n",
    "    # updating(learning) parameters is number_of_iterarion times\n",
    "    for i in range(number_of_iterarion):\n",
    "        # make forward and backward propagation and find cost and gradients\n",
    "        cost,gradients = forward_backward_propagation(w,b,x_train,y_train)\n",
    "        cost_list.append(cost)\n",
    "        # lets update\n",
    "        w = w - learning_rate * gradients[\"derivative_weight\"]\n",
    "        b = b - learning_rate * gradients[\"derivative_bias\"]\n",
    "        if i % 10 == 0:\n",
    "            cost_list2.append(cost)\n",
    "            index.append(i)\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "    # we update(learn) parameters weights and bias\n",
    "    parameters = {\"weight\": w,\"bias\": b}\n",
    "    plt.plot(index,cost_list2)\n",
    "    plt.xticks(index,rotation='vertical')\n",
    "    plt.xlabel(\"Number of Iterarion\")\n",
    "    plt.ylabel(\"Cost\")\n",
    "    plt.show()\n",
    "    return parameters, gradients, cost_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "db34e1dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.014222401929988\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'derivate_weight' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-181-c3418818eecf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradients\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcost_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.009\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnumber_of_iterarion\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-180-7116fb8e8163>\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(w, b, x_train, y_train, learning_rate, number_of_iterarion)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumber_of_iterarion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;31m# make forward and backward propagation and find cost and gradients\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mcost\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgradients\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mforward_backward_propagation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0mcost_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;31m# lets update\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-179-f132c975e9e8>\u001b[0m in \u001b[0;36mforward_backward_propagation\u001b[1;34m(w, b, x_train, y_train)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;31m#backward propagation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mderivative_weight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_head\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mderivate_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mderivative_bias\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_head\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mgradients\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"derivative_weight\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mderivative_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"derivative_bias\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mderivative_bias\u001b[0m \u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'derivate_weight' is not defined"
     ]
    }
   ],
   "source": [
    "parameters, gradients, cost_list = update(w, b, x_train, y_train, learning_rate = 0.009,number_of_iterarion = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "eeaa9422",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_backward_propagation(w ,b ,x_train, y_train):\n",
    "    z = np.dot(w.T , x_train)+b\n",
    "    y_head = sigmoid(z)\n",
    "    loss = -y_train*np.log(y_head)-(1-y_train)*np.log(1-y_head)\n",
    "    cost = np.sum(loss)/x_train.shape[1]\n",
    "    print(cost)\n",
    "    #backward propagation\n",
    "    derivative_weight = np.dot(x_train,((y_head-y_train).T))/x_train.shape[1]\n",
    "    print(derivate_weight)\n",
    "    derivative_bias = np.sum(y_head-y_train)/x_train.shape[1]\n",
    "    gradients = {\"derivative_weight\":derivative_weight, \"derivative_bias\":derivative_bias }\n",
    "    return cost , gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "fdffccec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction\n",
    "def predict(w,b,x_test):\n",
    "    # x_test is a input for forward propagation\n",
    "    z = sigmoid(np.dot(w.T,x_test)+b)\n",
    "    Y_prediction = np.zeros((1,x_test.shape[1]))\n",
    "    # if z is bigger than 0.5, our prediction is sign one (y_head=1),\n",
    "    # if z is smaller than 0.5, our prediction is sign zero (y_head=0),\n",
    "    for i in range(z.shape[1]):\n",
    "        if z[0,i]<= 0.5:\n",
    "            Y_prediction[0,i] = 0\n",
    "        else:\n",
    "            Y_prediction[0,i] = 1\n",
    "    return Y_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "f5912b10",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'parameters' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-185-6875c347ad36>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"weight\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"bias\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'parameters' is not defined"
     ]
    }
   ],
   "source": [
    "predict(parameters[\"weight\"],parameters[\"bias\"],x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "1b28f0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(x_train, y_train, x_test, y_test, learning_rate ,  num_iterations):\n",
    "    # initialize\n",
    "    dimension =  x_train.shape[0]  # that is 4096\n",
    "    w,b = initialize_weights_and_bias(dimension)\n",
    "    # do not change learning rate\n",
    "    parameters, gradients, cost_list = update(w, b, x_train, y_train, learning_rate,num_iterations)\n",
    "    y_prediction_test = predict(parameters[\"weight\"],parameters[\"bias\"],x_test)\n",
    "    y_prediction_train = predict(parameters[\"weight\"],parameters[\"bias\"],x_train)\n",
    "    # Print train/test Errors\n",
    "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_train - y_train)) * 100))\n",
    "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1766f03",
   "metadata": {},
   "source": [
    "# sklearn이용하여 logistic reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "b27df496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy: 0.967741935483871 \n",
      "train accuracy: 1.0 \n"
     ]
    }
   ],
   "source": [
    "#sklearn 이용\n",
    "from sklearn import linear_model\n",
    "logreg = linear_model.LogisticRegression(random_state=42, max_iter=150)\n",
    "\n",
    "print(\"test accuracy: {} \".format(logreg.fit(x_train.T, y_train.T).score(x_test.T, y_test.T)))\n",
    "print(\"train accuracy: {} \".format(logreg.fit(x_train.T, y_train.T).score(x_train.T, y_train.T)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5387d9",
   "metadata": {},
   "source": [
    "# 딥러닝 날코딩 tensorflow 안씀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "b51d2a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# intialize parameters and layer sizes\n",
    "def initialize_parameters_and_layer_sizes_NN(x_train, y_train):\n",
    "    parameters = {\"weight1\": np.random.randn(3,x_train.shape[0]) * 0.1,\n",
    "                  \"bias1\": np.zeros((3,1)),\n",
    "                  \"weight2\": np.random.randn(y_train.shape[0],3) * 0.1,\n",
    "                  \"bias2\": np.zeros((y_train.shape[0],1))}\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "f39f0ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation_NN(x_train, parameters):\n",
    "    Z1 = np.dot(parameters[\"weight1\"],x_train) +parameters[\"bias1\"]\n",
    "    A1 = np.tanh(Z1)\n",
    "    Z2 = np.dot(parameters[\"weight2\"],A1) + parameters[\"bias2\"]\n",
    "    A2 = sigmoid(Z2)\n",
    "    cache = {\"Z1\": Z1,\n",
    "             \"A1\": A1,\n",
    "             \"Z2\": Z2,\n",
    "             \"A2\": A2}\n",
    "    return A2, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "066933c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute cost\n",
    "def compute_cost_NN(A2, Y, parameters):\n",
    "    logprobs = np.multiply(np.log(A2),Y)\n",
    "    cost = -np.sum(logprobs)/Y.shape[1]\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "0adab8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward Propagation\n",
    "def backward_propagation_NN(parameters, cache, X, Y):\n",
    "    dZ2 = cache[\"A2\"]-Y\n",
    "    dW2 = np.dot(dZ2,cache[\"A1\"].T)/X.shape[1]\n",
    "    db2 = np.sum(dZ2,axis =1,keepdims=True)/X.shape[1]\n",
    "    dZ1 = np.dot(parameters[\"weight2\"].T,dZ2)*(1 - np.power(cache[\"A1\"], 2))\n",
    "    dW1 = np.dot(dZ1,X.T)/X.shape[1]\n",
    "    db1 = np.sum(dZ1,axis =1,keepdims=True)/X.shape[1]\n",
    "    grads = {\"dweight1\": dW1,\n",
    "             \"dbias1\": db1,\n",
    "             \"dweight2\": dW2,\n",
    "             \"dbias2\": db2}\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "b0def6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update parameters\n",
    "def update_parameters_NN(parameters, grads, learning_rate = 0.01):\n",
    "    parameters = {\"weight1\": parameters[\"weight1\"]-learning_rate*grads[\"dweight1\"],\n",
    "                  \"bias1\": parameters[\"bias1\"]-learning_rate*grads[\"dbias1\"],\n",
    "                  \"weight2\": parameters[\"weight2\"]-learning_rate*grads[\"dweight2\"],\n",
    "                  \"bias2\": parameters[\"bias2\"]-learning_rate*grads[\"dbias2\"]}\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "c338afbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction\n",
    "def predict_NN(parameters,x_test):\n",
    "    # x_test is a input for forward propagation\n",
    "    A2, cache = forward_propagation_NN(x_test,parameters)\n",
    "    Y_prediction = np.zeros((1,x_test.shape[1]))\n",
    "    # if z is bigger than 0.5, our prediction is sign one (y_head=1),\n",
    "    # if z is smaller than 0.5, our prediction is sign zero (y_head=0),\n",
    "    for i in range(A2.shape[1]):\n",
    "        if A2[0,i]<= 0.5:\n",
    "            Y_prediction[0,i] = 0\n",
    "        else:\n",
    "            Y_prediction[0,i] = 1\n",
    "    return Y_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "b80d1149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.328968\n",
      "Cost after iteration 100: 0.336379\n",
      "Cost after iteration 200: 0.301014\n",
      "Cost after iteration 300: 0.250538\n",
      "Cost after iteration 400: 0.203739\n",
      "Cost after iteration 500: 0.167159\n",
      "Cost after iteration 600: 0.139918\n",
      "Cost after iteration 700: 0.119787\n",
      "Cost after iteration 800: 0.104390\n",
      "Cost after iteration 900: 0.092154\n",
      "Cost after iteration 1000: 0.082112\n",
      "Cost after iteration 1100: 0.073599\n",
      "Cost after iteration 1200: 0.066122\n",
      "Cost after iteration 1300: 0.059340\n",
      "Cost after iteration 1400: 0.053107\n",
      "Cost after iteration 1500: 0.047461\n",
      "Cost after iteration 1600: 0.042466\n",
      "Cost after iteration 1700: 0.038194\n",
      "Cost after iteration 1800: 0.034651\n",
      "Cost after iteration 1900: 0.031729\n",
      "Cost after iteration 2000: 0.029287\n",
      "Cost after iteration 2100: 0.027214\n",
      "Cost after iteration 2200: 0.025430\n",
      "Cost after iteration 2300: 0.023878\n",
      "Cost after iteration 2400: 0.022513\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEYCAYAAABGJWFlAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Z1A+gAAAACXBIWXMAAAsTAAALEwEAmpwYAAAy2UlEQVR4nO3deZxU1Zn/8c/TG9AsTQPN2g2NyOoG2qLignvAGDXGiGQSxcnESTLGJGbTTGZMTMaYOMmYSXR+GhOjTowh0UTGDZeAW1RodgFZhEaatVmbtdfn98e9rUVRBV1FV1d19/f9et1XV517Tp3ndnfdp+49p+41d0dERCRaVroDEBGRzKQEISIiMSlBiIhITEoQIiISkxKEiIjEpAQhIiIxpTRBmNkkM1thZqvN7LYY679oZkvMbKGZvWFmY8LyUjM7EJYvNLP/l8o4RUTkcJaq70GYWTawErgEqATmAlPdfVlEnR7uXh0+vgL4srtPMrNS4Bl3PzElwYmIyFHlpPC1xwOr3X0NgJk9AVwJfJggmpJDqCuQdLbq06ePl5aWJttcRKRDmjdv3jZ3L4q1LpUJYhCwPuJ5JXBGdCUz+xfgViAPuDBi1VAzWwBUA99z99djtL0JuAlg8ODBlJeXt1z0IiIdgJmti7cu7YPU7n6fuw8DvgN8LyzeBAx293EEyeNxM+sRo+2D7l7m7mVFRTEToIiIJCmVCWIDUBLxvDgsi+cJ4CoAd69x9+3h43nA+8CI1IQpIiKxpDJBzAWGm9lQM8sDrgNmRFYws+ERTz8OrArLi8JBbszsOGA4sCaFsYqISJSUjUG4e72Z3QzMBLKB37r7UjO7Eyh39xnAzWZ2MVAH7ARuCJufB9xpZnVAI/BFd9+RqlhFRORwKZvm2trKyspcg9QiIokxs3nuXhZrXdoHqUVEJDMpQYiISExKEAn6YPt+7n15JV99YgH7a+vTHY6ISMqk8oty7Ub1wTqeW7yJJ+dXMrdiJ2bgDqW9u/L1SzT7VkTaJyWIOOobGnl91TaenF/JS8u2UFPfyHFFXfnWx0Zy1bhB3PXcch547X2uG1/CgIIu6Q5XRKTFKUFEWbaxmqfmV/LXhRvZtreGwvxcrju9hKtPLebk4gLMDIDbJo3ipWVb+OkLK/ivKWPTG7SISAooQQBbqw/y9MKNPDm/kvc27yE327hwVF+uPrWYC0b2JS/n8KGakl75fOHcodw3632uP2sI4wYXpiFyEZHU6fAJYt32fVzwn7NpdDilpCd3XnkCnzh5IIVd847a9kvnH8/08krufGYZT31pwodHFyIi7UGHTxCDe+Xz3ctGc/7Ivhzft1tCbbt1yuFbl47k208uZsaijVw5dlCKohQRaX0dfpqrmfFP5x6XcHJo8qnTijlhYA9+8vx7HKxraOHoRETSp8MniGOVnWX82+Vj2Lj7IL9+TdcTFJH2QwmiBZx5XG8mn9if+2e/z5bqg+kOR0SkRShBtJDbJ4+modH56Qsr0h2KiEiLUIJoIYN753PjOaU8Ob+SxZW70h2OiMgxU4JoQTdfcDx9uuXxw2eW0V4uoy4iHZcSRAvq3jmXb1w6krkVO3luyeZ0hyMickyUIFrYtWUljOrfnbueW65pryLSpilBtLDsLOPfLx/Dhl0H+M0ba9MdjohI0pQgUmDC8X24ZEw/7p+1mq17NO1VRNomJYgU+e5lo6ltaORnM1emOxQRkaQoQaTI0D5dmTahlOnz1vPuht3pDkdEJGFKECl084XDKczXtFcRaZuUIFKooEsuX79kBO+s3cHMpZr2KiJtS0oThJlNMrMVZrbazG6Lsf6LZrbEzBaa2RtmNiZi3e1huxVm9rFUxplKU08vYUS/btz13HvU1Gvaq4i0HSlLEGaWDdwHTAbGAFMjE0DocXc/yd3HAj8Ffh62HQNcB5wATALuD1+vzcnJzuLfLh/DBzv28/CbFekOR0Sk2VJ5BDEeWO3ua9y9FngCuDKygrtXRzztCjSdqL8SeMLda9x9LbA6fL026dzhRUwcUcRDr6+htr4x3eGIiDRLKhPEIGB9xPPKsOwQZvYvZvY+wRHELQm2vcnMys2svKqqqsUCT4VpZ5eybW8tLy/fku5QRESaJe2D1O5+n7sPA74DfC/Btg+6e5m7lxUVFaUmwBZy3vAiBvXswh/mfJDuUEREmiWVCWIDUBLxvDgsi+cJ4Kok22a87CxjyuklvL5qGx9s35/ucEREjiqVCWIuMNzMhppZHsGg84zICmY2POLpx4FV4eMZwHVm1snMhgLDgTkpjLVVXFtWQpbBE3N1FCEimS9lCcLd64GbgZnAcmC6uy81szvN7Iqw2s1mttTMFgK3AjeEbZcC04FlwAvAv7h7m58j2r+gMxeO6sf08krqGjRYLSKZzdrLN3zLysq8vLw83WEc1az3tnLj7+byP/9wKpNPGpDucESkgzOzee5eFmtd2gepO5rzRhQxsKAzj2uwWkQynBJEKwsGqwfz+qptrN+hwWoRyVxKEGlw7enFGqwWkYynBJEGAwq6cOGovhqsFpGMpgSRJlPHD6ZqTw2vLN+a7lBERGJSgkiTiSOKGFDQWd+sFpGMpQSRJjnZWVxbVsJrq6o0WC0iGUkJIo2mnF6CAX+cu/6odUVEWpsSRBoN7NmFC0b2ZXr5eg1Wi0jGUYJIs6njB7N1Tw1/e0+D1SKSWZQg0uz8kUX076HBahHJPEoQaZaTncW1p5fw6soqKndqsFpEMocSRAaYcnpw64vpGqwWkQyiBJEBBvXswvkjivhj+XrqNVgtIhlCCSJDTB0/mC3VGqwWkcyhBJEhLhzVl77dO2mwWkQyhhJEhsjJzmLK6SXMXlnFhl0H0h2OiIgSRCZpGqzWN6tFJBMoQWSQ4sJ8Jo4oYvpcDVaLSPopQWSYqeMHs7n6ILNXVKU7FBHp4JQgMowGq0UkUyhBZJjc8DLgs1ZsZaMGq0UkjVKaIMxskpmtMLPVZnZbjPW3mtkyM1tsZq+Y2ZCIdQ1mtjBcZqQyzkwz5fQSHA1Wi0h6pSxBmFk2cB8wGRgDTDWzMVHVFgBl7n4y8GfgpxHrDrj72HC5IlVxZqKSXvmcO7yI6fpmtYikUSqPIMYDq919jbvXAk8AV0ZWcPdZ7t50hbq3geIUxtOmfGZ8CZt2a7BaRNInlQliEBB5jqQyLIvn88DzEc87m1m5mb1tZlfFamBmN4V1yquq2teO9KLR/ejXoxOPvr0u3aGISAeVEYPUZvZZoAy4J6J4iLuXAZ8B7jWzYdHt3P1Bdy9z97KioqJWirZ15GZn8dkzhvDayipWb92b7nBEpANKZYLYAJREPC8Oyw5hZhcD/wpc4e41TeXuviH8uQaYDYxLYawZaeoZg8nLzuLRtyrSHYqIdECpTBBzgeFmNtTM8oDrgENmI5nZOOABguSwNaK80Mw6hY/7AGcDy1IYa0bq060TnzhlIH+eV8nuA3XpDkdEOpiUJQh3rwduBmYCy4Hp7r7UzO40s6ZZSfcA3YA/RU1nHQ2Um9kiYBZwt7t3uAQBMG1CKftrG/hTuaa8ikjrMndPdwwtoqyszMvLy9MdRkpc8z9/Z+ueGmZ983yysyzd4YhIO2Jm88Lx3sNkxCC1HNmNZw/lgx37maWbCYlIK1KCaAMuPaEfAwo687u/V6Q7FBHpQJQg2oDc7Cw+e+YQ3li9jVVb9qQ7HBHpIJQg2oip4weTl5OlowgRaTVKEG1Er655XDV2IE/N38Du/ZryKiKppwTRhkybMJQDdQ38sVz3ihCR1FOCaEPGDOzBGUN78cjf19HQ2D6mJ4tI5lKCaGNuPLuUDbsO8PLyLekORUTaOSWINubi0f0Y1LMLv3uzIt2hiEg7pwTRxuRkZ/G5s4bw1prtvLe5Ot3hiEg7pgTRBl13egmdc7N0FCEiKaUE0Qb1zM/jk+MG8ZcFG9i5rzbd4YhIO6UE0UZNmzCUmvpGnpirq7yKSGooQbRRI/t3Z8Kw3jz2VgX1DY3pDkdE2iEliDZs2oRSNu4+yEvLNOVVRFqeEkQbdtHofhQXduFhXZ9JRFJACaINy84ybjirlDlrd7B04+50hyMi7YwSRBt3bVkJXXKzNeVVRFqcEkQbV5Cfy9WnDuLpRRvZvrcm3eGISDuiBNEOTJtQSq2mvIpIC1OCaAeG9+vOucP78Nhb66jTlFcRaSFKEO3EtAmlbK4+yMylm9Mdioi0E0oQ7cQFI/sypHc+D2uwWkRaSEoThJlNMrMVZrbazG6Lsf5WM1tmZovN7BUzGxKx7gYzWxUuN6QyzvYgK8u4/qxS5q3byZy1O9Idjoi0AylLEGaWDdwHTAbGAFPNbExUtQVAmbufDPwZ+GnYthdwB3AGMB64w8wKUxVre/GZ8YPp36Mzdz23HHfdcU5Ejk0qjyDGA6vdfY271wJPAFdGVnD3We6+P3z6NlAcPv4Y8JK773D3ncBLwKQUxtoudMnL5tZLR7Bw/S6eXbIp3eGISBuXygQxCIicd1kZlsXzeeD5RNqa2U1mVm5m5VVVVccYbvvwqVOLGdW/Oz99YQU19Q3pDkdE2rCMGKQ2s88CZcA9ibRz9wfdvczdy4qKilITXBuTnWXcftloPtixn/99+4N0hyMibVgqE8QGoCTieXFYdggzuxj4V+AKd69JpK3ENnFEEecO78Mv/7aK3Qfq0h2OiLRRqUwQc4HhZjbUzPKA64AZkRXMbBzwAEFy2BqxaiZwqZkVhoPTl4Zl0ky3Tx7N7gN13D9rdbpDEZE2KmUJwt3rgZsJduzLgenuvtTM7jSzK8Jq9wDdgD+Z2UIzmxG23QH8kCDJzAXuDMukmcYM7MHV44p5+O8VVO7cf/QGIiJRrDnTIc3sMXf/3NHK0qmsrMzLy8vTHUZG2bjrABf852wmn9ife68bl+5wRCQDmdk8dy+Lta65RxAnRL1gNnDasQYmqTWwZxf+8Zyh/HXhRpZU6n4RIpKYIyYIM7vdzPYAJ5tZdbjsAbYCT7dKhHJMvnT+MHp1zdOX50QkYUdMEO7+Y3fvDtzj7j3Cpbu793b321spRjkGPTrncsuFx/PWmu3MXqHviohI8zX3FNMzZtYVgu8smNnPI6+bJJntM2cMobR3Pj9+fjn1uhy4iDRTcxPE/wD7zewU4BvA+8CjKYtKWlReThbfmTSKlVv28ud5lekOR0TaiOYmiHoPTmBfCfzK3e8DuqcuLGlpk07sz2lDCvn5SyvZX1uf7nBEpA1oboLYY2a3A58DnjWzLCA3dWFJSzMzvnvZKLbuqeHXr61Ndzgi0gY0N0FMAWqAf3T3zQSXvkjoukmSfqcN6cXkE/vzwGvvs3XPwXSHIyIZrlkJIkwKvwcKzOxy4KC7awyiDfr2pFHU1jdy78ur0h2KiGS4ZiUIM7sWmAN8GrgWeMfMrkllYJIaQ/t05bNnDuGPc9ezeuuedIcjIhmsuaeY/hU43d1vcPfrCW4G9G+pC0tS6ZaLhpOfm83dz7+X7lBEJIM1N0FkRV1tdXsCbSXD9Oqax5cuGMbLy7fy9prt6Q5HRDJUc3fyL5jZTDObZmbTgGeB51IXlqTaP549lIEFwf2rGxt1CQ4ROdzRrsV0vJmd7e7fIrhvw8nh8hbwYCvEJynSOTebb1w6ksWVu3lG968WkRiOdgRxL1AN4O5Pufut7n4r8JdwnbRhV40bxJgBPbjr2eXs3Feb7nBEJMMcLUH0c/cl0YVhWWlKIpJWk51l/ORTJ7N9Xw3f/NMiXe1VRA5xtATR8wjrurRgHJImJxUXcPvk0bzy3lZ++2ZFusMRkQxytARRbmZfiC40s38C5qUmJGltN55dysWj+3H388tZXLkr3eGISIY44i1HzawfwXhDLR8lhDIgD/hk+A3rjKBbjh6bXftruewXr5OTncUzt5xDj8661JZIR5D0LUfdfYu7TwB+AFSEyw/c/axMSg5y7Hrm5/HfU8exYdcBvvvUEo1HiAg5zank7rOAWSmORdKsrLQXt14ygntmruDs4/swdfzgdIckImmkb0PLIb40cRjnHN+H789YyorNulaTSEeW0gRhZpPMbIWZrTaz22KsP8/M5ptZffTF/8yswcwWhsuMVMYpH8nKMn4+5RS6d87l5sfnc6C2Id0hiUiapCxBmFk2cB8wGRgDTDWzMVHVPgCmAY/HeIkD7j42XK5IVZxyuL7dO3PvlLGsrtrL92csTXc4IpImqTyCGA+sdvc17l4LPEFwy9IPuXuFuy8GGlMYhyThnOF9+PL5w/hj+XqeXrgh3eGISBqkMkEMAtZHPK8My5qrs5mVm9nbZnZVrApmdlNYp7yqquoYQpVYvn7xCMqGFPLdp5awdtu+dIcjIq0skweph4Rzcz8D3Gtmw6IruPuD7l7m7mVFRUWtH2E7l5OdxX9PHUduThZf+cN8auo1HiHSkaQyQWwASiKeF4dlzeLuG8Kfa4DZwLiWDE6aZ2DPLtxzzSm8u6GaHz+nGwyJdCSpTBBzgeFmNtTM8oDrgGbNRjKzQjPrFD7uA5wNLEtZpHJEl4zpx41nl/K7v1cwc6m+HynSUaQsQbh7PXAzMBNYDkx396VmdqeZXQFgZqebWSXBva4fMLOmKTOjCa4DtYjgC3p3u7sSRBrdNnkUJw7qwbf/vJgNuw6kOxwRaQVHvBZTW6JrMaVexbZ9XP7LNxjZvztP3HQmudmZPIQlIs2R9LWYRCKV9unKXVefxLx1O/n3p5fqVqUi7VyzrsUk0uSKUwayYnM19816nyyDH155IllZlu6wRCQFlCAkYd+8dCTucP/s9wElCZH2SglCEmZmfOtjIwElCZH2TAlCkqIkIdL+KUFI0pQkRNo3JQg5JtFJwoEfKUmItAtKEHLMYh1JKEmItH1KENIilCRE2h8lCGkxShIi7YsShLQoJQmR9kMJQlqckoRI+6AEISlx2Owmhx9ddSLZShIibYYShKRMdJKo3Lmfe6eMpXe3TmmOTESaQ1dzlZRqShI/vvok3lm7g4//9xvMW7cj3WGJSDMoQUjKmRlTxw/mqS9NIDfHmPLA2zz0+hray71IRNorJQhpNScOKuCZr5zLBaP68qNnl/Ol/51P9cG6dIclInEoQUirKuiSy4OfO43vXjaKl5Zv4YpfvsGyjdXpDktEYlCCkFZnZtx03jD+8IUz2V/bwCfvf5Ppc9enOywRiaIEIWkzfmgvnr3lXE4bUsi3n1zMt/60iAO1DekOS0RCShCSVkXdO/HY58/glguP50/zKvnk/W+ydtu+dIclIihBSAbIzjJuvXQkD994OpurD/KJX77B80s2pTsskQ5PCUIyxgUj+/LsLedyfN9ufOn387nj6XfZW1Of7rBEOqyUJggzm2RmK8xstZndFmP9eWY238zqzeyaqHU3mNmqcLkhlXFK5hjUswvT//ksbjy7lEfeWsdFP5vNM4s36jsTImmQsgRhZtnAfcBkYAww1czGRFX7AJgGPB7VthdwB3AGMB64w8wKUxWrZJa8nCzu+MQJPPXlCfTp1ombH1/A9b+dw5qqvekOTaRDSeURxHhgtbuvcfda4AngysgK7l7h7ouBxqi2HwNecvcd7r4TeAmYlMJYJQOdOriQGTefww+uOIGFH+xi0r2v87MXV2imk0grSWWCGARETm6vDMtarK2Z3WRm5WZWXlVVlXSgkrmys4wbJpTyyjcn8vGTB/DLv63mkv96lVeWb0l3aCLtXpsepHb3B929zN3LioqK0h2OpFDf7p35rylj+cMXzqRzbjaff6ScLzxaTuXO/ekOTaTdSmWC2ACURDwvDstS3VbasbOG9ea5W87ltsmjeGPVNi7++avcN2s1NfU67STS0lKZIOYCw81sqJnlAdcBM5rZdiZwqZkVhoPTl4ZlIuTlZPHFicN4+RsTmTiiiHtmrmDyL17nzdXb0h2aSLuSsgTh7vXAzQQ79uXAdHdfamZ3mtkVAGZ2uplVAp8GHjCzpWHbHcAPCZLMXODOsEzkQ4N6duGBz5Xx8LTTqW9w/uGhd/inR8p5d8PudIcm0i5Ye5lfXlZW5uXl5ekOQ9LkYF0Dv35tDQ++voY9B+u5eHQ/vnbxcE4cVJDu0EQympnNc/eymOuUIKQ92X2gjt+9WcFv3lhDtRKFyFEpQUiHU30wSBQPvd6UKPry1YtGcFKxEoVIJCUI6bCqD9bxyJsVPPTGWnYfqOOiUX356sXDObm4Z7pDE8kIShDS4e05WMcjf6/g168rUYhEUoIQCUUnigtGFvGFc4/jrGG9MbN0hyfS6pQgRKLsOVjHo2+t46HX17Bzfx3D+3bj+gmlXD1uEF075aQ7PJFWowQhEsfBugb+b9FGHnmrgnc3VNO9Uw6fOq2Y688awnFF3dIdnkjKKUGIHIW7s2D9Lh79ewXPLtlEXYNz3ogibjhrCOeP7Et2lk4/SfukBCGSgK17DvLEnPX8/p11bKmuYXCvfD535hA+XVZMz/y8dIcn0qKUIESSUNfQyMylm3n07+uYU7GDzrlZXDV2EJ89c4i+eCfthhKEyDFatrGax96u4C8LNnCwrpFR/btzzWnFXDl2EEXdO6U7PJGkKUGItJBd+2uZsWgjT86rZFHlbrKzjPNHFPGp04q5aHRfOuVkpztEkYQoQYikwKote3hy/gb+sqCSLdU1FHTJ5ROnDOBTpxYztqSnvlchbYIShEgKNTQ6b67expPzK3nh3c3U1DdyXFFXPnVqMVefOogBBV3SHaJIXEoQIq1kz8E6nluyiSfnbWBOxQ7M4Oxhfbhq3CAuGdOPgi656Q5R5BBKECJpsG77Pp6av4GnFlSyfscBcrON84YX8fGTB3DJmH5076xkIemnBCGSRu7OosrdPLt4I88u3sTG3QfJy8li4ogiLj95ABeN7kc3Xd5D0kQJQiRDNDYG39h+dvEmnluyic3VQbK4YGQRHz95IBeN6qtrQUmrUoIQyUCNjc68D3by7OJNPLtkE1V7auicm8WFo/ry8ZMGcv7IIiULSTklCJEM19DolFfs4JnFm3j+3U1s21tLXk4WZw/rzSVj+nPxmL707d453WFKO6QEIdKGNDQ6c9bu4KVlW3hp+WbW7ziAGYwt6cklY/px6Zj+HN9XV5qVlqEEIdJGuTsrtuzhxaVbeGnZFpZs2A3AcX26csmYflwyph/jBhfqarOStLQlCDObBPwCyAYecve7o9Z3Ah4FTgO2A1PcvcLMSoHlwIqw6tvu/sUj9aUEIR3Bpt0HeHnZFl5ctoW33t9OfaPTp1seF43qx8Vj+jFhWG+NW0hC0pIgzCwbWAlcAlQCc4Gp7r4sos6XgZPd/Ytmdh3wSXefEiaIZ9z9xOb2pwQhHU31wTpmr6jipWVbmP3eVvbU1JObbZQN6cXEkUVMHFHEqP7ddckPOaJ0JYizgO+7+8fC57cDuPuPI+rMDOu8ZWY5wGagCBiCEoRIs9XWN1JesYNXV1Xx6ooq3tu8B4C+3TsxcUQRE0cWcc7xfXQ/CznMkRJEKo9FBwHrI55XAmfEq+Pu9Wa2G+gdrhtqZguAauB77v56dAdmdhNwE8DgwYNbNnqRNiQvJ4sJx/dhwvF9uH3yaDbvPshrq6p4dWUVLy7bwp/mVZJlcEpJzyBhjCji5OKeGruQI8rUk5WbgMHuvt3MTgP+amYnuHt1ZCV3fxB4EIIjiDTEKZKR+hd05tqyEq4tK6Gh0VlUuYtXVwQJ4xevrOLel1fRMz+Xs47rzZnH9eaM43oxom93spQwJEIqE8QGoCTieXFYFqtOZXiKqQDY7sF5rxoAd59nZu8DIwCdQxJJUHaWcergQk4dXMjXLxnBzn21vLF6G6+urOKt97fz/LubASjMz2X80F5Bwhjam1H9lTA6ulQmiLnAcDMbSpAIrgM+E1VnBnAD8BZwDfA3d3czKwJ2uHuDmR0HDAfWpDBWkQ6jsGsenzhlIJ84ZSAA63fs5+0123ln7Q7eXrOdmUu3AFDQJUgYZ4RJY/SAHjol1cGkLEGEYwo3AzMJprn+1t2XmtmdQLm7zwB+AzxmZquBHQRJBOA84E4zqwMagS+6+45UxSrSkZX0yqekVz6fLgsO+Ct37uedNTt4Z+123l4TfGEPoHvnHMaX9uLUIYWMLenJScUF9NAVads1fVFORI5o464DvLN2O++s2cGctTtYs20fAGYwrKgbpxT3ZGxJAWNLChnZvzt5OVlpjlgSoW9Si0iL2bW/lsWVu1m4fheL1u9i4fpdbN9XCwSzqU4Y2IOxJT0ZW9KTU4p7MqR3vr6LkcGUIEQkZdydDbsOfJgwFq3fzZINuzlQ1wBAj845jOrfg1EDujOyf3dG9e/OiH7ddcOkDJGu70GISAdgZhQX5lNcmM/lJwcD3/UNjazcspdFlbtYsmE3Kzbv4an5G9hbU/9hu0E9uzCqf5A0gsTRg+OKupKbrVNUmUIJQkRaXE52FmMG9mDMwB5MDcuajjRWbN7De+GyYnM1r66sor4xOJORm20MK+rGsL7dOK5PV0p7d6W0T1eG9ulKYX6uTlW1MiUIEWkVkUcaF43u92F5bX0j71ft/TBxrNhczbsbdvPCu5tpaPzoFHiPzjkM7RMkjNLeQdJoel7QRaerUkEJQkTSKi8ni9EDejB6QI9DymvrG1m/cz8V2/axdts+Krbvo2LbfsordjJj0UYih097dc2juLALg3oGS3FhFwYV5gfPC7sogSRJCUJEMlJeTlZwuqno8JsjHaxr4IMd+4PEESaPyp0HWLFlD397bys19Y2H1O/eKYdBTQkk4mf/Hp3p16MzRd070Tk3u7U2rc1QghCRNqdzbjYj+gWzoaK5O9v31bJh5wE27DpA5c79EY8PMKdiB3sO1h/WrjA/l35hwujXo1PE4+B5/x6d6d2tU4f6NrkShIi0K2ZGn26d6NOtE6eU9IxZp/pgHZt2HWRL9UE2Vx9ka/hzS3UNW6sP8t7maqr21NDo0a8NvfLz6N0t78M+mh4XRTzu070TvbvmtfmjEiUIEelwenTOpUf/XEb2P/wIpElDo7Ntbw1bwsSxufogVXtq2La3hm17ati+r5ZFlbvYtqeGfbUNMV+je6ccenXLo2d+Hr3ycynsmkdhfh69PvyZG6wLn/fMz82oab5KECIiMWRn2YenmI7mQG1DkDj21rB9b+2Hj7ftrWXn/lp27Kulam8NK7fsZef+WvbHSSgQXPOqoEsuPfNzKejStOR9+PjQ8mAp7JpHtxTcalYJQkTkGHXJy/7woofNcbCugZ37a9m5r+7DBLJrfy07wue7D9R9uGzevYfdB+rZfaCWuobYV744aVAB//eVc1pykwAlCBGRVtc5N5sBBV0YUNCl2W3cnQN1DR8mjl37P0oiXfNSsytXghARaQPMjPy8HPLzchJKLMcic0ZDREQkoyhBiIhITEoQIiISkxKEiIjEpAQhIiIxKUGIiEhMShAiIhKTEoSIiMRk7rG/ut3WmFkVsO4YXqIPsC2F9VurjeJqH3El00ZxKa5kDHH3ophr3F1LkCTLU1m/tdoorvYRV3vaFsWVmXE1Z9EpJhERiUkJQkREYlKC+MiDKa7fWm0UV+b10VptFFfm9ZFMm9aK66jazSC1iIi0LB1BiIhITEoQIiISkxKEiIjE1CHvKGdmo4ArgUFh0QZghrsvT0Ff/SL7cfctzWjTC8DddyTQT0JtkowroTbJ9BG2a/a2JNuHdDxmVgBM4tD3/Ux335W2oEgurtbalg43SG1m3wGmAk8AlWFxMXAd8IS7332Ets3eGZnZWOD/AQUEf7ymfnYBX3b3+VH1BwM/BS4K6xjQA/gbcJu7V8ToI5k2CcWV5LYk00dC25JMHxFt28WOIlO3Ixmp3hYzux64A3iRQ/9fLgF+4O6PxmmX0IfJJOonHFey25KMjpggVgInuHtdVHkesNTdh8doM5bEd3gLgX9293eiys8EHnD3U6LK3wLuBf7s7g1hWTbwaeBr7n5mjD6SaZNQXEluSzJ9JLQtyfQRrk/mDZnwEWeqdxSttcNLcltSvpNMtB8zWwGcEZ1wzKwQeMfdR8Rok9CHyWQ+fCYZV8JtkpaKr2dn8gK8R3DtkejyIcCKOG0Whn+Q6PIzgUVx2qw6QgyrE6wfc10K2hwWVwq2JZk+DluXTB/huhVAzxjlhcDKGOXfCf/+twGfDZfbmsri9JFMm0TjSqh+a21La2x7knGtBApilBcc4b2yEsiNUZ4X538yofrHGFdCbZJdOuIYxNeAV8xsFbA+LBsMHA/cHKdNV4/6pArg7m+bWdc4bZ43s2eBRyP6KQGuB16IUX+emd0PPBJV/wZgQZw+kmmTaFzJtEmmj0S3JZk+IDh1FeuwuTFcF+3zxD7i/DmwFIh1SjKZNonGlWj9ZONKtE1rbHsy/fwHMN/MXuTQ9/0lwA/j9NEIDOTwi4AOCNcda/1k40qmTVI6XIJw9xfMbAQwnkMPTed6eGojhoR3Ru5+i5lN5vBD4Pvc/bkYTa4n+Kf/QVT9GcBv4sSVcJsk4kq4TTJ9JLotSfYBib+5knnTt8aOojV2eMm0aa2dZEL9uPsjZjYD+Bgf/b/MBm53951x+vgaiX2YTLR+UnEluS1J6XBjEMmKszOacZSdkWSg8Fxt5JuraUD0sDeXmU0CfgXEfNO7+2EfEJJpk2hcSdZP+ba04rYn20+iM/GySODDZKL1k40r2TaJUoJIkXBWxu0ESaUfwSH0VuBp4G4/fIAph+AT9FUc+s/1NPCb6EPpY2iTUFxJbksyfSS0Lcn0EdU+kRlpCb/pW2tHkeodXjJtWnHbm91P1ESTSoJTV0ed9WZmFqOPOR5nx5lE/YTjSnZbktHhTjElI8md0XSCKZoXuPvm8HX6A9PCdZdG1X+M4A/8Aw6dAXED8L/AlBh9JNMm0biSaZNMH4luSzJ9xH1zmdku4r+5PGJpeh7vdElSbRKNK8ntaJVtSbR+K23L74g/6+1h4JQYcV0K3E9wlBI5u+p4M/uyu794LPWTjSvJNslpyRHv9roAMwlmTfSPKOtPMGvixThtYs6IireOOLM1jrQuyTYJxZXktiTTR0Lbkkwf4bqFJDAjjSDRrAaeBx4KlxfCskvj9JFMm0TjSqh+a21La2x7knElM7NuOVAao3wosPxY6x9DXEnN4EtmabEXas9Lkju8F4FvA/0iyvoRJJqXY9R/m2DOf1ZEWRbBJ+d34vSRTJuE4kpyW5LpI6FtSaaPsE6iU3aTedOnfEfRGju8ZNq04k4y0bj+G3g2/H+aEC5TwrJfxYsLyIlRnhfvb5JI/WOIK+E2yS46xdQ868zs28AjHp4XDc+XTuOjAbJoUwiOMF4N6zqwhWBWzrUx6l8H/AS4Lzy0BugJzArXxdLU5n4z20lwaF5wlDaJxpVMm2T6SHT7m/qYHfZBM/qAxGek5fDRKa9IG4DcOH0k06Y1phK3xra0xrYn3I8nN+vtt8BcM3siKq7riD1LMNH6TXFdBlzR3LiS3JakaJC6GcIZFrcR/EH6hsVNO6O7Pf5Mi1EE5yDfdve9EeWTPPaMkTMIdqbvA6OAs4Blzfmjm1nv8OEv3P2zCWzbuQSDaks89jnSprjec/fdZpZP8Ls4lWC++V3uvjuq/i3AX9w9XvKM1UcewbdQNwLzCS67cHbYx4Mee8B9GHA1wZuwgeALV4+7e/VR+mr2jDQzu50g4cR600939x+3RJuwXawdRdyZconOrGvBbRlMkKAPa9OK255UP4kys9HE/h0vi1N/TJztiFm/pZhZX3ff2uKvqwRxbMzsRnd/OEb5LcC/EBwKjwW+6u5Ph+vmu/upUfXvACYTfDJ6iWCnPZtgLvhMd/+PGH3MiBHShQSDt7j7FTHazHH38eHjfwpj/CvBOd3/89iXA1gKnOLu9Wb2ILAPeJLgukmnuPvVUfV3h3XeBx4H/uTu22LEGtnm9+G2dwF2A12Bv4R9mLvfEFX/FuBy4DXgMoIv0+0CPkkwsDn7SP0lItGdRNim1XcUzdlJJLktie4kE+4jGYn8jo911luqhBMr7iAYXP934CsEH3reI9hnbIrRpleMl5oPjCN4rzT7Ip9H1ZLnqzriAnwQp3wJ0C18XAqUh39wgAVx6mcD+UA10CMs7wIsjtPHfIIZPucDE8Ofm8LHE+O0WRDxeC5QFD7uSnAUEavN8sg+o9YtjNUHwfjBpQSH1lUEpwpuALrH6WNx+DOH4OgsO3xusba/6fcVPs4HZoePB8f6/Ua0KyD4lu1yYAewPXx8NzEu99CK/0f9gf8B7gN6A98HFhPMyBoQo36vGEsFweUpeqU41t4t/HqnE5xK/F+Co4CXCJL9XGBcC/WR8ESTo7ze8zHKegA/JpiRNzVq3f1xXucFgqRwW/j3/k74O/gK8HScNo3A2qilLvy5pkX/Nqn8R2ovS/iHi7UsAWritFka9bxb+M/w83g71ViPw+eH1Q/Ls4Cvh2+osWHZEf9BgEXhTqQ3UB4vhqjyPwE3ho8fBsrCxyMI5p1H149OIrkEn/T+AFTF6eNdgsG8QmBP004O6EzsQcclQKfwcWHktgDvHmH7W2xHEWsnEZanfEeRzE4CmBTxuIBg5s9igqO8fnHa3A30CR+fBqwhGIxdR4wPIQQfWr4HHJfA73EOwdHzVILTRdeE5RcBb8Vp0w24k+AU5G6CDyFvA9Pi1E9mosmpcZbTgE0x6j8Z/r6uIjj9/GTE/+j8OH0siHj8QdS6eO/7b4T/LydFlK1N5H+32X+bVLxoe1sIPtGOJbigX+RSCmyM0+ZvhDvtiLIcgoG4hhj13wHyw8eRM3kK4v1zRdQpJtiJ/yr6nyxG3YrwTb42/DkgLO92hH/IAoK51++HcdaFbV8lOMUUXX/BEfrPj1P+9fA11wG3AK8AvyZIBHfEqP9Vgp3brwkOx5sSWBHw2hH6T3TKbkI7ibBNyncUyewkIvsmSA4/Cv+Pvw78NU6bJRGPZwGnh49HEPUBoykG4D+BDwh2/F8HBh4lriNte8z/JYJTQ9PC//1bgX8DhhNcy+uuGPWTmVnXQPA+nhVjORCj/sKo5/8KvEnwYSze331RxOMfxfvdx2jX9J7/OdCdFj5y+LCfVLxoe1sITpOcE2fd40f4A/aPs+7sGGWd4tTtE7kTOEqcH4/15mhm23xg6FHq9CD4Es5pxPnEGdYbkWQMA5t2JgQzmK4Bxh+h/glhnVEJ9JHolN2EdhJhm4VRz1Oyo0h0J8GhCSI6xoVx2iwnnLpJMNniiHFF9XEuwRfHNoe/r5vi9PEWwenITxN8QLgqLJ9IjCQU/fsKn88Nf2YRTKiIrl9IMEvuPWAnwenF5WFZzFNyBEe1w+OsWx/nd5UVVTaN4ChnXZzXuZPwVHRU+fEEl74/2v/zFQRHTpub+x5IZGnxF9SiJZOXqB3FjqgdRWGM+gntJMLyVt1RNHcnQTAt9FaCo481hJNUwnXxxrm+QpBULyQYF/lFuOP+AfBYjPqHJUCCsbVJwMNx+jiF4NTf8wSz935BMAaxFJgQp83fCT+0hds/M2JdvFNGo4CLo3/PRJx6iyq/BhgZZ91VMcp+Clwco3wSR/6uxyiC02nNiiu6DcE45YlHa5PM0ipvSi1a2sJCeJoqqiyhnURYntSOIpG4YtSJ3EnErE8wWyZyaZqg0B949AivfT7wR4LJB0uA54CbiP2lsCdS/TcJy08mOIW1E3iD8KiV4BTjLTHq30IwDfqvBKdZr4xYF/cUbqI77yPUnxyn/lcSjSvZbUnq99+SL6ZFS1teOMr4TYz6R91xt1CbRONKqH5rbUtrbHu8fkhwVmFYntCOOMmdfTJxJdwm2UXfg5AOxcwWx1tF8Cm0UwKv9YG7D06w/5htEo2rJbfjSHG1ZJuW2vZk+jGzpe5+QsTzbsCfgWXAhe4+NsbrLAHOcve9ZlYa1n/M3X9hZgvcfdyx1D+GuBJukyxdakM6mn4E9x3YGVVuBOe1Dy088s6rX8wVSbRJNK4k6rfKtrTStifTzxYzG+vuCwHCnfjlBJfHOCnOa2V5eAUEd68ws/OBP5vZkLCfY62fbFzJtEmKEoR0NM8QHJ4vjF5hZrNj1E9455Vkm0TjSrR+snGlPHHROttyPVAfWeDu9cD1ZvZAnD4S3REns+NOJq5k2iSnJc9XadHS3haSm+KccJv2si2tte2t0Q+JT1VPqH5bWDQGISIiMWWlOwAREclMShAiIhKTEoS0CWbmZvaziOffNLPvt9Br/87MrmmJ1zpKP582s+VmNiuqvNTM3g0fjw3vjZDKOJ4zs56p7EPaByUIaStqgKvNrE+6A4lkZonMBPw88AV3v+AIdcYS3N+ixWOwQJa7X+Zpuv+BtC1KENJW1AMPElwd9BDRRwBmtjf8eb6ZvWpmT5vZGjO728z+wczmmNmS8I50TS42s3IzWxlOTcTMss3sHjOba2aLzeyfI1739fCGTbFuTjM1fP13zewnYdm/A+cAvzGze2JtoAV31bsTmGJmC81sipl1NbPfhjEvMLMrw7rTzGyGmf0NeMXMupnZK2Y2P+y7qV6pma0ws0cJritVYmYVTYnWzG4N43zXzL4W0Wa5mf3azJaa2Ytm1qX5fyppN9I9jUqLluYswF6Cq8lWEFx+/JvA98N1vyO8h0BT3fDn+QQXfRsAdCK449gPwnVfBe6NaP8CwQem4QQXtetMcL2h74V1OhFc0mBo+Lr7iHH1W4Ir0n5AcE2gHIIrwV4VrptNeC+NqDalhPewILio368i1t0FfDZ83BNYSXBzp2lhnE33zcjho5tM9QFWE3wnoJTg3hFnRrxmRVjnNILLNnQluOjbUoK7kpUSJOSxYf3pTTFo6ViLjiCkzfDgXtOPElwjp7nmuvsmd68huJ9F0323lxDsCJtMd/dGd19FcKXTUQSXoL7ezBYS3AejN0ECAZjj7mtj9Hc6wd3tqjz48tLvgfMSiDfapcBtYQyzCRJX02UkXvKPbi9pwF3hN4xfJrgNZ9M3ite5+9sxXvscgnuH7/PgG8BPEVyiG4J7SywMH8/j0N+VdBD6JrW0NfcS3LXs4YiyesLTpWaWRXBnuiY1EY8bI543cuj/f/QXgpxgp/sVd58ZuSK8hMK+ZIJPggGfcvcVUTGcERXDPxActZzm7nVmVkGQTCC5WCN/bw0EV4uVDkZHENKmhJ+YpxMM+DapIDhdAsG9AXKTeOlPm1lWOC5xHMFVOWcCXzKzXAAzG2FmXY/yOnOAiWbWx8yyCW6j+WoCcewhuPlPk5nAV8zMwhgOu+BbqADYGiaHCwjuFHc0rwNXmVl+uF2fDMtEACUIaZt+RnAOvcmvCXbKi4CzSO4Tc9MtMp8HvujuBwluy7kMmB9OQ32Aoxx1u/smgvtKzyK4//c8d386gThmAWOaBqmBHxIkvMVmtjR8HsvvgTILrih6PcENkY7I3ecTjL/MITiF9pC7L0ggVmnndKkNERGJSUcQIiISkxKEiIjEpAQhIiIxKUGIiEhMShAiIhKTEoSIiMSkBCEiIjH9f47NwFMaZlz7AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 99.71264367816092 %\n",
      "test accuracy: 95.16129032258064 %\n"
     ]
    }
   ],
   "source": [
    "# 2 - Layer neural network\n",
    "def two_layer_neural_network(x_train, y_train,x_test,y_test, num_iterations):\n",
    "    cost_list = []\n",
    "    index_list = []\n",
    "    #initialize parameters and layer sizes\n",
    "    parameters = initialize_parameters_and_layer_sizes_NN(x_train, y_train)\n",
    "    for i in range(0, num_iterations):\n",
    "         # forward propagation\n",
    "        A2, cache = forward_propagation_NN(x_train,parameters)\n",
    "        # compute cost\n",
    "        cost = compute_cost_NN(A2, y_train, parameters)\n",
    "         # backward propagation\n",
    "        grads = backward_propagation_NN(parameters, cache, x_train, y_train)\n",
    "         # update parameters\n",
    "        parameters = update_parameters_NN(parameters, grads)\n",
    "        if i % 100 == 0:\n",
    "            cost_list.append(cost)\n",
    "            index_list.append(i)\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "    plt.plot(index_list,cost_list)\n",
    "    plt.xticks(index_list,rotation='vertical')\n",
    "    plt.xlabel(\"Number of Iterarion\")\n",
    "    plt.ylabel(\"Cost\")\n",
    "    plt.show()\n",
    "    # predict\n",
    "    y_prediction_test = predict_NN(parameters,x_test)\n",
    "    y_prediction_train = predict_NN(parameters,x_train)\n",
    "    # Print train/test Errors\n",
    "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_train - y_train)) * 100))\n",
    "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n",
    "    return parameters\n",
    "parameters = two_layer_neural_network(x_train, y_train,x_test,y_test, num_iterations=2500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "a05b9159",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 1s 2ms/step - loss: 0.6920 - accuracy: 0.5570\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.6910 - accuracy: 0.5284\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.6874 - accuracy: 0.5655\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.6909 - accuracy: 0.5114\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.6885 - accuracy: 0.5300\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.6855 - accuracy: 0.5475\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.6852 - accuracy: 0.5254\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.6849 - accuracy: 0.5238\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 993us/step - loss: 0.6683 - accuracy: 0.5824\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.6759 - accuracy: 0.5362\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.6675 - accuracy: 0.5365\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.6606 - accuracy: 0.5536\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.6537 - accuracy: 0.5145\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.6407 - accuracy: 0.5179\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.6038 - accuracy: 0.5685\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.5865 - accuracy: 0.6473\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.5740 - accuracy: 0.7111\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.5525 - accuracy: 0.5865\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 998us/step - loss: 0.5272 - accuracy: 0.7881\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.5229 - accuracy: 0.7203\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4951 - accuracy: 0.7961\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 994us/step - loss: 0.4540 - accuracy: 0.8260\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4501 - accuracy: 0.8474\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.4370 - accuracy: 0.9132\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.4305 - accuracy: 0.7937\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4334 - accuracy: 0.9033\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.4199 - accuracy: 0.9113\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4115 - accuracy: 0.8563\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4210 - accuracy: 0.9227\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.4083 - accuracy: 0.8482\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3806 - accuracy: 0.9368\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 858us/step - loss: 0.3812 - accuracy: 0.8955\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.3805 - accuracy: 0.9200\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3690 - accuracy: 0.9053\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3559 - accuracy: 0.9320\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3543 - accuracy: 0.9420\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3423 - accuracy: 0.9066\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3195 - accuracy: 0.9414\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.3245 - accuracy: 0.9476\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.3049 - accuracy: 0.9449\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.3307 - accuracy: 0.9312\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.3331 - accuracy: 0.9321\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2828 - accuracy: 0.9598\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.3043 - accuracy: 0.9452\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2761 - accuracy: 0.9564\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 998us/step - loss: 0.3317 - accuracy: 0.9233\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.3030 - accuracy: 0.9296\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3049 - accuracy: 0.9447\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.2869 - accuracy: 0.9494\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2649 - accuracy: 0.9734\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 998us/step - loss: 0.2626 - accuracy: 0.9454\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.2545 - accuracy: 0.9659\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2560 - accuracy: 0.9476\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.2656 - accuracy: 0.9532\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2489 - accuracy: 0.9440\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2610 - accuracy: 0.9508\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.2446 - accuracy: 0.9460\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.2522 - accuracy: 0.9577\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.2265 - accuracy: 0.9721\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.2876 - accuracy: 0.9331\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.2605 - accuracy: 0.9035\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.2831 - accuracy: 0.9451\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 998us/step - loss: 0.2618 - accuracy: 0.9416\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.2331 - accuracy: 0.9565\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.2342 - accuracy: 0.9710\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.2275 - accuracy: 0.9613\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2568 - accuracy: 0.9672\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.2351 - accuracy: 0.9351\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 712us/step - loss: 0.2259 - accuracy: 0.9737\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.2252 - accuracy: 0.9623\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.2021 - accuracy: 0.9713\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2066 - accuracy: 0.9569\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.1844 - accuracy: 0.9827\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.2044 - accuracy: 0.9562\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.1796 - accuracy: 0.9734\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.1683 - accuracy: 0.9768\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.1656 - accuracy: 0.9684\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.1689 - accuracy: 0.9801\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.1656 - accuracy: 0.9740\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 854us/step - loss: 0.1593 - accuracy: 0.9694\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.1659 - accuracy: 0.9802\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.1750 - accuracy: 0.9739\n",
      "Epoch 83/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 855us/step - loss: 0.1590 - accuracy: 0.9726\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 712us/step - loss: 0.1522 - accuracy: 0.9716\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.1603 - accuracy: 0.9703\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.1826 - accuracy: 0.9561\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.1461 - accuracy: 0.9773\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.1530 - accuracy: 0.9697\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1318 - accuracy: 0.9828\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.1399 - accuracy: 0.9850\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.1498 - accuracy: 0.9689\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.1411 - accuracy: 0.9794\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.1415 - accuracy: 0.9787\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.1343 - accuracy: 0.9773\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.1376 - accuracy: 0.9806\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 712us/step - loss: 0.1262 - accuracy: 0.9862\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1348 - accuracy: 0.9854\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.1257 - accuracy: 0.9797\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1574 - accuracy: 0.9587\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.1486 - accuracy: 0.9732\n",
      "4/4 [==============================] - 0s 0s/step - loss: 0.1524 - accuracy: 0.9914\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6931 - accuracy: 0.5163\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.6930 - accuracy: 0.5063\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.6925 - accuracy: 0.5192\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.6923 - accuracy: 0.5122\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.6920 - accuracy: 0.5117\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.6911 - accuracy: 0.5178\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.6905 - accuracy: 0.5159\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.6895 - accuracy: 0.5160\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.6891 - accuracy: 0.5111\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 890us/step - loss: 0.6890 - accuracy: 0.4999\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.6858 - accuracy: 0.5061\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 840us/step - loss: 0.6835 - accuracy: 0.4964\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.6789 - accuracy: 0.4899\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 985us/step - loss: 0.6743 - accuracy: 0.4981\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.6652 - accuracy: 0.5025\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.6445 - accuracy: 0.5690\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 851us/step - loss: 0.6376 - accuracy: 0.5354\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 998us/step - loss: 0.6335 - accuracy: 0.5101\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.6135 - accuracy: 0.5482\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.6028 - accuracy: 0.7192\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 712us/step - loss: 0.5701 - accuracy: 0.6420\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.5659 - accuracy: 0.7700\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.5026 - accuracy: 0.75 - 0s 1ms/step - loss: 0.5386 - accuracy: 0.7215\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.5196 - accuracy: 0.8069\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.5169 - accuracy: 0.8482\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 820us/step - loss: 0.5009 - accuracy: 0.7635\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.4851 - accuracy: 0.8447\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.4852 - accuracy: 0.8787\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.4791 - accuracy: 0.8032\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.4712 - accuracy: 0.9052\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4856 - accuracy: 0.8543\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 712us/step - loss: 0.4663 - accuracy: 0.8450\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.4313 - accuracy: 0.8861\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.4134 - accuracy: 0.8726\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.4086 - accuracy: 0.9301\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.4077 - accuracy: 0.8767\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.3915 - accuracy: 0.9660\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.3823 - accuracy: 0.8993\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.4043 - accuracy: 0.9677\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.3869 - accuracy: 0.8587\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.4324 - accuracy: 0.9586\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.3972 - accuracy: 0.8531\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.3765 - accuracy: 0.9646\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.3581 - accuracy: 0.9119\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.3625 - accuracy: 0.9553\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.3522 - accuracy: 0.9074\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 712us/step - loss: 0.3662 - accuracy: 0.9629\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.3624 - accuracy: 0.8987\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.3551 - accuracy: 0.9674\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.3532 - accuracy: 0.9296\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.3661 - accuracy: 0.9279\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.3773 - accuracy: 0.9626\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.3218 - accuracy: 0.9319\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.3385 - accuracy: 0.9707\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.3476 - accuracy: 0.9396\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.3397 - accuracy: 0.9560\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.3108 - accuracy: 0.9500\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 712us/step - loss: 0.3334 - accuracy: 0.9469\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 998us/step - loss: 0.3096 - accuracy: 0.9310\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 712us/step - loss: 0.3485 - accuracy: 0.9727\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.3149 - accuracy: 0.9639\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.3011 - accuracy: 0.9819\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.2978 - accuracy: 0.9799\n",
      "Epoch 64/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 855us/step - loss: 0.3161 - accuracy: 0.9618\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.2991 - accuracy: 0.9568\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.3307 - accuracy: 0.9589\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.3056 - accuracy: 0.9555\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.2824 - accuracy: 0.9767\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.2894 - accuracy: 0.9748\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.3061 - accuracy: 0.9723\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.2847 - accuracy: 0.9752\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.2858 - accuracy: 0.9770\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 712us/step - loss: 0.2758 - accuracy: 0.9771\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.2838 - accuracy: 0.9655\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.2715 - accuracy: 0.9595\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.2674 - accuracy: 0.9812\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 712us/step - loss: 0.2726 - accuracy: 0.9847\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.2715 - accuracy: 0.9840\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.2430 - accuracy: 0.9777\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.2634 - accuracy: 0.9882\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.2796 - accuracy: 0.9752\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.2435 - accuracy: 0.9786\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.2564 - accuracy: 0.9850\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.2576 - accuracy: 0.9704\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 998us/step - loss: 0.2729 - accuracy: 0.9855\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 712us/step - loss: 0.2500 - accuracy: 0.9865\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.2335 - accuracy: 0.9873\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 712us/step - loss: 0.2482 - accuracy: 0.9889\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.2397 - accuracy: 0.9888\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.2343 - accuracy: 0.9808\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.2497 - accuracy: 0.9869\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.2481 - accuracy: 0.9758\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.2409 - accuracy: 0.9951\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.2451 - accuracy: 0.9864\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.2245 - accuracy: 0.9912\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.2239 - accuracy: 0.9889\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.2335 - accuracy: 0.9873\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.2387 - accuracy: 0.9853\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.2538 - accuracy: 0.9671\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 712us/step - loss: 0.2395 - accuracy: 0.9833\n",
      "4/4 [==============================] - 0s 0s/step - loss: 0.3412 - accuracy: 0.9397\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6933 - accuracy: 0.5293\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.6930 - accuracy: 0.5083\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.6928 - accuracy: 0.5148\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.6925 - accuracy: 0.5015\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.6919 - accuracy: 0.5011\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.6914 - accuracy: 0.4627\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.6883 - accuracy: 0.4927\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 970us/step - loss: 0.6810 - accuracy: 0.5366\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.6768 - accuracy: 0.4798\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.6619 - accuracy: 0.5224\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.6492 - accuracy: 0.6740\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 920us/step - loss: 0.6362 - accuracy: 0.5252\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.6309 - accuracy: 0.8260\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 927us/step - loss: 0.6183 - accuracy: 0.5190\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.6013 - accuracy: 0.7623\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.5981 - accuracy: 0.5208\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.5555 - accuracy: 0.7951\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.5361 - accuracy: 0.8644\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 996us/step - loss: 0.5101 - accuracy: 0.7812\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 994us/step - loss: 0.4987 - accuracy: 0.8153\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 854us/step - loss: 0.4982 - accuracy: 0.7945\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.4691 - accuracy: 0.8709\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.4604 - accuracy: 0.8693\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.4468 - accuracy: 0.8768\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.4612 - accuracy: 0.8888\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 994us/step - loss: 0.4299 - accuracy: 0.8863\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.4320 - accuracy: 0.9144\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.4249 - accuracy: 0.9325\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 851us/step - loss: 0.4270 - accuracy: 0.8820\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4223 - accuracy: 0.9359\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.3931 - accuracy: 0.9026\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.4026 - accuracy: 0.9263\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.3959 - accuracy: 0.9321\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.3585 - accuracy: 0.9340\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.3741 - accuracy: 0.9514\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.3691 - accuracy: 0.8976\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 998us/step - loss: 0.3714 - accuracy: 0.9376\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.3435 - accuracy: 0.9582\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 994us/step - loss: 0.3408 - accuracy: 0.9585\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.3445 - accuracy: 0.9611\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3483 - accuracy: 0.9265\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.3541 - accuracy: 0.9369\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3307 - accuracy: 0.9712\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.3363 - accuracy: 0.9689\n",
      "Epoch 45/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 997us/step - loss: 0.3539 - accuracy: 0.9605\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.3370 - accuracy: 0.9665\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 931us/step - loss: 0.3422 - accuracy: 0.9666\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 998us/step - loss: 0.3382 - accuracy: 0.9561\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 916us/step - loss: 0.3575 - accuracy: 0.9284\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.3222 - accuracy: 0.93 - 0s 1ms/step - loss: 0.3196 - accuracy: 0.9707\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.3261 - accuracy: 0.9770\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.2965 - accuracy: 0.9674\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.3242 - accuracy: 0.9827\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.3046 - accuracy: 0.9409\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.3226 - accuracy: 0.9733\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.2838 - accuracy: 0.9452\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.3233 - accuracy: 0.9505\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 752us/step - loss: 0.3225 - accuracy: 0.9825\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 994us/step - loss: 0.2960 - accuracy: 0.9540\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.2896 - accuracy: 0.9764\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.2939 - accuracy: 0.9569\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.3049 - accuracy: 0.9782\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.2709 - accuracy: 0.9923\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.2819 - accuracy: 0.9852\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2817 - accuracy: 0.9721\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 912us/step - loss: 0.2924 - accuracy: 0.9580\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.2697 - accuracy: 0.9794\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.2835 - accuracy: 0.9888\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.2724 - accuracy: 0.9481\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 859us/step - loss: 0.2732 - accuracy: 0.9894\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.2545 - accuracy: 0.9595\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.2750 - accuracy: 0.9746\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 854us/step - loss: 0.2412 - accuracy: 0.9943\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 851us/step - loss: 0.2644 - accuracy: 0.9801\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.2700 - accuracy: 0.9774\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.2554 - accuracy: 0.9815\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 845us/step - loss: 0.2534 - accuracy: 0.9745\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2606 - accuracy: 0.9914\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 851us/step - loss: 0.2505 - accuracy: 0.9871\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9891\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.2408 - accuracy: 0.9950\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 854us/step - loss: 0.2520 - accuracy: 0.9693\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.2532 - accuracy: 0.9915\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.2643 - accuracy: 0.9790\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.2609 - accuracy: 0.9888\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 838us/step - loss: 0.2241 - accuracy: 0.9795\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 859us/step - loss: 0.2159 - accuracy: 0.9958\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 851us/step - loss: 0.2382 - accuracy: 0.9899\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 994us/step - loss: 0.2245 - accuracy: 0.9857\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 860us/step - loss: 0.2158 - accuracy: 1.0000\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2427 - accuracy: 0.9817\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.2253 - accuracy: 0.9937\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2261 - accuracy: 1.0000\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.2374 - accuracy: 0.9806\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2227 - accuracy: 0.9985\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 851us/step - loss: 0.2069 - accuracy: 0.9888\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.2074 - accuracy: 0.9976\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 858us/step - loss: 0.2120 - accuracy: 1.0000\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.2136 - accuracy: 1.0000\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 998us/step - loss: 0.2278 - accuracy: 0.9801\n",
      "4/4 [==============================] - 0s 0s/step - loss: 0.3267 - accuracy: 0.9483\n",
      "Accuracy mean: 0.959770123163859\n",
      "Accuracy variance: 0.022626460191162837\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = x_train.T, x_test.T, y_train.T, y_test.T\n",
    "\n",
    "# Evaluating the DNN\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from keras.models import Sequential # initialize neural network library\n",
    "from keras.layers import Dense # build our layers library\n",
    "def build_classifier():\n",
    "    classifier = Sequential() # initialize neural network\n",
    "    classifier.add(Dense(units = 8, kernel_initializer = 'uniform', activation = 'relu', input_dim = x_train.shape[1]))\n",
    "    classifier.add(Dense(units = 4, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "    classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
    "    classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    return classifier\n",
    "classifier = KerasClassifier(build_fn = build_classifier, epochs = 100)\n",
    "accuracies = cross_val_score(estimator = classifier, X = x_train, y = y_train, cv = 3)\n",
    "mean = accuracies.mean()\n",
    "variance = accuracies.std()\n",
    "print(\"Accuracy mean: \"+ str(mean))\n",
    "print(\"Accuracy variance: \"+ str(variance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d39ae1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
